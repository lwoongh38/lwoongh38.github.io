<?xml version="1.0" encoding="utf-8"?>

<feed xmlns="http://www.w3.org/2005/Atom" >
  <generator uri="https://jekyllrb.com/" version="3.7.4">Jekyll</generator>
  <link href="http://localhost:4000/author/woongE/feed.xml" rel="self" type="application/atom+xml" />
  <link href="http://localhost:4000/" rel="alternate" type="text/html" />
  <updated>2023-11-15T20:49:21+09:00</updated>
  <id>http://localhost:4000/author/woongE/feed.xml</id>

  
  
  

  
    <title type="html">Data Scientist 성장기 | </title>
  

  
    <subtitle>Data Science 학습일지</subtitle>
  

  

  
    
      
    
  

  
  

  
    <entry>
      <title type="html">Time seriesAnomaly Detection Generative Adversarial Networks-Model Structure</title>
      <link href="http://localhost:4000/tadgan-modelstructure" rel="alternate" type="text/html" title="Time seriesAnomaly Detection Generative Adversarial Networks-Model Structure" />
      <published>2021-11-28T07:00:00+09:00</published>
      <updated>2021-11-28T07:00:00+09:00</updated>
      <id>http://localhost:4000/tadgan-modelstructure</id>
      <content type="html" xml:base="http://localhost:4000/tadgan-modelstructure">&lt;p&gt;#deeplearning #TadGAN #modelstructure&lt;/p&gt;

&lt;p&gt;&lt;span class=&quot;table-of-contents-list&quot;&gt;목차 &lt;/span&gt;&lt;/p&gt;
&lt;ul class=&quot;table-of-contents-list&quot;&gt;
    &lt;li&gt;&lt;a href=&quot;./tadgan-modelstructure&quot;&gt;Time seriesAnomaly Detection Generative Adversarial Networks-Model Structure &lt;/a&gt;&lt;/li&gt;  
    &lt;li&gt;&lt;a href=&quot;./tadgan-lossfunction&quot;&gt;Time seriesAnomaly Detection Generative Adversarial Networks-loss function &lt;/a&gt;&lt;/li&gt;  
    &lt;li&gt;&lt;a href=&quot;./tadgan-intro&quot;&gt;Intro to the Time Series Anomaly Detection &lt;/a&gt;&lt;/li&gt;  
    &lt;li&gt;&lt;a href=&quot;./text-ads-filtering&quot;&gt;Text 데이터 광고 필터링을 위한 분류 모델 구축 &lt;/a&gt;&lt;/li&gt;
    &lt;li&gt;&lt;a href=&quot;./bert-sentiment-keyword&quot;&gt;사용자 리뷰 핵심어 추출 및 감성분석 &lt;/a&gt;&lt;/li&gt;
    &lt;li&gt;&lt;a href=&quot;./deeplearning-summary&quot;&gt;Deep Learning - 핵심 개념 &amp;amp; 용어 &lt;/a&gt;&lt;/li&gt;
    &lt;li&gt;&lt;a href=&quot;./versatility&quot;&gt;NLP - 딥러닝 모델의 범용적 활용가능성 분석 &lt;/a&gt;&lt;/li&gt;
    &lt;li&gt;&lt;a href=&quot;./bank-marketing&quot;&gt;Bank marketing 정기예금 가입여부 예측 &lt;/a&gt;&lt;/li&gt;
    &lt;li&gt;&lt;a href=&quot;./roccurve&quot;&gt;ROC curve에 대해 알아보자 &lt;/a&gt;&lt;/li&gt;
    &lt;li&gt;&lt;a href=&quot;./machinelearning-linearregression&quot;&gt;연어의 회귀본능이 아닌 선형회귀에 대해 알아보자 &lt;/a&gt;&lt;/li&gt;  
    
        
    
    


    &lt;!-- &lt;li&gt;&lt;a href=&quot;./proposal&quot;&gt;코스레스토랑에 보내는 제안서 &lt;/a&gt;&lt;/li&gt; --&gt;
    &lt;!-- &lt;li&gt;&lt;a href=&quot;./proposal&quot;&gt;코스레스토랑에 보내는 제안서 &lt;/a&gt;&lt;/li&gt; --&gt;
    &lt;!-- &lt;li&gt;&lt;a href=&quot;./proposal&quot;&gt;코스레스토랑에 보내는 제안서 &lt;/a&gt;&lt;/li&gt; --&gt;
    &lt;!-- &lt;li&gt;&lt;a href=&quot;./proposal&quot;&gt;코스레스토랑에 보내는 제안서 &lt;/a&gt;&lt;/li&gt; --&gt;
    &lt;!-- &lt;li&gt;&lt;a href=&quot;./proposal&quot;&gt;코스레스토랑에 보내는 제안서 &lt;/a&gt;&lt;/li&gt; --&gt;
    &lt;!-- &lt;li&gt;&lt;a href=&quot;./proposal&quot;&gt;코스레스토랑에 보내는 제안서 &lt;/a&gt;&lt;/li&gt; --&gt;
    &lt;!-- &lt;li&gt;&lt;a href=&quot;./proposal&quot;&gt;코스레스토랑에 보내는 제안서 &lt;/a&gt;&lt;/li&gt; --&gt;
    &lt;!-- &lt;li&gt;&lt;a href=&quot;./proposal&quot;&gt;코스레스토랑에 보내는 제안서 &lt;/a&gt;&lt;/li&gt; --&gt;
    &lt;!-- &lt;li&gt;&lt;a href=&quot;./proposal&quot;&gt;코스레스토랑에 보내는 제안서 &lt;/a&gt;&lt;/li&gt; --&gt;
    &lt;!-- &lt;li&gt;&lt;a href=&quot;./proposal&quot;&gt;코스레스토랑에 보내는 제안서 &lt;/a&gt;&lt;/li&gt; --&gt;
    &lt;!-- &lt;li&gt;&lt;a href=&quot;./proposal&quot;&gt;코스레스토랑에 보내는 제안서 &lt;/a&gt;&lt;/li&gt; --&gt;
    &lt;!-- &lt;li&gt;&lt;a href=&quot;./proposal&quot;&gt;코스레스토랑에 보내는 제안서 &lt;/a&gt;&lt;/li&gt; --&gt;
    &lt;!-- &lt;li&gt;&lt;a href=&quot;./proposal&quot;&gt;코스레스토랑에 보내는 제안서 &lt;/a&gt;&lt;/li&gt; --&gt;
    &lt;!-- &lt;li&gt;&lt;a href=&quot;./proposal&quot;&gt;코스레스토랑에 보내는 제안서 &lt;/a&gt;&lt;/li&gt; --&gt;
    &lt;!-- &lt;li&gt;&lt;a href=&quot;./proposal&quot;&gt;코스레스토랑에 보내는 제안서 &lt;/a&gt;&lt;/li&gt;     --&gt;









&lt;!-- 
    &lt;li&gt;&lt;a href=&quot;./test&quot;&gt;시험용 포스팅입니다 - 출력되어라&lt;/a&gt;&lt;/li&gt;
    &lt;li&gt;&lt;a href=&quot;./test&quot;&gt;시험용 포스팅입니다 - 출력되어라&lt;/a&gt;&lt;/li&gt;
    &lt;li&gt;&lt;a href=&quot;./test&quot;&gt;시험용 포스팅입니다 - 출력되어라&lt;/a&gt;&lt;/li&gt;
    &lt;li&gt;&lt;a href=&quot;./test&quot;&gt;시험용 포스팅입니다 - 출력되어라&lt;/a&gt;&lt;/li&gt;
    &lt;li&gt;&lt;a href=&quot;./test&quot;&gt;시험용 포스팅입니다 - 출력되어라&lt;/a&gt;&lt;/li&gt;
    &lt;li&gt;&lt;a href=&quot;./test&quot;&gt;시험용 포스팅입니다 - 출력되어라&lt;/a&gt;&lt;/li&gt;
    &lt;li&gt;&lt;a href=&quot;./test&quot;&gt;시험용 포스팅입니다 - 출력되어라&lt;/a&gt;&lt;/li&gt;
    &lt;li&gt;&lt;a href=&quot;./test&quot;&gt;시험용 포스팅입니다 - 출력되어라&lt;/a&gt;&lt;/li&gt;
    &lt;li&gt;&lt;a href=&quot;./test&quot;&gt;시험용 포스팅입니다 - 출력되어라&lt;/a&gt;&lt;/li&gt;
    &lt;li&gt;&lt;a href=&quot;./test&quot;&gt;시험용 포스팅입니다 - 출력되어라&lt;/a&gt;&lt;/li&gt;
    &lt;li&gt;&lt;a href=&quot;./test&quot;&gt;시험용 포스팅입니다 - 출력되어라&lt;/a&gt;&lt;/li&gt;
    &lt;li&gt;&lt;a href=&quot;./test&quot;&gt;시험용 포스팅입니다 - 출력되어라&lt;/a&gt;&lt;/li&gt;
    &lt;li&gt;&lt;a href=&quot;./test&quot;&gt;시험용 포스팅입니다 - 출력되어라&lt;/a&gt;&lt;/li&gt;
    &lt;li&gt;&lt;a href=&quot;./test&quot;&gt;시험용 포스팅입니다 - 출력되어라&lt;/a&gt;&lt;/li&gt;
    &lt;li&gt;&lt;a href=&quot;./test&quot;&gt;시험용 포스팅입니다 - 출력되어라&lt;/a&gt;&lt;/li&gt;
    &lt;li&gt;&lt;a href=&quot;./test&quot;&gt;시험용 포스팅입니다 - 출력되어라&lt;/a&gt;&lt;/li&gt;
    &lt;li&gt;&lt;a href=&quot;./test&quot;&gt;시험용 포스팅입니다 - 출력되어라&lt;/a&gt;&lt;/li&gt;
    &lt;li&gt;&lt;a href=&quot;./test&quot;&gt;시험용 포스팅입니다 - 출력되어라&lt;/a&gt;&lt;/li&gt;
    &lt;li&gt;&lt;a href=&quot;./test&quot;&gt;시험용 포스팅입니다 - 출력되어라&lt;/a&gt;&lt;/li&gt;
    &lt;li&gt;&lt;a href=&quot;./test&quot;&gt;시험용 포스팅입니다 - 출력되어라&lt;/a&gt;&lt;/li&gt;
    &lt;li&gt;&lt;a href=&quot;./test&quot;&gt;시험용 포스팅입니다 - 출력되어라&lt;/a&gt;&lt;/li&gt;
  --&gt;
    
&lt;/ul&gt;

&lt;h1 id=&quot;time-series-anomaly-detection-generative-adversarial-networks---model-structure&quot;&gt;Time series Anomaly Detection Generative Adversarial Networks - Model Structure&lt;/h1&gt;

&lt;h2 id=&quot;1-tadgan의-구조&quot;&gt;1. TadGAN의 구조&lt;/h2&gt;

&lt;p&gt;지금까지 우리는 TadGAN이 어떤 모델이고, 어떤 loss function을 쓰는 지 알아봤습니다. 하지만 우리는 가장 중요한 걸 모르고 있습니다. 바로 모델 그 자체가 어떻게 작동하는지 입니다. 논문에는 tadgan모델 구조를 다음과 같이 나타냈습니다.&lt;/p&gt;

&lt;p align=&quot;center&quot;&gt;&lt;img src=&quot;https://user-images.githubusercontent.com/70134676/152630537-6932bae6-fe81-4b10-82e3-a65c1e2cc886.png&quot; width=&quot;60%&quot; height=&quot;80%&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;p&gt;모델은 기본적으로 AutoEncoder의 구조와 유사한 구조를 가지고 있습니다. 두 개의 Generator, 두개의 Critic 함수로 이루어졌다는 것을 확인할 수 있습니다. Generator E는 time series sequence를 latent space로 바꿔주는 역할을 하고, Generator G는 latent space를 다시 time series sequence로 변환해주는 역할을 합니다. 여기까지는 기존의 AE와 별반 다를 것이 없어 보입니다. 하지만 Critic X함수는 원본 데이터와 생성된 데이터 중 어떤 데이터가 원본인지를 가려내고, Critic Z 함수는 Generator E가 time series squence를 latent space로 얼마나 잘 맵핑 했는지를 판별 합니다. 더 자세하게 모델이 어떻게 작동하는지를 알기 위해서 TadGAN 공식 github의 이미지를 가지고 설명을 할까 합니다. 모델 이해는 pytorch에 능숙하다는 가정하에 (https://github.com/arunppsg/TadGAN)를 보시는 것을 추천드립니다.&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;p align=&quot;center&quot;&gt;&lt;img src=&quot;https://user-images.githubusercontent.com/70134676/152630614-58377363-5d81-41bb-8306-e12e7e496b62.png&quot; width=&quot;60%&quot; height=&quot;80%&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;p&gt;전체 모델 구조입니다. E,G는 Encoder과 Decoder를 나타내고, Cx를 통해서 실제 데이터와 Reconstruction 데이터를 판별합니다. 그리고 Cz는 E를 통해 생성된 Latent space와 렌덤으로 생성된 데이터를 가지고 얼마나 잘 맵핑되어 있는 지를 판별합니다. 다음 부터는 각 Phase 별로 어떤 학습이 이루어지는지 볼텐데, 우리가 주목할 것은 학습의 주체가 무엇인지 입니다.&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;p align=&quot;center&quot;&gt;&lt;img src=&quot;https://user-images.githubusercontent.com/70134676/152630638-afe2af36-81d0-4b87-b455-1c5e680989e3.png&quot; width=&quot;60%&quot; height=&quot;80%&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;p&gt;Phase 1 입니다. 실제 데이터와 random latent space가 Generator G를 통과한 재생성 데이터를 판별하는 Cx를 훈련 시키게 됩니다.&lt;/p&gt;

&lt;p&gt;Cx Loss function = critic_score_fake_x - critic_score_valid_x + sqrt[sum(Cx(alpha * x + (1 - alpha) * x_)^2)]&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;p align=&quot;center&quot;&gt;&lt;img src=&quot;https://user-images.githubusercontent.com/70134676/152630664-00de5c7f-d4a4-4f24-897a-707be22821a3.png&quot; width=&quot;60%&quot; height=&quot;80%&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Phase 2 입니다. 여기서는 실제 데이터를 맵핑 시킨 latent space와 random latent space를 비교해 Cz가 어떤 데이터가 원본 데이터의 latent space인지 판별하도록 학습합니다.&lt;/p&gt;

&lt;p&gt;Cz Loss function =critic_score_fake_z - critic_score_valid_z + sqrt[sum(Cx(alpha * z)+ (1 - alpha) * z_)^2)]&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;p align=&quot;center&quot;&gt;&lt;img src=&quot;https://user-images.githubusercontent.com/70134676/152630687-52c09ef8-2511-4a22-815c-8c108a351bfd.png&quot; width=&quot;60%&quot; height=&quot;80%&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Phase 3 입니다. 실제 데이터에서 나온 latent space를 가지고 Generator G로 재생성한 time series squence(gen_x)와, random latent space를 가지고 Genrator G로 생성한 time series squence (fake_x)를 Cx로 판별해 실제 데이터를 구별합니다. 이 때 학습 주체는 Generator E 이고, Cx를 최대한 속이는 쪽으로 학습을 진행합니다. 그리고 원본데이터와 random latent space로 생성한 fake_x 데이터의 Cx Score들을 loss function에 추가했는데, Decoder의 학습 정도에 따라 학습 속도를 변화시킨 것을 볼 수 있습니다.&lt;/p&gt;

&lt;p&gt;Generator E(Encoder) Loss function =mse_loss(x,gen_x)+critic_score_valid_z - critic_score_fake_z&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;p align=&quot;center&quot;&gt;&lt;img src=&quot;https://user-images.githubusercontent.com/70134676/152630709-4aa325f3-01d8-4ba1-bdf0-093727a1b6ff.png&quot; width=&quot;60%&quot; height=&quot;80%&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;p&gt;Phase 4 입니다. 실제 데이터를 Generator E에 통과 시킨 latent space를 Generator G에 통과시킨 gen_x와 실제 데이터 x를 비교합니다. 이 때 원본 데이터의 latent space와 random latent space를 통과시킨 Cz의 score들을 loss function에 추가했는데, Phase3 처럼 Encoder의 학습 정도에 따라 학습 속도를 변화시킨 것을 볼 수 있습니다.&lt;/p&gt;

&lt;p&gt;Generator G(Decoder) Loss function =mse_loss(x,gen_x)+critic_score_valid_x - critic_score_fake_x&lt;/p&gt;

&lt;p&gt;위의 과정은 차례대로 encoder_iteration,decoder_iteration critic_x_iteration, 그리고 critic_z_iteration로 소스코드에 포함되어 있습니다. 코드를 보고 천천히 따라가시면 어떻게 모델이 작동하는 건지 알 수 있습니다.&lt;/p&gt;</content>

      
      
      
      
      

      <author>
          <name>woongE</name>
        
        
      </author>

      

      
        <category term="ai" />
      

      
        <summary type="html">#deeplearning #TadGAN #modelstructure</summary>
      

      
      
    </entry>
  
    <entry>
      <title type="html">Time seriesAnomaly Detection Generative Adversarial Networks-loss function</title>
      <link href="http://localhost:4000/tadgan-lossfunction" rel="alternate" type="text/html" title="Time seriesAnomaly Detection Generative Adversarial Networks-loss function" />
      <published>2021-11-24T07:00:00+09:00</published>
      <updated>2021-11-24T07:00:00+09:00</updated>
      <id>http://localhost:4000/tadgan-lossfunction</id>
      <content type="html" xml:base="http://localhost:4000/tadgan-lossfunction">&lt;p&gt;#TadGAN #lossfunction #AnomalyDetection&lt;/p&gt;

&lt;p&gt;&lt;span class=&quot;table-of-contents-list&quot;&gt;목차 &lt;/span&gt;&lt;/p&gt;
&lt;ul class=&quot;table-of-contents-list&quot;&gt;
    &lt;li&gt;&lt;a href=&quot;./tadgan-modelstructure&quot;&gt;Time seriesAnomaly Detection Generative Adversarial Networks-Model Structure &lt;/a&gt;&lt;/li&gt;  
    &lt;li&gt;&lt;a href=&quot;./tadgan-lossfunction&quot;&gt;Time seriesAnomaly Detection Generative Adversarial Networks-loss function &lt;/a&gt;&lt;/li&gt;  
    &lt;li&gt;&lt;a href=&quot;./tadgan-intro&quot;&gt;Intro to the Time Series Anomaly Detection &lt;/a&gt;&lt;/li&gt;  
    &lt;li&gt;&lt;a href=&quot;./text-ads-filtering&quot;&gt;Text 데이터 광고 필터링을 위한 분류 모델 구축 &lt;/a&gt;&lt;/li&gt;
    &lt;li&gt;&lt;a href=&quot;./bert-sentiment-keyword&quot;&gt;사용자 리뷰 핵심어 추출 및 감성분석 &lt;/a&gt;&lt;/li&gt;
    &lt;li&gt;&lt;a href=&quot;./deeplearning-summary&quot;&gt;Deep Learning - 핵심 개념 &amp;amp; 용어 &lt;/a&gt;&lt;/li&gt;
    &lt;li&gt;&lt;a href=&quot;./versatility&quot;&gt;NLP - 딥러닝 모델의 범용적 활용가능성 분석 &lt;/a&gt;&lt;/li&gt;
    &lt;li&gt;&lt;a href=&quot;./bank-marketing&quot;&gt;Bank marketing 정기예금 가입여부 예측 &lt;/a&gt;&lt;/li&gt;
    &lt;li&gt;&lt;a href=&quot;./roccurve&quot;&gt;ROC curve에 대해 알아보자 &lt;/a&gt;&lt;/li&gt;
    &lt;li&gt;&lt;a href=&quot;./machinelearning-linearregression&quot;&gt;연어의 회귀본능이 아닌 선형회귀에 대해 알아보자 &lt;/a&gt;&lt;/li&gt;  
    
        
    
    


    &lt;!-- &lt;li&gt;&lt;a href=&quot;./proposal&quot;&gt;코스레스토랑에 보내는 제안서 &lt;/a&gt;&lt;/li&gt; --&gt;
    &lt;!-- &lt;li&gt;&lt;a href=&quot;./proposal&quot;&gt;코스레스토랑에 보내는 제안서 &lt;/a&gt;&lt;/li&gt; --&gt;
    &lt;!-- &lt;li&gt;&lt;a href=&quot;./proposal&quot;&gt;코스레스토랑에 보내는 제안서 &lt;/a&gt;&lt;/li&gt; --&gt;
    &lt;!-- &lt;li&gt;&lt;a href=&quot;./proposal&quot;&gt;코스레스토랑에 보내는 제안서 &lt;/a&gt;&lt;/li&gt; --&gt;
    &lt;!-- &lt;li&gt;&lt;a href=&quot;./proposal&quot;&gt;코스레스토랑에 보내는 제안서 &lt;/a&gt;&lt;/li&gt; --&gt;
    &lt;!-- &lt;li&gt;&lt;a href=&quot;./proposal&quot;&gt;코스레스토랑에 보내는 제안서 &lt;/a&gt;&lt;/li&gt; --&gt;
    &lt;!-- &lt;li&gt;&lt;a href=&quot;./proposal&quot;&gt;코스레스토랑에 보내는 제안서 &lt;/a&gt;&lt;/li&gt; --&gt;
    &lt;!-- &lt;li&gt;&lt;a href=&quot;./proposal&quot;&gt;코스레스토랑에 보내는 제안서 &lt;/a&gt;&lt;/li&gt; --&gt;
    &lt;!-- &lt;li&gt;&lt;a href=&quot;./proposal&quot;&gt;코스레스토랑에 보내는 제안서 &lt;/a&gt;&lt;/li&gt; --&gt;
    &lt;!-- &lt;li&gt;&lt;a href=&quot;./proposal&quot;&gt;코스레스토랑에 보내는 제안서 &lt;/a&gt;&lt;/li&gt; --&gt;
    &lt;!-- &lt;li&gt;&lt;a href=&quot;./proposal&quot;&gt;코스레스토랑에 보내는 제안서 &lt;/a&gt;&lt;/li&gt; --&gt;
    &lt;!-- &lt;li&gt;&lt;a href=&quot;./proposal&quot;&gt;코스레스토랑에 보내는 제안서 &lt;/a&gt;&lt;/li&gt; --&gt;
    &lt;!-- &lt;li&gt;&lt;a href=&quot;./proposal&quot;&gt;코스레스토랑에 보내는 제안서 &lt;/a&gt;&lt;/li&gt; --&gt;
    &lt;!-- &lt;li&gt;&lt;a href=&quot;./proposal&quot;&gt;코스레스토랑에 보내는 제안서 &lt;/a&gt;&lt;/li&gt; --&gt;
    &lt;!-- &lt;li&gt;&lt;a href=&quot;./proposal&quot;&gt;코스레스토랑에 보내는 제안서 &lt;/a&gt;&lt;/li&gt;     --&gt;









&lt;!-- 
    &lt;li&gt;&lt;a href=&quot;./test&quot;&gt;시험용 포스팅입니다 - 출력되어라&lt;/a&gt;&lt;/li&gt;
    &lt;li&gt;&lt;a href=&quot;./test&quot;&gt;시험용 포스팅입니다 - 출력되어라&lt;/a&gt;&lt;/li&gt;
    &lt;li&gt;&lt;a href=&quot;./test&quot;&gt;시험용 포스팅입니다 - 출력되어라&lt;/a&gt;&lt;/li&gt;
    &lt;li&gt;&lt;a href=&quot;./test&quot;&gt;시험용 포스팅입니다 - 출력되어라&lt;/a&gt;&lt;/li&gt;
    &lt;li&gt;&lt;a href=&quot;./test&quot;&gt;시험용 포스팅입니다 - 출력되어라&lt;/a&gt;&lt;/li&gt;
    &lt;li&gt;&lt;a href=&quot;./test&quot;&gt;시험용 포스팅입니다 - 출력되어라&lt;/a&gt;&lt;/li&gt;
    &lt;li&gt;&lt;a href=&quot;./test&quot;&gt;시험용 포스팅입니다 - 출력되어라&lt;/a&gt;&lt;/li&gt;
    &lt;li&gt;&lt;a href=&quot;./test&quot;&gt;시험용 포스팅입니다 - 출력되어라&lt;/a&gt;&lt;/li&gt;
    &lt;li&gt;&lt;a href=&quot;./test&quot;&gt;시험용 포스팅입니다 - 출력되어라&lt;/a&gt;&lt;/li&gt;
    &lt;li&gt;&lt;a href=&quot;./test&quot;&gt;시험용 포스팅입니다 - 출력되어라&lt;/a&gt;&lt;/li&gt;
    &lt;li&gt;&lt;a href=&quot;./test&quot;&gt;시험용 포스팅입니다 - 출력되어라&lt;/a&gt;&lt;/li&gt;
    &lt;li&gt;&lt;a href=&quot;./test&quot;&gt;시험용 포스팅입니다 - 출력되어라&lt;/a&gt;&lt;/li&gt;
    &lt;li&gt;&lt;a href=&quot;./test&quot;&gt;시험용 포스팅입니다 - 출력되어라&lt;/a&gt;&lt;/li&gt;
    &lt;li&gt;&lt;a href=&quot;./test&quot;&gt;시험용 포스팅입니다 - 출력되어라&lt;/a&gt;&lt;/li&gt;
    &lt;li&gt;&lt;a href=&quot;./test&quot;&gt;시험용 포스팅입니다 - 출력되어라&lt;/a&gt;&lt;/li&gt;
    &lt;li&gt;&lt;a href=&quot;./test&quot;&gt;시험용 포스팅입니다 - 출력되어라&lt;/a&gt;&lt;/li&gt;
    &lt;li&gt;&lt;a href=&quot;./test&quot;&gt;시험용 포스팅입니다 - 출력되어라&lt;/a&gt;&lt;/li&gt;
    &lt;li&gt;&lt;a href=&quot;./test&quot;&gt;시험용 포스팅입니다 - 출력되어라&lt;/a&gt;&lt;/li&gt;
    &lt;li&gt;&lt;a href=&quot;./test&quot;&gt;시험용 포스팅입니다 - 출력되어라&lt;/a&gt;&lt;/li&gt;
    &lt;li&gt;&lt;a href=&quot;./test&quot;&gt;시험용 포스팅입니다 - 출력되어라&lt;/a&gt;&lt;/li&gt;
    &lt;li&gt;&lt;a href=&quot;./test&quot;&gt;시험용 포스팅입니다 - 출력되어라&lt;/a&gt;&lt;/li&gt;
  --&gt;
    
&lt;/ul&gt;

&lt;h1 id=&quot;time-series-anomaly-detection-generative-adversarial-networks---loss-function&quot;&gt;Time series Anomaly Detection Generative Adversarial Networks - Loss Function&lt;/h1&gt;

&lt;h2 id=&quot;1-gan의-loss-function&quot;&gt;1. GAN의 Loss Function&lt;/h2&gt;

&lt;p&gt;이번 포스트에서는 TadGAN에 대해서 본격적으로 알아보겠습니다.
TadGAN은 이론상으로 훌륭한 방법론이지만 실제로 구현하기가 상당히 까다롭습니다. 이는 GAN을 사용하는 방법론들이 가지는 근본적인 문제점이기도 한데 학습의 어려움이 존재하기 때문입니다. 이에 대한 자세한 이야기는 잘 정리되어있는 포스트가 있어 링크를 첨부합니다.(https://brunch.co.kr/@kakao-it/162)
그래서 Time seriesAnomaly Detection Generative Adversarial Networks(이하 TadGAN) 논문에서는 Loss Function을 차별화하여 이런 어려움을 해결하고자 했습니다.
먼저 GAN에서 기본적으로 사용하는 loss function으로 부터 어떻게 KL divergence, JSD divergence를 유도하는지를 살펴보면 아래와 같습니다.&lt;/p&gt;

&lt;p align=&quot;center&quot;&gt;&lt;img src=&quot;https://user-images.githubusercontent.com/70134676/152629548-e954e27a-a108-44d2-810c-b575c2793558.png&quot; width=&quot;40%&quot; height=&quot;80%&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;p align=&quot;center&quot;&gt;&lt;img src=&quot;https://user-images.githubusercontent.com/70134676/152629661-e51c7121-64b1-4b93-a750-7c64f39aedfd.png&quot; width=&quot;60%&quot; height=&quot;80%&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;p align=&quot;center&quot;&gt;&lt;img src=&quot;https://user-images.githubusercontent.com/70134676/152629689-6be09800-bede-41ee-a819-3e2315e99ba5.png&quot; width=&quot;80%&quot; height=&quot;80%&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;p&gt;KL,JSD를 이용해서 f(x)로 표현된 분포를 g(x)의 분포가 되도록 학습을 시켜야 하는데 그러기 위해서는 아래 그림의 왼쪽 부분은 왼쪽 부분 끼리, 오른쪽 부분은 오른쪽 부분끼리 같아지도록 만들어줘야 합니다. 문제는 두 분포에 대해서 x,T(x) 값이 달라지면 KL,JSD는 같은 기준값을 가진 확률들만 계산하도록 고안된 확률거리 함수라서 기준값이 달라지면 항상 무한대의 값, 혹은 일정한 값만 나오게 되면서 학습을 전혀 할 수 없게 됩니다. 그래서 TadGAN에서는 새로운 loss function을 차용하게 되었는데, 그게 바로 wgan-gp 입니다. wgan-gp는 Wasserstein GAN + gradient panelty를 합친 용어인데 먼저 이 loss function을 알려면 Wasserstein distance가 무엇인지 알아야 할 필요가 있습니다.&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;h2 id=&quot;wasserstein-distance&quot;&gt;Wasserstein distance?&lt;/h2&gt;

&lt;p&gt;KL,JSD가 가지는 한계는 어떤 확률 집합, 혹은 확률함수등이 주어질 때 항상 같은 값에서만 비교가 가능하다는 것입니다. 만약에 다른 값에 대해서 얼마나 거리가 떨어져 있는 지를 확인한다면, KL과 JSD는 아래의 값만 계속 전달하게 됩니다.&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;p align=&quot;center&quot;&gt;&lt;img src=&quot;https://user-images.githubusercontent.com/70134676/152629868-5d5957b2-81cf-449f-8e45-905b7f10001e.png&quot; width=&quot;60%&quot; height=&quot;100%&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;p&gt;하지만 대부분의 무작위로 주어지는 확률 집합, 혹은 확률 함수들은 특수한 경우(값의 범위가 정해져 있는 경우)를 제외하고는 분포가 같을 경우가 거의 없습니다. 값이 정해져 있다면 애초에 관심이 가지 않겠죠. 풀어서 생각해보면 우리는 사과를 다른 모양의 사과로 변환시키는 것(값의 분포가 같은 경우)보다는 사과를 오렌지, 혹은 배등으로 변환시키는 것(값의 분포가 다를 경우)에 더 관심이 있을 것 입니다.&lt;/p&gt;

&lt;p&gt;우리는 다른 분포를 가진 확률함수에 대해 KL과 JSD가 유의미한 결과를 주지 않는다는 것을 알았습니다. 그래서 TadGAN 논문에서는 다른 분포를 가지는 확률 함수들에 대해서 값을 비교하기 위해 Wasserstein distance를 채택하게 됩니다. Wasserstein distance는 다른 말로 EMD(Eearth Mover distance)라고도 하는데, 쉽게 말해 Euclidean distance라고 생각하시면 됩니다. 이 확률 거리 측정 방법은 KL과 JSD와는 확실히 다릅니다. KL과 JSD가 다른 값에 대해서 서로 같은 확률을 가질 때 무한 대의 값이나 일정값만을 출력한다면, Wasserstein distance는 다른 분포에 대한 거리 정보까지 담고 있습니다.&lt;/p&gt;

&lt;p&gt;조금 더 쉽게 설명해 보겠습니다. 예를 들어서 X=[2,2,2,2,3,4,4,5], Y=[3,3,4,4,6,6,6,6]이 존재한다고 하겠습니다. 이 때 X와 Y가 각각 원소를 뽑게 될 때 두 집합의 확률이 얼마나 거리가 떨어져 있는 지에 대해서 알아보려고 합니다. 바로 전체를 비교하면 어려울 수 있으니 각 집합에서 X(2)와 Y(6)이 얼마나 떨어져 있는지 한번 비교해 보도록 하겠습니다.&lt;/p&gt;

&lt;p&gt;각 원소들이 뽑힐 확률이 1/n이라고 했을 때, X가 2를 뽑을 확률은 1/2이고, Y가 6을 뽑을 확률도 역시 1/2 입니다. 그리고 X가 2를 뽑을 확률과 Y가 6을 뽑을 확률에 대한 거리를 계산한다면, 두 경우에 대해서 확률이 동일하기 때문에 위에서 저희가 이미 구한 것 처럼 KL은 inf, JSD는 log2 가 나오지만 Wasserstein distance는 다음과 같이 계산됩니다.&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;p align=&quot;center&quot;&gt;&lt;img src=&quot;https://user-images.githubusercontent.com/70134676/152630165-0a8631cb-f3ed-4fbb-bd2e-e625f9be7e03.png&quot; width=&quot;40%&quot; height=&quot;100%&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;p&gt;즉 확률은 같아도 2를 1/2로 뽑을 확률과 6을 1/2로 뽑을 확률은 정확히 4 만큼의 거리를 가진다는 것을 알 수 있습니다.&lt;/p&gt;

&lt;p&gt;지금까지 우리는 wasserstein distance를 어떻게 구하는 지에 대해 알아봤습니다. 하지만 이 것은 X(2) -&amp;gt; Y(6) 로 갈 때만의 이야기를 한 것입니다. 우리가 알고 싶은 것은 X와 Y에 대한 확률 거리가 얼마나 떨어져 있는지를 어떻게 하면 정량적으로 나타낼 수 있는가? 입니다.&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;p align=&quot;center&quot;&gt;&lt;img src=&quot;https://user-images.githubusercontent.com/70134676/152630205-cfd4d08d-e1b0-4e77-a546-ebe0e8f9986e.png&quot; width=&quot;80%&quot; height=&quot;100%&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;p&gt;X,Y에 대해서 각 원소들이 뽑힐 확률이 uniform(전부) 균일)이라고 가정할 때, 각 원소들이 뽑힐 확률은 모두 1/n으로 동일 합니다. 그래서 각 원소들이 뽑힐 확률은 Y축에 표시되어 있고, 그 원소의 값들은 X축에 표시되어 있습니다. 그리고 우리는 이제 X를 Y처럼 만들면 되는데, 만드는 방식은 블록 쌓기와 같습니다. 이 블록은 모든 블록의 크기를 더하면 1이 되는 블록입니다. 하지만 이렇게 되면 경우의 수가 너무 많이 발생합니다. 예를 들면 X(2)를 X(6)에 전부 다 옮겨 Y(6) 처럼 만들어주는 방법이 있습니다. 그것도 아니라면 X(3),X(4),X(5)를 X(6)으로 모두 옮긴 다음, X(2)를 잘게 쪼개서 X(3),X(4),X(5)를 채워주는 방법도 있습니다. 이렇게 여러가지의 방법들이 존재하기 때문에 Wasserstein distance에는 제약 조건이 있습니다. 어떻게 옮기든 상관은 없는데, 그 중 최소비용이 드는 방법을 두 함수의 거리로 생각하겠다는 것입니다. 비용 측정은 다음과 같이 합니다.&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;p align=&quot;center&quot;&gt;&lt;img src=&quot;https://user-images.githubusercontent.com/70134676/152630229-9a370792-c94c-4cfa-a8c2-0e327cf17917.png&quot; width=&quot;80%&quot; height=&quot;100%&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;p&gt;이 계산식은 EM distance, Wasserstein-1 이라고 불리는 거리계산 방법입니다. 언뜻 보면 어려워 보일 수 있지만 집합 r과 집합 g에서 사건이 동시에 발생할 때 가능한 모든 확률들 중에서,L2 norm 공식을 적용했을 때 임의로 선택된 x,y의 확률의 차이와 값의 차이가 가장 작은 것을 Wasserstein distance라고 정하기로 한 것입니다. 그렇다면 이제 우리는 우리가 처음에 말했던, “왼쪽 부분은 왼쪽 부분 끼리, 오른쪽 부분은 오른쪽 부분끼리 같아지도록 만들어줘야”라는 의미가 무엇인지 생각해볼 수 있습니다. 물론 이렇게 확률을 변화시키지 않아도 되지만 최대한 근접하게 확률을 변화해야만 Wasserstein distance를 구하기 수월해 집니다.&lt;/p&gt;

&lt;p&gt;즉, Wasserstein distance는 어떤 사건이 동시에 발생했을 때, 두 사건이 가지는 확률과 분포가 달라지는 경우가 발생합니다. 분포가 같고 확률만 다르다면 기존의 KL, JSD로 충분히 계산이 가능하겠지만, 분포가 다르면 KL은 무한대로 발산하고 JSD는 log2의 값만 나타내게 됩니다. 하지만 Wasserstein distance는 분포가 다르더라도 분포의 차이, 그리고 확률의 차이를 동시에 고려합니다. 그래서 분포가 다른 두 확률에 대해서 얼마나 차이가 나는 지를 구별하여 나타낼 수 있는 거리 함수입니다.&lt;/p&gt;

&lt;h2 id=&quot;wgan-gp&quot;&gt;wgan-gp?&lt;/h2&gt;

&lt;p&gt;이제까지 기본적인 GAN loss function의 한계를 알아보았고, 왜 KL, JSD divergnece를 사용해서 학습을 하면 안되는지, 그리고 Wasserstein distance라는 확률거리 함수를 사용해서 어떻게 이를 극복하는지를 했는지를 알아보았습니다. 실제로 tadgan 논문의 5쪽을 보면 일반적으로 GAN에서 사용하는 loss function과, 저자가 학습이 잘 되지 않는 이슈 때문에 wgan-gp를 사용하겠다는 내용이 나옵니다. 그리고 Wasserstein loss는 1-Lipschitz continuous function 조건(llf(x1) − f(x2)ll ≤ Klx1 − x2l, ∀x1, x2 ∈ dom f)을 만족 시키기 때문에 어느정도 기울기에 대한 변화를 제어할 수 있습니다. 여기까지 보면보면 Wasserstein distance를 이용한 Wasserstein loss로 충분해보이지만, 사실은 그렇지 않습니다. Improved Training of Wasserstein GANs:https://arxiv.org/abs/1704.00028)논문을 보면 1-Lipschitz 조건을 만족하긴 하지만, 이것 역시 아직 gredient 변화에 대해서 적절히 반응하지 못한다는 것을 확인할 수 있습니다. 그래서 gradient penalty를 추가 해서 gradient의 변화를 조절합니다.&lt;/p&gt;

&lt;h2 id=&quot;conclusion&quot;&gt;Conclusion&lt;/h2&gt;

&lt;p&gt;loss function을 사용할 때, 기존의 GAN에 적용하던 확률거리함수(KL, JSD) 보다는 Wasserstein distance를 사용해야 모델이 학습을 잘 하게 되는데, 그 이유는 KL, JSD 발산은 서로 다른 값을 뽑을 확률에 대한 계산을 하게 되면 특정 값이나 무한대의 값을 측정하기 때문에 학습이 제대로 이루어지지 않습니다. 하지만 Wasserstein distance는 서로 다른 값을 뽑을 확률에 대해서 그 확률에 상관 없이 가변적인 값을 도출해 내기 때문에 손실 함수의 거리 계산으로 적합하다는 내용 입니다. 이 Wasserstein distance를 사용한 GAN을 wgan이라고 하는데, 이 wgan도 조금 문제가 있어서 wgan-gp라는 손실 함수가 새로 나오게 되었습니다.
다음 포스트에서는 TadGAN의 모델구조에 대해서 알아보겠습니다.&lt;/p&gt;</content>

      
      
      
      
      

      <author>
          <name>woongE</name>
        
        
      </author>

      

      
        <category term="ai" />
      

      
        <summary type="html">#TadGAN #lossfunction #AnomalyDetection</summary>
      

      
      
    </entry>
  
    <entry>
      <title type="html">Intro to the Time Series Anomaly Detection</title>
      <link href="http://localhost:4000/tadgan-intro" rel="alternate" type="text/html" title="Intro to the Time Series Anomaly Detection" />
      <published>2021-11-20T07:00:00+09:00</published>
      <updated>2021-11-20T07:00:00+09:00</updated>
      <id>http://localhost:4000/tadgan-intro</id>
      <content type="html" xml:base="http://localhost:4000/tadgan-intro">&lt;p&gt;#TadGAN #Timeseries #AnomalyDetection&lt;/p&gt;

&lt;p&gt;&lt;span class=&quot;table-of-contents-list&quot;&gt;목차 &lt;/span&gt;&lt;/p&gt;
&lt;ul class=&quot;table-of-contents-list&quot;&gt;
    &lt;li&gt;&lt;a href=&quot;./tadgan-modelstructure&quot;&gt;Time seriesAnomaly Detection Generative Adversarial Networks-Model Structure &lt;/a&gt;&lt;/li&gt;  
    &lt;li&gt;&lt;a href=&quot;./tadgan-lossfunction&quot;&gt;Time seriesAnomaly Detection Generative Adversarial Networks-loss function &lt;/a&gt;&lt;/li&gt;  
    &lt;li&gt;&lt;a href=&quot;./tadgan-intro&quot;&gt;Intro to the Time Series Anomaly Detection &lt;/a&gt;&lt;/li&gt;  
    &lt;li&gt;&lt;a href=&quot;./text-ads-filtering&quot;&gt;Text 데이터 광고 필터링을 위한 분류 모델 구축 &lt;/a&gt;&lt;/li&gt;
    &lt;li&gt;&lt;a href=&quot;./bert-sentiment-keyword&quot;&gt;사용자 리뷰 핵심어 추출 및 감성분석 &lt;/a&gt;&lt;/li&gt;
    &lt;li&gt;&lt;a href=&quot;./deeplearning-summary&quot;&gt;Deep Learning - 핵심 개념 &amp;amp; 용어 &lt;/a&gt;&lt;/li&gt;
    &lt;li&gt;&lt;a href=&quot;./versatility&quot;&gt;NLP - 딥러닝 모델의 범용적 활용가능성 분석 &lt;/a&gt;&lt;/li&gt;
    &lt;li&gt;&lt;a href=&quot;./bank-marketing&quot;&gt;Bank marketing 정기예금 가입여부 예측 &lt;/a&gt;&lt;/li&gt;
    &lt;li&gt;&lt;a href=&quot;./roccurve&quot;&gt;ROC curve에 대해 알아보자 &lt;/a&gt;&lt;/li&gt;
    &lt;li&gt;&lt;a href=&quot;./machinelearning-linearregression&quot;&gt;연어의 회귀본능이 아닌 선형회귀에 대해 알아보자 &lt;/a&gt;&lt;/li&gt;  
    
        
    
    


    &lt;!-- &lt;li&gt;&lt;a href=&quot;./proposal&quot;&gt;코스레스토랑에 보내는 제안서 &lt;/a&gt;&lt;/li&gt; --&gt;
    &lt;!-- &lt;li&gt;&lt;a href=&quot;./proposal&quot;&gt;코스레스토랑에 보내는 제안서 &lt;/a&gt;&lt;/li&gt; --&gt;
    &lt;!-- &lt;li&gt;&lt;a href=&quot;./proposal&quot;&gt;코스레스토랑에 보내는 제안서 &lt;/a&gt;&lt;/li&gt; --&gt;
    &lt;!-- &lt;li&gt;&lt;a href=&quot;./proposal&quot;&gt;코스레스토랑에 보내는 제안서 &lt;/a&gt;&lt;/li&gt; --&gt;
    &lt;!-- &lt;li&gt;&lt;a href=&quot;./proposal&quot;&gt;코스레스토랑에 보내는 제안서 &lt;/a&gt;&lt;/li&gt; --&gt;
    &lt;!-- &lt;li&gt;&lt;a href=&quot;./proposal&quot;&gt;코스레스토랑에 보내는 제안서 &lt;/a&gt;&lt;/li&gt; --&gt;
    &lt;!-- &lt;li&gt;&lt;a href=&quot;./proposal&quot;&gt;코스레스토랑에 보내는 제안서 &lt;/a&gt;&lt;/li&gt; --&gt;
    &lt;!-- &lt;li&gt;&lt;a href=&quot;./proposal&quot;&gt;코스레스토랑에 보내는 제안서 &lt;/a&gt;&lt;/li&gt; --&gt;
    &lt;!-- &lt;li&gt;&lt;a href=&quot;./proposal&quot;&gt;코스레스토랑에 보내는 제안서 &lt;/a&gt;&lt;/li&gt; --&gt;
    &lt;!-- &lt;li&gt;&lt;a href=&quot;./proposal&quot;&gt;코스레스토랑에 보내는 제안서 &lt;/a&gt;&lt;/li&gt; --&gt;
    &lt;!-- &lt;li&gt;&lt;a href=&quot;./proposal&quot;&gt;코스레스토랑에 보내는 제안서 &lt;/a&gt;&lt;/li&gt; --&gt;
    &lt;!-- &lt;li&gt;&lt;a href=&quot;./proposal&quot;&gt;코스레스토랑에 보내는 제안서 &lt;/a&gt;&lt;/li&gt; --&gt;
    &lt;!-- &lt;li&gt;&lt;a href=&quot;./proposal&quot;&gt;코스레스토랑에 보내는 제안서 &lt;/a&gt;&lt;/li&gt; --&gt;
    &lt;!-- &lt;li&gt;&lt;a href=&quot;./proposal&quot;&gt;코스레스토랑에 보내는 제안서 &lt;/a&gt;&lt;/li&gt; --&gt;
    &lt;!-- &lt;li&gt;&lt;a href=&quot;./proposal&quot;&gt;코스레스토랑에 보내는 제안서 &lt;/a&gt;&lt;/li&gt;     --&gt;









&lt;!-- 
    &lt;li&gt;&lt;a href=&quot;./test&quot;&gt;시험용 포스팅입니다 - 출력되어라&lt;/a&gt;&lt;/li&gt;
    &lt;li&gt;&lt;a href=&quot;./test&quot;&gt;시험용 포스팅입니다 - 출력되어라&lt;/a&gt;&lt;/li&gt;
    &lt;li&gt;&lt;a href=&quot;./test&quot;&gt;시험용 포스팅입니다 - 출력되어라&lt;/a&gt;&lt;/li&gt;
    &lt;li&gt;&lt;a href=&quot;./test&quot;&gt;시험용 포스팅입니다 - 출력되어라&lt;/a&gt;&lt;/li&gt;
    &lt;li&gt;&lt;a href=&quot;./test&quot;&gt;시험용 포스팅입니다 - 출력되어라&lt;/a&gt;&lt;/li&gt;
    &lt;li&gt;&lt;a href=&quot;./test&quot;&gt;시험용 포스팅입니다 - 출력되어라&lt;/a&gt;&lt;/li&gt;
    &lt;li&gt;&lt;a href=&quot;./test&quot;&gt;시험용 포스팅입니다 - 출력되어라&lt;/a&gt;&lt;/li&gt;
    &lt;li&gt;&lt;a href=&quot;./test&quot;&gt;시험용 포스팅입니다 - 출력되어라&lt;/a&gt;&lt;/li&gt;
    &lt;li&gt;&lt;a href=&quot;./test&quot;&gt;시험용 포스팅입니다 - 출력되어라&lt;/a&gt;&lt;/li&gt;
    &lt;li&gt;&lt;a href=&quot;./test&quot;&gt;시험용 포스팅입니다 - 출력되어라&lt;/a&gt;&lt;/li&gt;
    &lt;li&gt;&lt;a href=&quot;./test&quot;&gt;시험용 포스팅입니다 - 출력되어라&lt;/a&gt;&lt;/li&gt;
    &lt;li&gt;&lt;a href=&quot;./test&quot;&gt;시험용 포스팅입니다 - 출력되어라&lt;/a&gt;&lt;/li&gt;
    &lt;li&gt;&lt;a href=&quot;./test&quot;&gt;시험용 포스팅입니다 - 출력되어라&lt;/a&gt;&lt;/li&gt;
    &lt;li&gt;&lt;a href=&quot;./test&quot;&gt;시험용 포스팅입니다 - 출력되어라&lt;/a&gt;&lt;/li&gt;
    &lt;li&gt;&lt;a href=&quot;./test&quot;&gt;시험용 포스팅입니다 - 출력되어라&lt;/a&gt;&lt;/li&gt;
    &lt;li&gt;&lt;a href=&quot;./test&quot;&gt;시험용 포스팅입니다 - 출력되어라&lt;/a&gt;&lt;/li&gt;
    &lt;li&gt;&lt;a href=&quot;./test&quot;&gt;시험용 포스팅입니다 - 출력되어라&lt;/a&gt;&lt;/li&gt;
    &lt;li&gt;&lt;a href=&quot;./test&quot;&gt;시험용 포스팅입니다 - 출력되어라&lt;/a&gt;&lt;/li&gt;
    &lt;li&gt;&lt;a href=&quot;./test&quot;&gt;시험용 포스팅입니다 - 출력되어라&lt;/a&gt;&lt;/li&gt;
    &lt;li&gt;&lt;a href=&quot;./test&quot;&gt;시험용 포스팅입니다 - 출력되어라&lt;/a&gt;&lt;/li&gt;
    &lt;li&gt;&lt;a href=&quot;./test&quot;&gt;시험용 포스팅입니다 - 출력되어라&lt;/a&gt;&lt;/li&gt;
  --&gt;
    
&lt;/ul&gt;

&lt;h1 id=&quot;time-series-anomaly-detection-generative-adversarial-networks&quot;&gt;Time series Anomaly Detection Generative Adversarial Networks&lt;/h1&gt;

&lt;h2 id=&quot;1-intro&quot;&gt;1. Intro&lt;/h2&gt;
&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;p align=&quot;center&quot;&gt;&lt;img src=&quot;https://user-images.githubusercontent.com/70134676/150893257-5d606456-8c67-4801-84e4-7a82dd906dd9.png&quot; width=&quot;80%&quot; height=&quot;100%&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;p&gt;이번에 TadGAN: Time Series Anomaly Detection Using Generative Adversarial Networks
논문을 읽고 공부하면서 알게 된 내용을 정리하는 포스트입니다. 한 포스트에 담기에는 양이 너무 많을 것 같아 2~3개 정도의 시리즈로 나누어 정리해볼 생각입니다.
TadGAN은 machine learning 중 비지도 학습 방식입니다. 입력되는 시계열 데이터와 최대한 유사한 데이터를 모방하도록 학습하고, Threshold에 따라 이상치를 판단하는 이상 탐지 기법입니다. 비지도 학습 이상 탐지 분야는 다양한 목적을 위해 활용되고 있는데 다음과 같이 열거할 수 있습니다.&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;h3 id=&quot;데이터-품질-향상&quot;&gt;데이터 품질 향상&lt;/h3&gt;
&lt;p&gt;스탠포드 대학의 Andrew ng 교수는 모델을 튜닝하는 것 못지않게 모델의 학습에 이용되는 데이터의 품질을 높이는 것이 모델 성능 향상에 주요한 영향을 미친다고 말했습니다. 따라서 신뢰할 수 있는 데이터를 수집해야 하는데 이 과정에서 데이터 품질을 높이기 위해 필터링하는 작업에 이상치 탐지 모델을 적용하려는 노력이 이어지고 있습니다.&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;h3 id=&quot;비지도-학습방법의-필요성label의-부재&quot;&gt;비지도 학습방법의 필요성(Label의 부재)&lt;/h3&gt;
&lt;p&gt;현실에 존재하는 데이터의 대부분은 독립변수에 따른 종속변수인 Y Label을 알 수 없는 경우가 많습니다. 이러한 데이터에 Labeling 작업을 통해 지도학습방식으로 모델을 학습시킬 수도 있지만 굉장히 많은 비용(시간, 돈)이 소모될 뿐만 아니라 Labling을 하는데 필요한 기준을 세우는 것도 어렵습니다. 그래서 결국에는 비지도학습에 의한 이상치 탐지방법이 필요합니다.&lt;/p&gt;

&lt;p&gt;이상치 탐지를 하는 방법은 여러가지가 존재하며 저는 그 중에서도 논문에서 소개한 세가지 방법론을 본격적으로 TadGAN을 얘기하기 전에 소개 하려고 합니다.  &lt;br /&gt;
&lt;br /&gt;&lt;/p&gt;

&lt;h3 id=&quot;1-proximity-based-methods&quot;&gt;1. Proximity-based methods&lt;/h3&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;p align=&quot;center&quot;&gt;&lt;img src=&quot;https://user-images.githubusercontent.com/70134676/150920567-fb0236b3-1a25-41b5-9521-3c1d2273f29f.png&quot; width=&quot;80%&quot; height=&quot;100%&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;p&gt;첫번째는 근접도 기반 방법입니다. 대표적으로 clutering 기법이 존재합니다. 각 데이터 요소들간의 거리를 계산하고 이를 바탕으로 근접도를 판단을 합니다. 근접도가 커지면 커질수록 멀리 있다는 뜻이기 때문에 여러가지 제약조건을 통해서 outlier을 결정할 수 있습니다.&lt;br /&gt;
&lt;br /&gt;&lt;/p&gt;

&lt;h3 id=&quot;2-predict-based-methods&quot;&gt;2. Predict-based methods&lt;/h3&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;p align=&quot;center&quot;&gt;&lt;img src=&quot;https://user-images.githubusercontent.com/70134676/150920713-9916bf4f-c3f3-4cef-b76f-c06c1a9a21ca.png&quot; width=&quot;80%&quot; height=&quot;100%&quot; /&gt;&lt;/p&gt;

&lt;p&gt;두번째는 예측 기반 방법입니다. 전통적인 통계방법인 ARIMA가 가장 대표적입니다. 시점 t-1까지의 데이터를 활용해서 시점 t를 예측합니다. 그리고 실제 값이 예측값과 얼마나 다른지를 보고 이상치인지 아닌지를 판별합니다. 세 가지 방법중에서는 난이도가 가장 높은 편이라고 할 수 있습니다.&lt;br /&gt;
&lt;br /&gt;&lt;/p&gt;

&lt;h3 id=&quot;3-reconstruction-based-methods&quot;&gt;3. Reconstruction-based methods&lt;/h3&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;p align=&quot;center&quot;&gt;&lt;img src=&quot;https://user-images.githubusercontent.com/70134676/150920953-f5d3a489-6ac8-4a36-a207-c2201c291ff1.png&quot; width=&quot;80%&quot; height=&quot;100%&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;p&gt;재생성 기반 방법입니다. Auto Encoder가 가장 대표적입니다. 여러개의 특징(다변량 혹은 시계열데이터)을 Encoder을 통해 latent space로 압축시키고, 이후 Decoder을 통과해 원래의 데이터로 복원하게 되는데 이때 Auto Encoder는 그 데이터의 특징을 학습한 상태이므로 이상치가 포함된 데이터라면 복원이 잘 되어있지 않을 것입니다. 이러한 원리로 복원된 데이터와 원본 데이터의 차이를 비교해서 이상치를 추정하게 됩니다.&lt;br /&gt;
&lt;br /&gt;&lt;/p&gt;

&lt;h2 id=&quot;그래서-tadgan은&quot;&gt;그래서 TadGAN은…&lt;/h2&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;p&gt;위의 방법중에서 TadGAN은 마지막 방법인 Reconsturction-based methods를 통해 이상치를 판별합니다. 조금 더 나아가서 TadGAN은 이런 reconsturction method를 CycleGAN에 사용된 기법을 이용하는 모델입니다. 먼저 CycleGAN에 대해서 이야기해 보자면 CycleGAN은 최초로 등장했을 때 이미지에 대한 변환을 주로 담당했는데, 원본 데이터의 형태를 유지하면서 이미지의 질감이나 스타일을 잘 바꿔줍니다. 예를들면, 초원에서 뛰어다니고 있는 말 사진에서 말을 얼룩말로 바꿔줄 수 있습니다. 중요한 것은 CycleGAN은 이미지의 형태가 완전히 다르면 제대로 기능을 하지 못한다는 것입니다.&lt;br /&gt;
&lt;br /&gt;&lt;/p&gt;

&lt;p align=&quot;center&quot;&gt;&lt;img src=&quot;https://user-images.githubusercontent.com/70134676/150922003-011f01b4-d479-4909-a577-382459d39f9b.png&quot; width=&quot;80%&quot; height=&quot;100%&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;p&gt;예를 들면 말에 사람이 타고 있다면 모델은 사람도 말로 인식해서 얼룩말 무늬를 씌워버립니다. 이 것은 이미지를 학습하는 입장에서는 별로 달갑지 않겠지만 시계열 데이터가 속해 있는지 아닌지를 판별할 때는 굉장히 유용합니다. CycleGAN은 AutoEncoder와 굉장히 유사하지만 GAN으로 학습하기 때문에 조금더 폭넓은 데이터들, 그러니까 초원과 말(들)이 존재하는 모든 사진을 하나의 필드로 인식합니다. 이것은 저희가 TadGAN에서 하려는 것과 굉장히 유사합니다. 다양하지만 하나의 도메인에서 비롯된 시계열 데이터를 학습시키면 그 모델은 특정 도메인에 대해서만 좋은 복원률을 보이고, 아니라면 완전히 이상한(ex. 얼룩무늬로 이루어진 사람)복원을 하게 될 것입니다.&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;p align=&quot;center&quot;&gt;&lt;img src=&quot;https://user-images.githubusercontent.com/70134676/150922354-27418052-342f-41b8-8863-bc29a24c2134.png&quot; width=&quot;80%&quot; height=&quot;100%&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;p align=&quot;center&quot;&gt;&lt;img src=&quot;https://user-images.githubusercontent.com/70134676/150922442-52fba2ab-6640-468c-9dd5-c8553dcdebec.png&quot; width=&quot;80%&quot; height=&quot;100%&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;p&gt;그래프를 보면 조금 더 쉽게 비교가 가능합니다. 처음 볼 수 있는 그림은 cyclegan의 원본을 시계열 데이터에 맞춰서 변경한 것입니다. 원본 시계열과 재생성된 시계열을 검사하는 Cx가 추가된 것 말고는 굉장히 흡사하게 보입니다. 우리가 여기서 하고 싶은 것은 명확합니다. 앞서 설명드렸던 cycleGAN의 아이디어를 TadGAN으로 가져와서 생각해보면 원본 이미지는 우리가 가지고 있는 시계열 데이터를 의미합니다. 우리가 가장 중요하게 생각하는 것은 시계열 데이터의 다양한 형태를 모델이 익히는 것인데 이미지로 치환해서 생각해보자면 우리가 관심 있는 것은 이미지의 형태이지 질감이나 스타일이 아닙니다. 질감이나 스타일은 여러가지 조건에 따라 달라질 수 있지만, 형태가 달라져버리면(말과 초원만 있는 사진을 학습한 모델에 말을 탄 사람이 초원에 있는 사진을 물어본 경우) 굉장히 다른 형태(얼룩무늬 사람)로 복원을 하기 때문입니다. TadGAN은 바로 이런 점에 착안을 해서 고안된 모델입니다.&lt;/p&gt;

&lt;p&gt;이번 포스트에서는 이정도로 하고 다음 포스트부터 TadGAN의 구조나 원리 등에 대해 제가 이해한바를 적어볼 예정입니다. 제가 이해한 바가 틀릴 수도 있으니 잘못알고 있는 것이 있다면 지적해주시면 감사하겠습니다.&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;p align=&quot;center&quot;&gt;&lt;img src=&quot;https://user-images.githubusercontent.com/70134676/150922354-27418052-342f-41b8-8863-bc29a24c2134.png&quot; width=&quot;80%&quot; height=&quot;100%&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;</content>

      
      
      
      
      

      <author>
          <name>woongE</name>
        
        
      </author>

      

      
        <category term="ai" />
      

      
        <summary type="html">#TadGAN #Timeseries #AnomalyDetection</summary>
      

      
      
    </entry>
  
    <entry>
      <title type="html">Text 데이터 광고 필터링을 위한 분류 모델 구축</title>
      <link href="http://localhost:4000/text-ads-filtering" rel="alternate" type="text/html" title="Text 데이터 광고 필터링을 위한 분류 모델 구축" />
      <published>2021-04-13T07:00:00+09:00</published>
      <updated>2021-04-13T07:00:00+09:00</updated>
      <id>http://localhost:4000/text-ads-filtering</id>
      <content type="html" xml:base="http://localhost:4000/text-ads-filtering">&lt;p&gt;#deeplearning #NLP #BERT #classification #filtering&lt;/p&gt;

&lt;p&gt;&lt;span class=&quot;table-of-contents-list&quot;&gt;목차 &lt;/span&gt;&lt;/p&gt;
&lt;ul class=&quot;table-of-contents-list&quot;&gt;
    &lt;li&gt;&lt;a href=&quot;./tadgan-modelstructure&quot;&gt;Time seriesAnomaly Detection Generative Adversarial Networks-Model Structure &lt;/a&gt;&lt;/li&gt;  
    &lt;li&gt;&lt;a href=&quot;./tadgan-lossfunction&quot;&gt;Time seriesAnomaly Detection Generative Adversarial Networks-loss function &lt;/a&gt;&lt;/li&gt;  
    &lt;li&gt;&lt;a href=&quot;./tadgan-intro&quot;&gt;Intro to the Time Series Anomaly Detection &lt;/a&gt;&lt;/li&gt;  
    &lt;li&gt;&lt;a href=&quot;./text-ads-filtering&quot;&gt;Text 데이터 광고 필터링을 위한 분류 모델 구축 &lt;/a&gt;&lt;/li&gt;
    &lt;li&gt;&lt;a href=&quot;./bert-sentiment-keyword&quot;&gt;사용자 리뷰 핵심어 추출 및 감성분석 &lt;/a&gt;&lt;/li&gt;
    &lt;li&gt;&lt;a href=&quot;./deeplearning-summary&quot;&gt;Deep Learning - 핵심 개념 &amp;amp; 용어 &lt;/a&gt;&lt;/li&gt;
    &lt;li&gt;&lt;a href=&quot;./versatility&quot;&gt;NLP - 딥러닝 모델의 범용적 활용가능성 분석 &lt;/a&gt;&lt;/li&gt;
    &lt;li&gt;&lt;a href=&quot;./bank-marketing&quot;&gt;Bank marketing 정기예금 가입여부 예측 &lt;/a&gt;&lt;/li&gt;
    &lt;li&gt;&lt;a href=&quot;./roccurve&quot;&gt;ROC curve에 대해 알아보자 &lt;/a&gt;&lt;/li&gt;
    &lt;li&gt;&lt;a href=&quot;./machinelearning-linearregression&quot;&gt;연어의 회귀본능이 아닌 선형회귀에 대해 알아보자 &lt;/a&gt;&lt;/li&gt;  
    
        
    
    


    &lt;!-- &lt;li&gt;&lt;a href=&quot;./proposal&quot;&gt;코스레스토랑에 보내는 제안서 &lt;/a&gt;&lt;/li&gt; --&gt;
    &lt;!-- &lt;li&gt;&lt;a href=&quot;./proposal&quot;&gt;코스레스토랑에 보내는 제안서 &lt;/a&gt;&lt;/li&gt; --&gt;
    &lt;!-- &lt;li&gt;&lt;a href=&quot;./proposal&quot;&gt;코스레스토랑에 보내는 제안서 &lt;/a&gt;&lt;/li&gt; --&gt;
    &lt;!-- &lt;li&gt;&lt;a href=&quot;./proposal&quot;&gt;코스레스토랑에 보내는 제안서 &lt;/a&gt;&lt;/li&gt; --&gt;
    &lt;!-- &lt;li&gt;&lt;a href=&quot;./proposal&quot;&gt;코스레스토랑에 보내는 제안서 &lt;/a&gt;&lt;/li&gt; --&gt;
    &lt;!-- &lt;li&gt;&lt;a href=&quot;./proposal&quot;&gt;코스레스토랑에 보내는 제안서 &lt;/a&gt;&lt;/li&gt; --&gt;
    &lt;!-- &lt;li&gt;&lt;a href=&quot;./proposal&quot;&gt;코스레스토랑에 보내는 제안서 &lt;/a&gt;&lt;/li&gt; --&gt;
    &lt;!-- &lt;li&gt;&lt;a href=&quot;./proposal&quot;&gt;코스레스토랑에 보내는 제안서 &lt;/a&gt;&lt;/li&gt; --&gt;
    &lt;!-- &lt;li&gt;&lt;a href=&quot;./proposal&quot;&gt;코스레스토랑에 보내는 제안서 &lt;/a&gt;&lt;/li&gt; --&gt;
    &lt;!-- &lt;li&gt;&lt;a href=&quot;./proposal&quot;&gt;코스레스토랑에 보내는 제안서 &lt;/a&gt;&lt;/li&gt; --&gt;
    &lt;!-- &lt;li&gt;&lt;a href=&quot;./proposal&quot;&gt;코스레스토랑에 보내는 제안서 &lt;/a&gt;&lt;/li&gt; --&gt;
    &lt;!-- &lt;li&gt;&lt;a href=&quot;./proposal&quot;&gt;코스레스토랑에 보내는 제안서 &lt;/a&gt;&lt;/li&gt; --&gt;
    &lt;!-- &lt;li&gt;&lt;a href=&quot;./proposal&quot;&gt;코스레스토랑에 보내는 제안서 &lt;/a&gt;&lt;/li&gt; --&gt;
    &lt;!-- &lt;li&gt;&lt;a href=&quot;./proposal&quot;&gt;코스레스토랑에 보내는 제안서 &lt;/a&gt;&lt;/li&gt; --&gt;
    &lt;!-- &lt;li&gt;&lt;a href=&quot;./proposal&quot;&gt;코스레스토랑에 보내는 제안서 &lt;/a&gt;&lt;/li&gt;     --&gt;









&lt;!-- 
    &lt;li&gt;&lt;a href=&quot;./test&quot;&gt;시험용 포스팅입니다 - 출력되어라&lt;/a&gt;&lt;/li&gt;
    &lt;li&gt;&lt;a href=&quot;./test&quot;&gt;시험용 포스팅입니다 - 출력되어라&lt;/a&gt;&lt;/li&gt;
    &lt;li&gt;&lt;a href=&quot;./test&quot;&gt;시험용 포스팅입니다 - 출력되어라&lt;/a&gt;&lt;/li&gt;
    &lt;li&gt;&lt;a href=&quot;./test&quot;&gt;시험용 포스팅입니다 - 출력되어라&lt;/a&gt;&lt;/li&gt;
    &lt;li&gt;&lt;a href=&quot;./test&quot;&gt;시험용 포스팅입니다 - 출력되어라&lt;/a&gt;&lt;/li&gt;
    &lt;li&gt;&lt;a href=&quot;./test&quot;&gt;시험용 포스팅입니다 - 출력되어라&lt;/a&gt;&lt;/li&gt;
    &lt;li&gt;&lt;a href=&quot;./test&quot;&gt;시험용 포스팅입니다 - 출력되어라&lt;/a&gt;&lt;/li&gt;
    &lt;li&gt;&lt;a href=&quot;./test&quot;&gt;시험용 포스팅입니다 - 출력되어라&lt;/a&gt;&lt;/li&gt;
    &lt;li&gt;&lt;a href=&quot;./test&quot;&gt;시험용 포스팅입니다 - 출력되어라&lt;/a&gt;&lt;/li&gt;
    &lt;li&gt;&lt;a href=&quot;./test&quot;&gt;시험용 포스팅입니다 - 출력되어라&lt;/a&gt;&lt;/li&gt;
    &lt;li&gt;&lt;a href=&quot;./test&quot;&gt;시험용 포스팅입니다 - 출력되어라&lt;/a&gt;&lt;/li&gt;
    &lt;li&gt;&lt;a href=&quot;./test&quot;&gt;시험용 포스팅입니다 - 출력되어라&lt;/a&gt;&lt;/li&gt;
    &lt;li&gt;&lt;a href=&quot;./test&quot;&gt;시험용 포스팅입니다 - 출력되어라&lt;/a&gt;&lt;/li&gt;
    &lt;li&gt;&lt;a href=&quot;./test&quot;&gt;시험용 포스팅입니다 - 출력되어라&lt;/a&gt;&lt;/li&gt;
    &lt;li&gt;&lt;a href=&quot;./test&quot;&gt;시험용 포스팅입니다 - 출력되어라&lt;/a&gt;&lt;/li&gt;
    &lt;li&gt;&lt;a href=&quot;./test&quot;&gt;시험용 포스팅입니다 - 출력되어라&lt;/a&gt;&lt;/li&gt;
    &lt;li&gt;&lt;a href=&quot;./test&quot;&gt;시험용 포스팅입니다 - 출력되어라&lt;/a&gt;&lt;/li&gt;
    &lt;li&gt;&lt;a href=&quot;./test&quot;&gt;시험용 포스팅입니다 - 출력되어라&lt;/a&gt;&lt;/li&gt;
    &lt;li&gt;&lt;a href=&quot;./test&quot;&gt;시험용 포스팅입니다 - 출력되어라&lt;/a&gt;&lt;/li&gt;
    &lt;li&gt;&lt;a href=&quot;./test&quot;&gt;시험용 포스팅입니다 - 출력되어라&lt;/a&gt;&lt;/li&gt;
    &lt;li&gt;&lt;a href=&quot;./test&quot;&gt;시험용 포스팅입니다 - 출력되어라&lt;/a&gt;&lt;/li&gt;
  --&gt;
    
&lt;/ul&gt;

&lt;h1 id=&quot;text-데이터-광고-필터링을-위한-분류-모델-구축&quot;&gt;Text 데이터 광고 필터링을 위한 분류 모델 구축&lt;/h1&gt;

&lt;h2 id=&quot;abstract&quot;&gt;abstract&lt;/h2&gt;

&lt;p&gt;본 프로젝트는 시계열(text) 데이터를 분석하기 위한 전처리 과정에서 사용자가 원하지 않는 정보를 자동적으로 필터링하여 전처리 진행 속도를 상승시키는 것을 목적으로 한다.
Hate speech 분류에 사용되는 BiLSTM, CharCNN 등의 알고리즘을 활용 가능한지 파악하고 이를 차용하여 인스타그램의 텍스트 데이터에서 광고를 분류하는데 적용하고 각 모델의 성능을 비교한다.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://github.com/lwoongh38/project_cs2&quot; title=&quot;프로젝트링크&quot;&gt;Github repo&lt;/a&gt;&lt;/p&gt;

&lt;hr /&gt;

&lt;h2 id=&quot;1-intro&quot;&gt;1. Intro&lt;/h2&gt;

&lt;p&gt;&lt;img src=&quot;https://user-images.githubusercontent.com/70134676/119241353-b158be80-bb90-11eb-92cb-69c4b43ea285.png&quot; width=&quot;90%&quot; height=&quot;100%&quot; /&gt;&lt;/p&gt;

&lt;p&gt;코로나 바이러스 감염증이 확산된 후 2020년 세계 소매분야 e커머스 매출액은 약 4,414조원으로 전년도 매출 3494조원에 비해 920조원 가량 늘어났다고 합니다.(분석:미디어투자기업 Group M, 출처:e커머스 전망 보고서) 
언택트 소비가 활성화 되면서 더욱 많은 소비자들이 상품평이나 구매 후기 등에 의존하게 되고 이에 따라 기업들은 구매후기 작성에 따른 혜택을 늘려가고 있는데 이를 악용하는 사례 역시 늘어나고 있는 추세입니다.&lt;/p&gt;

&lt;p&gt;구매후기 데이터는 소비자들 뿐만 아니라 기업의 마케팅을 위해서도 활용되는데 이를 위해서는 데이터가 순수한 소비자의 후기를 제외한  이른바 ‘뒷광고’, ‘거짓 후기’ 등으로 오염되어있는 데이터를 정제하는 과정이 필요합니다. 하지만 이 과정에서 많은 시간과, 경제적인 자원이 소모됩니다. 따라서 대량의 시계열(text) 데이터를 빠른시간안에 효율적으로 정제하기 위하여 광고나 거짓후기를 필터링할 수 있는 기술이 필요합니다.&lt;/p&gt;

&lt;p&gt;저는 대학원 연구실에서 인스타그램의 후기 데이터를 바탕으로 마케팅 현황을 분석하는 프로젝트에 참여한 경험이 있습니다.
해당 프로젝트의 데이터 전처리 단계에서 사람이 일일이 읽어보고 광고여부를 판단하여 제거하는 과정을 수행했는데 시간이 매우 오래걸리고 작업자를 지치게 하는 과정이었어요. 따라서 시계열(text)데이터에서 광고를 필터링할 수 있는 모델이 있다면 데이터 전처리 과정의 소요시간을 대폭 감소시킬 수 있을 것이라는 생각이 들어 프로젝트를 기획하게 되었습니다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://user-images.githubusercontent.com/70134676/119241578-78b9e480-bb92-11eb-8327-ff9456e7d2ab.png&quot; width=&quot;90%&quot; height=&quot;100%&quot; /&gt;&lt;/p&gt;

&lt;p&gt;인스타그램.&lt;br /&gt;
현재 국내에서 가장 많이 쓰는 sns 2위를 차지하고 있고 세계적으로도 10억명 이상이 사용할 정도로 절대다수의 이용자들이 자신의 디지털 소통창구로 인스타그램을 활용하고 있습니다. 이런 막강한 유저풀을 활용한 콘텐츠 마케팅이 눈길을 끌고 있습니다.&lt;/p&gt;

&lt;p&gt;2021년 인스타그램에서 발표한 통계자료를 보면 사용자의 약 81%가 제품이나 서비스를 검색하기 위해 인스타그램을 사용하고 있고 1억 3천만명의 유저들이 매달 쇼핑에 관련된 게시글을 보고있습니다. 또 비즈니스계정 사용자는 하루에 평균 한 건의 게시물을 게재하고 있다고 합니다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://user-images.githubusercontent.com/70134676/119241664-10b7ce00-bb93-11eb-8799-e3a2ade9c4ec.png&quot; width=&quot;90%&quot; height=&quot;100%&quot; /&gt;&lt;/p&gt;

&lt;p&gt;다음은 인스타그램에서 화장품을 검색했을 때 노출되는 게시글입니다.
빨간색으로 강조되어있는 부분을 보면 이 게시글들은 광고인 것을 알 수 있습니다.&lt;/p&gt;

&lt;p&gt;문제는 일반 이용자의 경우는 해당 화장품의 솔직한 후기가 궁금하여 게시글을 검색하는 경우가 대부분인데 일일이 게시글을 열어보고 골라내야한다는 점과,
기업의 입장에서는 사용자들의 솔직한 후기를 제품의 개선점이나 신제품출시에 대한 지표를 사용할텐데 텍스트파일을 일일이 읽어보고 광고를 골라내야한다는 점에서 소비자와 기업 모두 인스타그램 텍스트데이터를 활용하는데 있어 어려움을 겪고 있습니다.&lt;/p&gt;

&lt;h3 id=&quot;11-가설-설정&quot;&gt;1.1 가설 설정&lt;/h3&gt;

&lt;p&gt;그래서 저는,&lt;/p&gt;
&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;1. Hate speech 탐지에 쓰이는 방법을 광고 분류 모델에도 적용할 수 있을 것이다.
2. Hate speech 탐지에 쓰이는 이상탐지 메커니즘을 활용하여 광고성 문구를 판별하는 모델을 생성, 0.9 이상의 정확도를 달성할 수 있을 것이다.
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;
&lt;p&gt;라는 가설을 세우고 여러 모델을 제작하여 모델의 성능을 비교해보기로 했습니다.&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;인스타그램 텍스트데이터를 입력하여 전처리, 토큰화하고, 레이블을 입력합니다.&lt;/li&gt;
  &lt;li&gt;각 모델별로 광고성, 비광고성 글에 나타나는 특징에 따라 광고 여부를 판단할 수 있도록 학습시킵니다.&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;인스타그램의 텍스트데이터를 사용하여 광고를 분류할 수 있는 모델을 만드는 것이 최종 목표입니다.&lt;/p&gt;

&lt;h2 id=&quot;2-modeling&quot;&gt;2. Modeling&lt;/h2&gt;

&lt;h3 id=&quot;21-모델-결정&quot;&gt;2.1 모델 결정&lt;/h3&gt;

&lt;p&gt;&lt;img src=&quot;https://user-images.githubusercontent.com/70134676/119241971-3c3bb800-bb95-11eb-9523-6d6564e40dc1.png&quot; width=&quot;90%&quot; height=&quot;100%&quot; /&gt;&lt;/p&gt;

&lt;p&gt;제가 구축할 파이프라인에 사용될 모델 중 하나는 버트입니다.&lt;br /&gt;
다층의 레이어를 활용해서 문장을 구성하고 있는 토큰 사이의 의미를 잘 추출해낼 수 있습니다.&lt;br /&gt;
간단하게 버트가 학습하는 과정을 살펴보면 문장의 일부 단어를 mask토큰으로 바꾸고 가려진 단어를 예측하도록 하는 첫번째 학습을 통해 문맥을 파악하는 능력을 기르게 되고
또 다음 문장이 올바른 문장인지 맞추는 학습을 통해 문장 사이의 관계를 학습하게 됩니다.&lt;br /&gt;
저는 버트 모델중에서도 한글에 특화되어있는 skt에서 한글로 학습시킨 kobert 을 선택했습니다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://user-images.githubusercontent.com/70134676/119241997-69886600-bb95-11eb-9a57-acf3efe2e1c1.png&quot; width=&quot;90%&quot; height=&quot;100%&quot; /&gt;&lt;/p&gt;

&lt;p&gt;두번째 모델은 bilstm입니다.&lt;br /&gt;
Bilstm에 대해 예를 들어 설명하면,&lt;/p&gt;

&lt;p&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;&quot; 나는 __을 뒤집어 쓰고 펑펑 울었다. &quot;&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;위의 문장에서 빈칸에 어떤 단어가 들어갈 지 유추할때, 한국어를 자유자재로 사용하는 사람은 빈칸에 들어갈 말이 이불이라는 것을 쉽게 알 수 있습니다. 그런데 이 문장의 경우, 빈칸을 유추 할때, 빈칸 앞보다는 빈칸 뒤에 나오는 단어들이 더 중요 합니다.&lt;br /&gt;
 ‘나는’ 뒤에 나올 수 있는 단어는 수없이 많은 반면에, ‘뒤집어 쓰고 펑펑 울었다’ 앞에 나올 수 있는 단어는 흔치 않기 때문 입니다.&lt;br /&gt;
 이 예제에서 볼 수 있듯이, 텍스트 데이터는 정방향 추론 못지 않게 역방향 추론도 유의미한 결과를 낼 수 있습니다. 그래서 lstm을 다음과 같이 정방향 학습, 역방향 학습을 할 수 있도록 설계한 것이 bilstm입니다.&lt;/p&gt;

&lt;h3 id=&quot;22-데이터-수집&quot;&gt;2.2 데이터 수집&lt;/h3&gt;

&lt;p&gt;&lt;img src=&quot;https://user-images.githubusercontent.com/70134676/119242063-f0d5d980-bb95-11eb-9a43-5339e7a8c57a.png&quot; width=&quot;90%&quot; height=&quot;100%&quot; /&gt;&lt;/p&gt;

&lt;p&gt;저는 인스타로더를 사용해서 인스타그램 텍스트데이터를 확보했습니다.&lt;br /&gt;
하지만 정제가 되어있지 않기 때문에 주어진 시간을 효율적으로 활용하기 위해 광고일 확률이 높은 해시태그와 광고가 아닐 확률이 높은 해시태그를 선정해서 크롤링한 다음, 광고 필터링 리스트를 제작해서 라벨링을 진행했습니다.&lt;/p&gt;

&lt;p&gt;검증, 테스트용 데이터셋은 일반적으로 사람이 광고를 분류하는 기준을 적용하기 위해서 약 3만개의 데이터를 직접 분류하는 방법으로 라벨링했습니다.&lt;br /&gt;
학습용 데이터셋은 제품이나 서비스 분야를 가리지 않고 적용시킬 수 있도록 #광고아님, #찐후기, #내돈내산, #홍보, #협찬 등과 같이 되도록 일반적인 해시태그로 데이터를 추출했고
검증, 테스트용 데이터셋은 광고와 광고가 아닌 일반 후기를 수집하기 수월한 #설화수, #미용기기 프라엘을 선정해서 이것을 해시태그로 수집했습니다.&lt;/p&gt;

&lt;h3 id=&quot;23-데이터-전처리&quot;&gt;2.3 데이터 전처리&lt;/h3&gt;

&lt;p&gt;&lt;img src=&quot;https://user-images.githubusercontent.com/70134676/119242145-a1dc7400-bb96-11eb-92e1-18a50068bda8.png&quot; width=&quot;90%&quot; height=&quot;100%&quot; /&gt;&lt;/p&gt;

&lt;p&gt;인스타그램 텍스트데이터를 크롤링하는 패키지를 사용해서 얻어지는 raw데이터는 위와 같고, 1차적으로 이모티콘을 제거하고 줄바꿈과 같은 제어문자를 제거했습니다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://user-images.githubusercontent.com/70134676/119242197-f2ec6800-bb96-11eb-8f27-86c7e0febe45.png&quot; width=&quot;90%&quot; height=&quot;100%&quot; /&gt;&lt;/p&gt;

&lt;p&gt;모든 특수 문자를 한번에 지우지 않은 이유는, 필터링을 통해 라벨링을 할 때 예를 들어 #광고 해시태그를 필터링 하고 싶을 때 #이 없으면 광고로 필터링을 진행해야 하는데 이 때 광고아님도 같이 포함되어 필터링이 되기 때문에 1차 전처리 완료하고 라벨링을 한다음, 이어서 전처리를 진행했습니다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://user-images.githubusercontent.com/70134676/119242226-1ca58f00-bb97-11eb-8663-80ccecd074c1.png&quot; width=&quot;90%&quot; height=&quot;100%&quot; /&gt;&lt;/p&gt;

&lt;p&gt;다음은 라벨링을 완료하고 2차전처리로 특수기호까지 제거해준 모습입니다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://user-images.githubusercontent.com/70134676/119242252-537ba500-bb97-11eb-9360-47922e294363.png&quot; width=&quot;90%&quot; height=&quot;100%&quot; /&gt;&lt;/p&gt;

&lt;p&gt;띄어쓰기가 되어있지 않은 데이터 같은 경우는 그냥 전처리를 하게 되면 토큰화를 할 때 의미단위로 코퍼스가 분리되지 않는 문제가 있어서 따로 처리해줬습니다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://user-images.githubusercontent.com/70134676/119242278-9178c900-bb97-11eb-9828-9979390601a5.png&quot; width=&quot;90%&quot; height=&quot;100%&quot; /&gt;&lt;/p&gt;

&lt;p&gt;최종적으로 전처리가 완료되면 위와 같은 방식으로 정제되는 것을 볼 수 있습니다.&lt;/p&gt;

&lt;h2 id=&quot;3-검증&quot;&gt;3. 검증&lt;/h2&gt;

&lt;h3 id=&quot;31-데이터-eda&quot;&gt;3.1 데이터 EDA&lt;/h3&gt;

&lt;p&gt;&lt;img src=&quot;https://user-images.githubusercontent.com/70134676/119242312-d3a20a80-bb97-11eb-86c2-869ce5da0d38.png&quot; width=&quot;90%&quot; height=&quot;100%&quot; /&gt;&lt;/p&gt;

&lt;p&gt;데이터셋의 단어 통계를 확인하고싶어 한국어 형태소 분석기인 Mecab 패키지를 사용해서 간단하게 EDA를 진행해봤습니다. 토크나이저를 사용하고, 각 학습용 데이터셋에 대해서 라벨별로 최빈 단어들을 모아본 결과, 다음과 같은 결과를 얻을 수 있었습니다. 광고 데이터셋의 최빈단어목록에서 ‘협찬’ 같은 단어가 발견되었지만 그 외에는 크게 의미가 있는 결과를 보여주진 못했습니다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://user-images.githubusercontent.com/70134676/119242428-870aff00-bb98-11eb-86de-2bed2386cb59.png&quot; width=&quot;90%&quot; height=&quot;100%&quot; /&gt;&lt;/p&gt;

&lt;p&gt;혹시 데이터의 길이로 판별이 가능한지 알아본 결과 광고성 글은 비 광고성 글의 길이보다 평균 두배 정도가 차이가 났습니다. 하지만 두 레이블 모두  겹치는 구간이 꽤 발생하기 때문에 문장의 길이로 단정짓기는 힘들다고 판단했습니다.&lt;/p&gt;

&lt;h3 id=&quot;4-실험-결과-및-결론&quot;&gt;4. 실험 결과 및 결론&lt;/h3&gt;

&lt;p&gt;&lt;img src=&quot;https://user-images.githubusercontent.com/70134676/119242466-d81af300-bb98-11eb-92b5-c66a83fe8c68.png&quot; width=&quot;90%&quot; height=&quot;100%&quot; /&gt;&lt;/p&gt;

&lt;p&gt;완성된 두가지 모델입니다.&lt;br /&gt;
정밀도와 재현율, f1 스코어를 평가지표로 결정했고 이중에 재현율을 좀더 중요하게 활용하기로 했습니다.&lt;br /&gt;
광고가 아닌 글들을 광고라고 판단하는 경우가 다소 있더라도, 실제 광고 글을 잘 분류해내는 것이 모델이 달성해야 할 최우선 목표라고 생각했기 때문입니다.&lt;br /&gt;
평가지표상의 성능으로 판단해보면 데이터들의 분포는 오른쪽 그림과 같이 되어있음을 추측해볼 수 있었습니다.&lt;/p&gt;

&lt;p&gt;이 모델들이 실제 광고를 광고로 올바르게 분류할 확률은 0.9 이상으로 가설로 제시했던 0.6을 상회하는 성능을 보여주었습니다. 이는 실제 광고를 광고라고 예측하는 기능은 모델의 목적에 맞도록 잘 작동하고 있다고 볼 수 있습니다.&lt;br /&gt;
하지만 재현율에 비해 정밀도나 F1스코어가 많이 낮은것을 보면 광고라고 예측된 것 중에 실제로는 광고가 아니었던 사례들이 꽤 많다는 것을 알 수 있어 모델의 전반적인 성능을 위해 개선작업이 필요하다고 판단됩니다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://user-images.githubusercontent.com/70134676/119242536-8aeb5100-bb99-11eb-882c-ba5998ac90ec.png&quot; width=&quot;90%&quot; height=&quot;100%&quot; /&gt;&lt;/p&gt;

&lt;p&gt;마지막으로 모델로 데이터 전체를 분류해본 결과입니다.&lt;br /&gt;
두가지 모델의 성능은 크게 다르지 않았지만 한글을 모델에 적용하는 과정에서 kobert가 전이학습된 모델을 가져와서 사용하기 때문에 형태소분석 등의 번거로운 과정을 생략할 수 있다는 점이 큰 장점으로 다가왔습니다.&lt;/p&gt;

&lt;hr /&gt;

&lt;h2 id=&quot;프로젝트를-마무리하며&quot;&gt;프로젝트를 마무리하며..&lt;/h2&gt;

&lt;p&gt;저는 이번 프로젝트에서 인스타그램의 텍스트 데이터 분류를 위한 광고 필터링 모델을 제작하는 것을 계획했습니다. 기술적인 어려움도 있었지만 데이터를 라벨링하는 과정에서 수동으로 일일이 광고인 것과 아닌 것을 분류해야하는 작업에서 시간적인 어려움을 겪었습니다. 전체 프로젝트에 할당된 시간중 거의 90%에 가까운 시간이 데이터를 크롤링하고, 정제하고, 라벨링하는데 소요되었습니다.&lt;/p&gt;

&lt;p&gt;여태껏 정제되어있는 데이터만을 프로젝트에서 사용했었는데 데이터셋 하나를 만든다는 것이 얼마나 많은 공이 들어가는 일인지를 알 수 있었습니다. 특히 NLP에 사용되는 데이터셋은 전처리 과정에서 이것저것 신경쓸 것도 많아 쉽지않은 과정이었습니다. 데이터셋을 만드는데 얼마나 많은 공이 들어가는지 알게 되었으니 앞으로는 고마운 마음으로 써야할 것 같네요.&lt;/p&gt;

&lt;p&gt;이번 프로젝트에서 가장 크게 배운점이라면 바로 윈도우와 맥os에서 한글의 유니코드 정규화가 다르게 이루어진다는 것을 알게 된 것입니다.&lt;br /&gt;
중2병이 돋은듯한 한글자모분리현상과 일부 특수기호나 이모티콘이 일괄제거가 되지 않아 원인을 파고파고 또 파보니 맥os는 NFD, 윈도우는 NFC라는 방식을 사용한다고 합니다.&lt;br /&gt;
같은 언어를 운영체제에 따라 다르게 받아들일 수 있다고는 상상도 못해봤기에 기업간의 쓸데없는 자존심 싸움이 사용자들에게 의미없는 수고를 강요한다는 생각이 들었습니다.&lt;/p&gt;

&lt;p&gt;프로젝트 진행과정에서 전처리와 레이블링 과정에서 굉장히 고생을 많이 했고 또 배운점이 많아 추후에 텍스트 전처리와 레이블링 작업에 대해 별도로 포스팅 해리라 결심해보며 이번 프로젝트를 마무리합니다.&lt;/p&gt;</content>

      
      
      
      
      

      <author>
          <name>woongE</name>
        
        
      </author>

      

      
        <category term="ai" />
      

      
        <summary type="html">#deeplearning #NLP #BERT #classification #filtering</summary>
      

      
      
    </entry>
  
    <entry>
      <title type="html">사용자 리뷰 핵심어 추출 및 감성분석</title>
      <link href="http://localhost:4000/bert-sentiment-keyword" rel="alternate" type="text/html" title="사용자 리뷰 핵심어 추출 및 감성분석" />
      <published>2021-03-18T07:00:00+09:00</published>
      <updated>2021-03-18T07:00:00+09:00</updated>
      <id>http://localhost:4000/bert-sentiment-keyword</id>
      <content type="html" xml:base="http://localhost:4000/bert-sentiment-keyword">&lt;p&gt;#deeplearning #NLP #BERT #sentimental classification #keyword extraction&lt;/p&gt;

&lt;p&gt;&lt;span class=&quot;table-of-contents-list&quot;&gt;목차 &lt;/span&gt;&lt;/p&gt;
&lt;ul class=&quot;table-of-contents-list&quot;&gt;
    &lt;li&gt;&lt;a href=&quot;./tadgan-modelstructure&quot;&gt;Time seriesAnomaly Detection Generative Adversarial Networks-Model Structure &lt;/a&gt;&lt;/li&gt;  
    &lt;li&gt;&lt;a href=&quot;./tadgan-lossfunction&quot;&gt;Time seriesAnomaly Detection Generative Adversarial Networks-loss function &lt;/a&gt;&lt;/li&gt;  
    &lt;li&gt;&lt;a href=&quot;./tadgan-intro&quot;&gt;Intro to the Time Series Anomaly Detection &lt;/a&gt;&lt;/li&gt;  
    &lt;li&gt;&lt;a href=&quot;./text-ads-filtering&quot;&gt;Text 데이터 광고 필터링을 위한 분류 모델 구축 &lt;/a&gt;&lt;/li&gt;
    &lt;li&gt;&lt;a href=&quot;./bert-sentiment-keyword&quot;&gt;사용자 리뷰 핵심어 추출 및 감성분석 &lt;/a&gt;&lt;/li&gt;
    &lt;li&gt;&lt;a href=&quot;./deeplearning-summary&quot;&gt;Deep Learning - 핵심 개념 &amp;amp; 용어 &lt;/a&gt;&lt;/li&gt;
    &lt;li&gt;&lt;a href=&quot;./versatility&quot;&gt;NLP - 딥러닝 모델의 범용적 활용가능성 분석 &lt;/a&gt;&lt;/li&gt;
    &lt;li&gt;&lt;a href=&quot;./bank-marketing&quot;&gt;Bank marketing 정기예금 가입여부 예측 &lt;/a&gt;&lt;/li&gt;
    &lt;li&gt;&lt;a href=&quot;./roccurve&quot;&gt;ROC curve에 대해 알아보자 &lt;/a&gt;&lt;/li&gt;
    &lt;li&gt;&lt;a href=&quot;./machinelearning-linearregression&quot;&gt;연어의 회귀본능이 아닌 선형회귀에 대해 알아보자 &lt;/a&gt;&lt;/li&gt;  
    
        
    
    


    &lt;!-- &lt;li&gt;&lt;a href=&quot;./proposal&quot;&gt;코스레스토랑에 보내는 제안서 &lt;/a&gt;&lt;/li&gt; --&gt;
    &lt;!-- &lt;li&gt;&lt;a href=&quot;./proposal&quot;&gt;코스레스토랑에 보내는 제안서 &lt;/a&gt;&lt;/li&gt; --&gt;
    &lt;!-- &lt;li&gt;&lt;a href=&quot;./proposal&quot;&gt;코스레스토랑에 보내는 제안서 &lt;/a&gt;&lt;/li&gt; --&gt;
    &lt;!-- &lt;li&gt;&lt;a href=&quot;./proposal&quot;&gt;코스레스토랑에 보내는 제안서 &lt;/a&gt;&lt;/li&gt; --&gt;
    &lt;!-- &lt;li&gt;&lt;a href=&quot;./proposal&quot;&gt;코스레스토랑에 보내는 제안서 &lt;/a&gt;&lt;/li&gt; --&gt;
    &lt;!-- &lt;li&gt;&lt;a href=&quot;./proposal&quot;&gt;코스레스토랑에 보내는 제안서 &lt;/a&gt;&lt;/li&gt; --&gt;
    &lt;!-- &lt;li&gt;&lt;a href=&quot;./proposal&quot;&gt;코스레스토랑에 보내는 제안서 &lt;/a&gt;&lt;/li&gt; --&gt;
    &lt;!-- &lt;li&gt;&lt;a href=&quot;./proposal&quot;&gt;코스레스토랑에 보내는 제안서 &lt;/a&gt;&lt;/li&gt; --&gt;
    &lt;!-- &lt;li&gt;&lt;a href=&quot;./proposal&quot;&gt;코스레스토랑에 보내는 제안서 &lt;/a&gt;&lt;/li&gt; --&gt;
    &lt;!-- &lt;li&gt;&lt;a href=&quot;./proposal&quot;&gt;코스레스토랑에 보내는 제안서 &lt;/a&gt;&lt;/li&gt; --&gt;
    &lt;!-- &lt;li&gt;&lt;a href=&quot;./proposal&quot;&gt;코스레스토랑에 보내는 제안서 &lt;/a&gt;&lt;/li&gt; --&gt;
    &lt;!-- &lt;li&gt;&lt;a href=&quot;./proposal&quot;&gt;코스레스토랑에 보내는 제안서 &lt;/a&gt;&lt;/li&gt; --&gt;
    &lt;!-- &lt;li&gt;&lt;a href=&quot;./proposal&quot;&gt;코스레스토랑에 보내는 제안서 &lt;/a&gt;&lt;/li&gt; --&gt;
    &lt;!-- &lt;li&gt;&lt;a href=&quot;./proposal&quot;&gt;코스레스토랑에 보내는 제안서 &lt;/a&gt;&lt;/li&gt; --&gt;
    &lt;!-- &lt;li&gt;&lt;a href=&quot;./proposal&quot;&gt;코스레스토랑에 보내는 제안서 &lt;/a&gt;&lt;/li&gt;     --&gt;









&lt;!-- 
    &lt;li&gt;&lt;a href=&quot;./test&quot;&gt;시험용 포스팅입니다 - 출력되어라&lt;/a&gt;&lt;/li&gt;
    &lt;li&gt;&lt;a href=&quot;./test&quot;&gt;시험용 포스팅입니다 - 출력되어라&lt;/a&gt;&lt;/li&gt;
    &lt;li&gt;&lt;a href=&quot;./test&quot;&gt;시험용 포스팅입니다 - 출력되어라&lt;/a&gt;&lt;/li&gt;
    &lt;li&gt;&lt;a href=&quot;./test&quot;&gt;시험용 포스팅입니다 - 출력되어라&lt;/a&gt;&lt;/li&gt;
    &lt;li&gt;&lt;a href=&quot;./test&quot;&gt;시험용 포스팅입니다 - 출력되어라&lt;/a&gt;&lt;/li&gt;
    &lt;li&gt;&lt;a href=&quot;./test&quot;&gt;시험용 포스팅입니다 - 출력되어라&lt;/a&gt;&lt;/li&gt;
    &lt;li&gt;&lt;a href=&quot;./test&quot;&gt;시험용 포스팅입니다 - 출력되어라&lt;/a&gt;&lt;/li&gt;
    &lt;li&gt;&lt;a href=&quot;./test&quot;&gt;시험용 포스팅입니다 - 출력되어라&lt;/a&gt;&lt;/li&gt;
    &lt;li&gt;&lt;a href=&quot;./test&quot;&gt;시험용 포스팅입니다 - 출력되어라&lt;/a&gt;&lt;/li&gt;
    &lt;li&gt;&lt;a href=&quot;./test&quot;&gt;시험용 포스팅입니다 - 출력되어라&lt;/a&gt;&lt;/li&gt;
    &lt;li&gt;&lt;a href=&quot;./test&quot;&gt;시험용 포스팅입니다 - 출력되어라&lt;/a&gt;&lt;/li&gt;
    &lt;li&gt;&lt;a href=&quot;./test&quot;&gt;시험용 포스팅입니다 - 출력되어라&lt;/a&gt;&lt;/li&gt;
    &lt;li&gt;&lt;a href=&quot;./test&quot;&gt;시험용 포스팅입니다 - 출력되어라&lt;/a&gt;&lt;/li&gt;
    &lt;li&gt;&lt;a href=&quot;./test&quot;&gt;시험용 포스팅입니다 - 출력되어라&lt;/a&gt;&lt;/li&gt;
    &lt;li&gt;&lt;a href=&quot;./test&quot;&gt;시험용 포스팅입니다 - 출력되어라&lt;/a&gt;&lt;/li&gt;
    &lt;li&gt;&lt;a href=&quot;./test&quot;&gt;시험용 포스팅입니다 - 출력되어라&lt;/a&gt;&lt;/li&gt;
    &lt;li&gt;&lt;a href=&quot;./test&quot;&gt;시험용 포스팅입니다 - 출력되어라&lt;/a&gt;&lt;/li&gt;
    &lt;li&gt;&lt;a href=&quot;./test&quot;&gt;시험용 포스팅입니다 - 출력되어라&lt;/a&gt;&lt;/li&gt;
    &lt;li&gt;&lt;a href=&quot;./test&quot;&gt;시험용 포스팅입니다 - 출력되어라&lt;/a&gt;&lt;/li&gt;
    &lt;li&gt;&lt;a href=&quot;./test&quot;&gt;시험용 포스팅입니다 - 출력되어라&lt;/a&gt;&lt;/li&gt;
    &lt;li&gt;&lt;a href=&quot;./test&quot;&gt;시험용 포스팅입니다 - 출력되어라&lt;/a&gt;&lt;/li&gt;
  --&gt;
    
&lt;/ul&gt;

&lt;h1 id=&quot;bert를-활용한-사용자-리뷰-핵심어-추출-및-감성분석&quot;&gt;BERT를 활용한 사용자 리뷰 핵심어 추출 및 감성분석&lt;/h1&gt;

&lt;h2 id=&quot;abstract&quot;&gt;abstract&lt;/h2&gt;

&lt;p&gt;본 프로젝트는 텍스트 데이터를 분석하여 리뷰 내용이 상품의 어떤 특성을 특정하고 있는 것인지 추출하고 감성분석을 하여 사용자 리뷰가 어떤 내용에 대해 서술된 것인지, 해당 내용에 대해 긍정적인지 부정적인지를 판단할 수 있도록 하는 파이프라인을 구축하는 것을 목적으로 한다. 
따라서 텍스트데이터에서 핵심어를 추출한 결과와 감성분석을 실시한 결과를 통합하여 유의미한 비즈니스 인사이트를 얻어낼 수 있도록 가설을 세우고 이를 리뷰데이터 원문과 추출된 핵심어, 감성분석 결과를 비교하여 신뢰할 만한 결과를 도출해내는지 실험하는 방향으로 프로젝트를 진행한다.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://github.com/lwoongh38/project_cs1&quot; title=&quot;프로젝트링크&quot;&gt;Github repo&lt;/a&gt;&lt;/p&gt;

&lt;hr /&gt;

&lt;h2 id=&quot;1-intro&quot;&gt;1. Intro&lt;/h2&gt;

&lt;p&gt;&lt;img src=&quot;https://i1.wp.com/www.startsmallmedia.com/wp-content/uploads/2019/01/SSM-blog-800x400.png?fit=800%2C400&quot; width=&quot;90%&quot; height=&quot;100%&quot; /&gt;&lt;/p&gt;

&lt;p&gt;리뷰, 온라인 쇼핑을 할 때면 늘 구매후기를 먼저 찾아보게 됩니다. 
저 또한 마찬가지인데요.
리뷰는 소비자들이 어떤 물건이나 서비스를 구매함에 있어 어느 매체의 정보보다 많은 영향을 끼치고 있습니다.
당장 저부터 어떤 재화를 구매하기 전에 항상 리뷰를 검색해서 찾아보는 편인데요.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://user-images.githubusercontent.com/70134676/119223450-7d4fb000-bb34-11eb-90fd-6facafa63589.png&quot; width=&quot;90%&quot; height=&quot;100%&quot; /&gt;&lt;/p&gt;

&lt;p&gt;위 그림은 구글에 리뷰와 마케팅을 검색했을 때 나오는 결과입니다.
이렇게 소비자에게 중요한 구매지표가 된 리뷰는 이제 생산하는 재화에 대한 상품성 개선, 신상품 기획에 필요한 소비자의 의견 파악이라는, 기업의 입장에서도 마케팅용으로 반드시 활용해야하고 또 활용하고 있을 것이라는 생각이 들어서 이번 프로젝트를 기획하게 되었습니다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://user-images.githubusercontent.com/70134676/119223507-d6b7df00-bb34-11eb-9995-830632a6ac77.png&quot; width=&quot;90%&quot; height=&quot;100%&quot; /&gt;&lt;/p&gt;

&lt;p&gt;현재 대중적인 온라인커머스 기업인 네이버쇼핑의 리뷰입니다.&lt;/p&gt;

&lt;p&gt;빨간색으로 강조되어있는 부분처럼 판매자가 정해놓은 주제 중에 선택하여 작성하게 되어있습니다. 이처럼 기존의 리뷰데이터는 평점이 재화의 어떤 점에 대한 평가인지 업체가 정해놓은 주제 외에는 구체적으로 알기가 힘듭니다.&lt;/p&gt;

&lt;p&gt;이런 사례에서 볼 수 있듯이 현재 리뷰를 기업의 입장에서 활용하는데 가장 큰 장해물은 리뷰가 나타내는 내용과 평점이 반드시 일치하지 않는다는 것입니다.&lt;br /&gt;
오른쪽 그림처럼 모두 평점5점의 리뷰이지만 리뷰마다 다른 특징을 평가하고 있기 때문에 리뷰내용을 분석하여 각 리뷰의 주제를 추출하고 추출된 주제에 대한 감성분석을 실시하는 것이 효율적인 사용자리뷰 활용을 위해 필요하다고 생각했습니다.&lt;/p&gt;

&lt;h3 id=&quot;11-가설-설정&quot;&gt;1.1 가설 설정&lt;/h3&gt;
&lt;p&gt;그래서 제가 세운 가설은 다음과 같습니다.&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;“리뷰데이터에서 키워드 추출 모델을 통해 추출된 핵심어와 감성분석 모델의 분석 결과를 합쳐서 리뷰 원문의 의미와 일치하도록 하는 리뷰 분석 파이프라인을 만들어 80%이상의 정확도를 확보할 수 있다”
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;&lt;img src=&quot;https://user-images.githubusercontent.com/70134676/119223720-ee439780-bb35-11eb-92de-578bc3bceb8e.png&quot; width=&quot;90%&quot; height=&quot;100%&quot; /&gt;&lt;/p&gt;

&lt;p&gt;예를들어 “생각보다 착용감이 좋아요. 사운드가 시원시원합니다”라는 리뷰가 주어졌을 때, 감성분석모델을 통해 해당 문장은 긍정으로 분류하고 키워드 추출모델에 의해서 착용감, 사운드, 좋아요 등의 키워드가 추출되어 최종적으로 해당리뷰는 착용감, 사운드 측면에서 긍정적인 리뷰임을 판별하는 파이프라인을 구축하는 것이 프로젝트의 목표입니다.&lt;/p&gt;

&lt;h2 id=&quot;2-modeling&quot;&gt;2. Modeling&lt;/h2&gt;

&lt;h3 id=&quot;21-모델-결정---kobert&quot;&gt;2.1 모델 결정 - KOBERT&lt;/h3&gt;

&lt;p&gt;제가 프로젝트에 사용될 모델은 버트입니다. 다층의 레이어를 활용해서 문장을 구성하고 있는 토큰 사이의 의미를 잘 추출해낼 수 있습니다.&lt;br /&gt;
간단하게 버트가 학습하는 과정을 살펴보면 문장의 일부 단어를 mask토큰으로 바꾸고 가려진 단어를 예측하도록 하는 학습을 통해 문맥을 파악하는 능력을 기르게 되고,
또 다음 문장이 올바른 문장인지 맞추는 비지도학습을 통해 문장 사이의 관계를 학습하게 됩니다.&lt;br /&gt;
저는 버트 모델중에서도 한글에 특화되어있는 skt에서 한글로 학습시킨 kobert 모델과 google의 다중언어 bert 모델을 이용하여 프로젝트를 진행했습니다.&lt;/p&gt;

&lt;h3 id=&quot;22-데이터셋-결정---네이버쇼핑-리뷰-네이버영화-리뷰-steam-리뷰&quot;&gt;2.2 데이터셋 결정 - 네이버쇼핑 리뷰, 네이버영화 리뷰, steam 리뷰&lt;/h3&gt;

&lt;p&gt;프로젝트를 진행하기 위하여 제가 수집한 데이터셋입니다.&lt;br /&gt;
저는 모델의 파인튜닝을 진행하기 위해 다양한 상품에 대한 리뷰 총 20만개의 리뷰를 학습과 검증용 데이터로 사용했고 테스트용 데이터를 별도로 특정 상품군을 선정하여 3600개의 데이터를 별도로 준비했습니다.&lt;br /&gt;
리뷰 원문과 파이프라인을 거친 텍스트의 내용이 일치하는지의 판단은 문장이 나타내는 디테일한 뉘앙스와 판별 결과를 비교하기 위해서 사람이 직접 읽어보고 올바르게 판별했는지를 비교했습니다.&lt;/p&gt;

&lt;h2 id=&quot;3-검증&quot;&gt;3. 검증&lt;/h2&gt;

&lt;h3 id=&quot;31-modeling&quot;&gt;3.1 Modeling&lt;/h3&gt;
&lt;p&gt;&lt;img src=&quot;https://user-images.githubusercontent.com/70134676/119224035-88f0a600-bb37-11eb-9dc0-a12f4a27c2a1.png&quot; width=&quot;90%&quot; height=&quot;100%&quot; /&gt;&lt;/p&gt;

&lt;p&gt;감성분석 모델은 f1스코어와 정밀도, 재현율 모두 0.9 이상을 기록하며 높은 성능을 나타내고 있습니다.&lt;br /&gt;
실제 텍스트를 넣고 검증을 해봤을 때,
첫번째처럼 부정 긍정이 확실한 경우는 예상처럼 잘 분류를 하는 것을 볼 수 있었습니다.&lt;br /&gt;
두번째 리뷰는 긍정적인 부분과 부정적인 부분을 같이 적었는데 긍정으로 분류를 하였고 전체적인 문장의 뉘앙스는 긍정이 맞으므로 올바르게 분석하고 있다고 판단했습니다.&lt;br /&gt;
마지막으로 세번째 또한 긍정적인 부분과 부정적인 부분이 같이 있지만 문장에서 느껴지는 분위기는 부정적이므로 맞게 판단하였습니다.
전반적으로 감성분석 모델은 잘 구축이 되었다고 판단했습니다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://user-images.githubusercontent.com/70134676/119224134-fc92b300-bb37-11eb-966e-e2de889f10ed.png&quot; width=&quot;90%&quot; height=&quot;100%&quot; /&gt;&lt;/p&gt;

&lt;p&gt;다음은 키워드 추출 모델인데 키워드 추출 모델의 경우 문장을 키워드 단위로 쪼개고 쪼개진 토큰들을 임베딩모델을 통해 임베딩하여 수치화했습니다. 임베딩된 키워드들은 전체 문장과 키워드 단위로 분리된 벡터 사이의 거리를 이용하여 코사인유사도를 계산했습니다. 원래 문장 벡터와 키워드의 벡터가 유사할수록 전체 문장을 잘 표현하는 단어라고 할 수 있습니다. 계산결과에서 가장 유사한 순서대로 5개의 키워드만 추출했습니다&lt;/p&gt;

&lt;h3 id=&quot;4-실험-결과-및-결론&quot;&gt;4. 실험 결과 및 결론&lt;/h3&gt;
&lt;p&gt;&lt;img src=&quot;https://user-images.githubusercontent.com/70134676/119224182-306dd880-bb38-11eb-9bb8-16dfd6e7d2d4.png&quot; width=&quot;90%&quot; height=&quot;100%&quot; /&gt;&lt;/p&gt;

&lt;p&gt;다음은 전체 결과입니다.
파란색으로 강조된 evaluation이 감성분석을 통해 분류된 결과이고
Review는 리뷰 원문, review_count는 원문을 키워드단위로 쪼갰을 때 전체 키워드입니다. 마지막으로 keyword가 전체 문장을 대표하는 키워드 5가지입니다.&lt;/p&gt;

&lt;p&gt;직접 3600개의 데이터를 비교했을 때 감성분석결과는 예상보다 세세한 문장의 분위기까지 잘 잡아내서 매우 만족스러웠는데 키워드 추출 모델의 경우는 생각보다 정확도가 많이 떨어졌습니다.&lt;/p&gt;

&lt;p&gt;이유를 세 가지 정도로 생각해봤는데,&lt;br /&gt;
첫번째는 키워드 추출과정입니다. 키워드를 추출하는 업무는 아무래도 문장이 어떻게 토큰화 되느냐가 중요합니다. 이번 프로젝트에서는 시간관계상 코버트에서 제공하는 토크나이저를 사용했는데 실험의 정확도를 위해서는 다른 토크나이저를 사용해서 비교해봐야 결과값을 제대로 판단할 수 있을 것 같습니다.&lt;/p&gt;

&lt;p&gt;두번째는 주요 키워드 선정 방법입니다. 주요 키워드를 추출하는 방법으로 단순하게 코사인 유사도를 계산해서 거리가 가까운 순으로 주요 키워드를 선정했는데 다양한 방법으로 유사도를 확인하는 과정이 필요할 것 같습니다.&lt;/p&gt;

&lt;p&gt;마지막 세번째는 모델 자체의 기능입니다.&lt;br /&gt;
최종 주요 키워드로 선정된 결과를 보면 제가 생각했던 것처럼 심플하게 주요 키워드가 선정되지 않았는데 모델에서 사람이 주요하다고 생각하는 키워드를 제대로 인식할 정도로 학습이 제대로 이루어지지 않은것이 아닌가 하는 의심을 해보게 되었습니다.&lt;/p&gt;

&lt;p&gt;버트 모델은 기본적으로 버트에서 사전학습된 임베딩 능력을 가져오고 내가 원하는 업무에 맞도록 파인튜닝하는 과정을 거치게 됩니다. 키워드 추출 업무의 경우, 키워드를 주요하다고 판단할 만한 근거를 finetunning을 실시할 때 충분히 학습시켜주지 않아서 이런 결과가 나왔을 수도 있겠다는 생각이 들었습니다.&lt;/p&gt;

&lt;hr /&gt;

&lt;h2 id=&quot;프로젝트를-마무리하며&quot;&gt;프로젝트를 마무리하며..&lt;/h2&gt;

&lt;p&gt;이번 섹션 프로젝트를 진행하며 가장 힘든 것은 절대적인 시간이 많이 부족해서 현실과 타협을 많이 해야했던 것이었습니다. 연구실 업무, 졸업 논문에 사소하게는 컴퓨터의 환경설정 문제까지 프로젝트 외의 다른 요소 때문에 시간을 뺏겨서 프로젝트를 기한 내에 끝내기 위해 처음 계획했던 것보다는 많은 실험을 하지 못했습니다.  그래도 기존에 PyTorch 라이브러리로 사용해봤던 BERT를 이번에는 익숙한 케라스로 구현하고 또 한글특화모델인 KOBERT를 사용해보면서 나름 모델에 대해 더 이해할 수 있었던 점은 좋았던 것 같습니다.
다음 프로젝트에는 이번 프로젝트에서 반드시 해보려고 했던 웹 스크래핑으로 실제로 데이터를 직접 수집해서 전처리부터 끝까지 진정한 의미의 End to End 프로젝트를 해보기로 결심하고 이번 프로젝트를 마무리합니다.&lt;/p&gt;</content>

      
      
      
      
      

      <author>
          <name>woongE</name>
        
        
      </author>

      

      
        <category term="ai" />
      

      
        <summary type="html">#deeplearning #NLP #BERT #sentimental classification #keyword extraction</summary>
      

      
      
    </entry>
  
    <entry>
      <title type="html">Deep Learning - 핵심 개념 &amp;amp; 용어</title>
      <link href="http://localhost:4000/deeplearning-summary" rel="alternate" type="text/html" title="Deep Learning - 핵심 개념 &amp; 용어" />
      <published>2021-01-18T07:00:00+09:00</published>
      <updated>2021-01-18T07:00:00+09:00</updated>
      <id>http://localhost:4000/deeplearning-summary</id>
      <content type="html" xml:base="http://localhost:4000/deeplearning-summary">&lt;p&gt;#deeplearning #용어 #개념&lt;/p&gt;

&lt;p&gt;&lt;span class=&quot;table-of-contents-list&quot;&gt;목차 &lt;/span&gt;&lt;/p&gt;
&lt;ul class=&quot;table-of-contents-list&quot;&gt;
    &lt;li&gt;&lt;a href=&quot;./tadgan-modelstructure&quot;&gt;Time seriesAnomaly Detection Generative Adversarial Networks-Model Structure &lt;/a&gt;&lt;/li&gt;  
    &lt;li&gt;&lt;a href=&quot;./tadgan-lossfunction&quot;&gt;Time seriesAnomaly Detection Generative Adversarial Networks-loss function &lt;/a&gt;&lt;/li&gt;  
    &lt;li&gt;&lt;a href=&quot;./tadgan-intro&quot;&gt;Intro to the Time Series Anomaly Detection &lt;/a&gt;&lt;/li&gt;  
    &lt;li&gt;&lt;a href=&quot;./text-ads-filtering&quot;&gt;Text 데이터 광고 필터링을 위한 분류 모델 구축 &lt;/a&gt;&lt;/li&gt;
    &lt;li&gt;&lt;a href=&quot;./bert-sentiment-keyword&quot;&gt;사용자 리뷰 핵심어 추출 및 감성분석 &lt;/a&gt;&lt;/li&gt;
    &lt;li&gt;&lt;a href=&quot;./deeplearning-summary&quot;&gt;Deep Learning - 핵심 개념 &amp;amp; 용어 &lt;/a&gt;&lt;/li&gt;
    &lt;li&gt;&lt;a href=&quot;./versatility&quot;&gt;NLP - 딥러닝 모델의 범용적 활용가능성 분석 &lt;/a&gt;&lt;/li&gt;
    &lt;li&gt;&lt;a href=&quot;./bank-marketing&quot;&gt;Bank marketing 정기예금 가입여부 예측 &lt;/a&gt;&lt;/li&gt;
    &lt;li&gt;&lt;a href=&quot;./roccurve&quot;&gt;ROC curve에 대해 알아보자 &lt;/a&gt;&lt;/li&gt;
    &lt;li&gt;&lt;a href=&quot;./machinelearning-linearregression&quot;&gt;연어의 회귀본능이 아닌 선형회귀에 대해 알아보자 &lt;/a&gt;&lt;/li&gt;  
    
        
    
    


    &lt;!-- &lt;li&gt;&lt;a href=&quot;./proposal&quot;&gt;코스레스토랑에 보내는 제안서 &lt;/a&gt;&lt;/li&gt; --&gt;
    &lt;!-- &lt;li&gt;&lt;a href=&quot;./proposal&quot;&gt;코스레스토랑에 보내는 제안서 &lt;/a&gt;&lt;/li&gt; --&gt;
    &lt;!-- &lt;li&gt;&lt;a href=&quot;./proposal&quot;&gt;코스레스토랑에 보내는 제안서 &lt;/a&gt;&lt;/li&gt; --&gt;
    &lt;!-- &lt;li&gt;&lt;a href=&quot;./proposal&quot;&gt;코스레스토랑에 보내는 제안서 &lt;/a&gt;&lt;/li&gt; --&gt;
    &lt;!-- &lt;li&gt;&lt;a href=&quot;./proposal&quot;&gt;코스레스토랑에 보내는 제안서 &lt;/a&gt;&lt;/li&gt; --&gt;
    &lt;!-- &lt;li&gt;&lt;a href=&quot;./proposal&quot;&gt;코스레스토랑에 보내는 제안서 &lt;/a&gt;&lt;/li&gt; --&gt;
    &lt;!-- &lt;li&gt;&lt;a href=&quot;./proposal&quot;&gt;코스레스토랑에 보내는 제안서 &lt;/a&gt;&lt;/li&gt; --&gt;
    &lt;!-- &lt;li&gt;&lt;a href=&quot;./proposal&quot;&gt;코스레스토랑에 보내는 제안서 &lt;/a&gt;&lt;/li&gt; --&gt;
    &lt;!-- &lt;li&gt;&lt;a href=&quot;./proposal&quot;&gt;코스레스토랑에 보내는 제안서 &lt;/a&gt;&lt;/li&gt; --&gt;
    &lt;!-- &lt;li&gt;&lt;a href=&quot;./proposal&quot;&gt;코스레스토랑에 보내는 제안서 &lt;/a&gt;&lt;/li&gt; --&gt;
    &lt;!-- &lt;li&gt;&lt;a href=&quot;./proposal&quot;&gt;코스레스토랑에 보내는 제안서 &lt;/a&gt;&lt;/li&gt; --&gt;
    &lt;!-- &lt;li&gt;&lt;a href=&quot;./proposal&quot;&gt;코스레스토랑에 보내는 제안서 &lt;/a&gt;&lt;/li&gt; --&gt;
    &lt;!-- &lt;li&gt;&lt;a href=&quot;./proposal&quot;&gt;코스레스토랑에 보내는 제안서 &lt;/a&gt;&lt;/li&gt; --&gt;
    &lt;!-- &lt;li&gt;&lt;a href=&quot;./proposal&quot;&gt;코스레스토랑에 보내는 제안서 &lt;/a&gt;&lt;/li&gt; --&gt;
    &lt;!-- &lt;li&gt;&lt;a href=&quot;./proposal&quot;&gt;코스레스토랑에 보내는 제안서 &lt;/a&gt;&lt;/li&gt;     --&gt;









&lt;!-- 
    &lt;li&gt;&lt;a href=&quot;./test&quot;&gt;시험용 포스팅입니다 - 출력되어라&lt;/a&gt;&lt;/li&gt;
    &lt;li&gt;&lt;a href=&quot;./test&quot;&gt;시험용 포스팅입니다 - 출력되어라&lt;/a&gt;&lt;/li&gt;
    &lt;li&gt;&lt;a href=&quot;./test&quot;&gt;시험용 포스팅입니다 - 출력되어라&lt;/a&gt;&lt;/li&gt;
    &lt;li&gt;&lt;a href=&quot;./test&quot;&gt;시험용 포스팅입니다 - 출력되어라&lt;/a&gt;&lt;/li&gt;
    &lt;li&gt;&lt;a href=&quot;./test&quot;&gt;시험용 포스팅입니다 - 출력되어라&lt;/a&gt;&lt;/li&gt;
    &lt;li&gt;&lt;a href=&quot;./test&quot;&gt;시험용 포스팅입니다 - 출력되어라&lt;/a&gt;&lt;/li&gt;
    &lt;li&gt;&lt;a href=&quot;./test&quot;&gt;시험용 포스팅입니다 - 출력되어라&lt;/a&gt;&lt;/li&gt;
    &lt;li&gt;&lt;a href=&quot;./test&quot;&gt;시험용 포스팅입니다 - 출력되어라&lt;/a&gt;&lt;/li&gt;
    &lt;li&gt;&lt;a href=&quot;./test&quot;&gt;시험용 포스팅입니다 - 출력되어라&lt;/a&gt;&lt;/li&gt;
    &lt;li&gt;&lt;a href=&quot;./test&quot;&gt;시험용 포스팅입니다 - 출력되어라&lt;/a&gt;&lt;/li&gt;
    &lt;li&gt;&lt;a href=&quot;./test&quot;&gt;시험용 포스팅입니다 - 출력되어라&lt;/a&gt;&lt;/li&gt;
    &lt;li&gt;&lt;a href=&quot;./test&quot;&gt;시험용 포스팅입니다 - 출력되어라&lt;/a&gt;&lt;/li&gt;
    &lt;li&gt;&lt;a href=&quot;./test&quot;&gt;시험용 포스팅입니다 - 출력되어라&lt;/a&gt;&lt;/li&gt;
    &lt;li&gt;&lt;a href=&quot;./test&quot;&gt;시험용 포스팅입니다 - 출력되어라&lt;/a&gt;&lt;/li&gt;
    &lt;li&gt;&lt;a href=&quot;./test&quot;&gt;시험용 포스팅입니다 - 출력되어라&lt;/a&gt;&lt;/li&gt;
    &lt;li&gt;&lt;a href=&quot;./test&quot;&gt;시험용 포스팅입니다 - 출력되어라&lt;/a&gt;&lt;/li&gt;
    &lt;li&gt;&lt;a href=&quot;./test&quot;&gt;시험용 포스팅입니다 - 출력되어라&lt;/a&gt;&lt;/li&gt;
    &lt;li&gt;&lt;a href=&quot;./test&quot;&gt;시험용 포스팅입니다 - 출력되어라&lt;/a&gt;&lt;/li&gt;
    &lt;li&gt;&lt;a href=&quot;./test&quot;&gt;시험용 포스팅입니다 - 출력되어라&lt;/a&gt;&lt;/li&gt;
    &lt;li&gt;&lt;a href=&quot;./test&quot;&gt;시험용 포스팅입니다 - 출력되어라&lt;/a&gt;&lt;/li&gt;
    &lt;li&gt;&lt;a href=&quot;./test&quot;&gt;시험용 포스팅입니다 - 출력되어라&lt;/a&gt;&lt;/li&gt;
  --&gt;
    
&lt;/ul&gt;

&lt;h1 id=&quot;deep-learning-summary---핵심-개념--용어&quot;&gt;Deep Learning Summary - 핵심 개념 &amp;amp; 용어&lt;/h1&gt;

&lt;p&gt;이제는 대세가 된 단어인 딥러닝. 대중적인 관심이 많이 늘어났지만 관련 분야를 공부를 하지 않은 일반인에겐 어려운 개념이나 용어가 많아 이해하기가 쉽지 않다. 그래서 오늘은 내가 배운 내용을 정리할 겸 일반인이 보기에도 이해하기 쉽도록 각 용어와 개념들을 설명해보려고 한다.&lt;/p&gt;

&lt;hr /&gt;
&lt;p&gt;&lt;img src=&quot;https://user-images.githubusercontent.com/70134676/104846469-0d2ee480-591e-11eb-86ee-e8bb5705f5ce.png&quot; width=&quot;100%&quot; height=&quot;100%&quot; /&gt;
전체적인 딥러닝 개요도&lt;/p&gt;

&lt;h2 id=&quot;1-신경망-기초&quot;&gt;1. 신경망 기초&lt;/h2&gt;
&lt;p&gt;&lt;img src=&quot;https://user-images.githubusercontent.com/70134676/104845364-60059d80-5918-11eb-8e0f-d48adb341dd8.png&quot; width=&quot;80%&quot; height=&quot;80%&quot; /&gt;&lt;/p&gt;

&lt;p&gt;(1) 뉴런&lt;br /&gt;
인공신경망에서 세포의 역할을 하고 신경망 구조도에서는 노드로 표현된다. 
뉴런마다 각각 입력을 받게되고 활성함수를 거쳐 다음 뉴런으로 출력한다.&lt;/p&gt;

&lt;p&gt;(2) weight(가중치)&lt;br /&gt;
인공신경망에서 뉴런을 연결하는 선으로 각 가중치들은 입력받은 신호를 필터링하는 역할을 한다.
입력받은 값에 어느정도의 가중을 주어 다음 뉴런에 전달할 것인지 해당 데이터의 경중을 결정하는 요소이다.&lt;/p&gt;

&lt;p&gt;(3) bias(편향)&lt;br /&gt;
뉴런들마다 각각 입력된 값과 가중치를 곱한 다음 더해주는 상수이다. 각 뉴런이 가지고있는 고유값이다.&lt;/p&gt;

&lt;p&gt;(4) 활성화함수&lt;br /&gt;
하나의 뉴런에서 다음 뉴런으로 가중합을 전달할 때, 값을 전달할지 말지를 결정하는 기준이 되는 함수이다. 임계점을 기준으로 출력값에 변화를 주게된다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://t1.daumcdn.net/cfile/tistory/994C83365D4FD0D125&quot; width=&quot;80%&quot; height=&quot;80%&quot; /&gt;&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;sigmoid&lt;br /&gt;
이진분류를 할 때 사용되며 기본적으로 임계값은 0.5로 이 값을 기준으로 작으면 0으로, 크면 1로 분류한다.&lt;/li&gt;
  &lt;li&gt;ReLu&lt;br /&gt;
0을 기준으로 0보다 작을 때는 0을, 클 때는 입력값 자체를 출력하는 기울기가 1인 함수&lt;/li&gt;
  &lt;li&gt;softmax&lt;br /&gt;
다중분류문제에 사용되며 0~1 사이의 값을 출력하며 확률값이기 때문에 전부 더하면 1이 된다.&lt;/li&gt;
  &lt;li&gt;Tanh&lt;br /&gt;
시그모이드의 변형버전으로 임계값을 기준으로 -1, 1로 분류한다.&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;2-신경망-학습&quot;&gt;2. 신경망 학습&lt;/h2&gt;

&lt;p&gt;(1) 역전파 &amp;amp; 경사하강법&lt;br /&gt;
실제 관측값과 예측값 사이의 오차가 최소가 되도록 역전파 알고리즘은 순전파와 반대로 거슬러올라가며 경사하강법을 사용하여 가중치를 업데이트한다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://user-images.githubusercontent.com/70134676/104846115-4f572680-591c-11eb-85ce-a559d3f55d7f.png&quot; width=&quot;80%&quot; height=&quot;80%&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://user-images.githubusercontent.com/70134676/104846131-6138c980-591c-11eb-8d1d-183b1c274987.png&quot; width=&quot;80%&quot; height=&quot;80%&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://user-images.githubusercontent.com/70134676/104846360-6c402980-591d-11eb-8643-190f357bdaab.png&quot; width=&quot;80%&quot; height=&quot;80%&quot; /&gt;&lt;/p&gt;

&lt;p&gt;(2) 손실함수&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;분류
    &lt;ul&gt;
      &lt;li&gt;binary cross-entropy&lt;br /&gt;
이진분류할 때 사용하는 함수로 0에 가까울수록 좋다.&lt;/li&gt;
      &lt;li&gt;categorical cross-entropy&lt;br /&gt;
다중분류에 쓰이는 함수로 0에 가까울 수록 좋다.&lt;/li&gt;
      &lt;li&gt;sparse cross-entropy&lt;br /&gt;
categoricalCrossentropy 와 비슷하지만, 레이블이 int 형이라는 점에서 다르다.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;회귀
    &lt;ul&gt;
      &lt;li&gt;MAE&lt;br /&gt;
관측값과 예측값의 차이에 절대값을 취하여 모두 합한 후 평균을 계산한 값이다. 실제값과 단위가 똑같아 직관적인 비교가 가능하다.&lt;/li&gt;
      &lt;li&gt;MSE&lt;br /&gt;
관측값과 예측값의 차이를 제곱하여 평균을 계산한 값이다. MAE보다 아웃라이어에 민감하다.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;(3) optimizer&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;SGD&lt;br /&gt;
경사하강법은 lost function를 미분을 통하여 최소값을 찾아가는 과정이다. 경사하강법 에서는 학습률를 중요하게 생각해야하는데, 이것이 너무 작을땐 탐색시간이 한참걸릴수도 있고, 너무 클땐 최소 손실값을 잘못 찾을수도 있기에 (전역 최솟값을 건너뛰고 지역 최솟값을 찾는 문제) 비교적 비효율적이다. 그래서 SGD는 GD 와 다르게, 한번 학습할 때 모든 데이터에 대해 가중치를 조절하는 것이 아니라, 랜덤하게 추출한 일부 데이터에 대해 가중치를 조절한다.&lt;/li&gt;
  &lt;li&gt;RMSprop&lt;br /&gt;
AdaGrad 은 학습률을 변수들에 다르게 적용하는 방법이다. 변화가 많았던 변수들엔 학습률을 작게하고, 그렇지 않은 변수들엔 크게 하는것이다. 많은 변화가 있었다는 것은 최적값에 가깝다는 뜻이니 학습률을 작게해서 세밀하게 조정하기 위함이다. 하지만, AdaGrad 로인해 너무 극단적으로 학습률이 작아질때 생기는 학습이 멈추는 문제가 있었는데 이것을 보완한 것이 RMSprop 이다.&lt;/li&gt;
  &lt;li&gt;Adam&lt;br /&gt;
Momentum 은 GD 의 학습률로 인한 문제를 기울기의 정도에따라 관성을 적용함으로써 완화한 것이다. Adam 은 Momentum 과 RMSprop 이 가지는 장점을 동시에 가지는 컨셉으로 생겨났다.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;(4) 학습 규제&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;dropout&lt;br /&gt;
노드를 랜덤하게 일시적으로 끊어낸다. 해당 뉴런을 물리적으로 차단하여 학습을 실시, 과적합을 방지한다.&lt;/li&gt;
  &lt;li&gt;early stopping&lt;br /&gt;
학습을 진행하면서 사용자가 정한 임계값 이상의 성능 개선이 없으면 학습을 조기종료하여 과적합을 방지한다.&lt;/li&gt;
  &lt;li&gt;weight decay&lt;br /&gt;
가중치가 커지는 것을 방지하도록 가중치를 감소시키는 방법으로 과적합을 방지한다.&lt;/li&gt;
  &lt;li&gt;learning rate decay&lt;br /&gt;
학습할수록 학습률을 감소시켜 과적합을 방지한다.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;(5) 메모리 사용 전략&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;batch&lt;br /&gt;
모델의 가중치를 업데이트 시킬 때 사용되는 데이터의 단위를 말한다. 1000개로 구성된 데이터가 있을 때, 배치 사이즈를 100으로 설정하면 100개 단위로 가중치를 업데이트 한다. 전체 데이터에 대하여 총 10번의 가중치가 업데이트된다.&lt;/li&gt;
  &lt;li&gt;epoch &amp;amp; iteration
학습 횟수를 의미하며 1000개로 구성된 데이터에서 batch size가 10이고 epoch이 10이면 100번의 가중치 업데이트를 한 사이클로 하여 총 10 사이클을 반복하여 실시한다. 이때 100번의 가중치 없데이트 횟수가 iteration이 된다. 1epoch은 데이터 전체를 한번 사용하는 것을 기준으로 한다.&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;3-cnn&quot;&gt;3. CNN&lt;/h2&gt;

&lt;p&gt;(1) convolution&lt;br /&gt;
합성곱을 통하여 fully connected 신경망보다 가중치의 갯수가 줄어들어 결과적으로 학습시간이 훨씬 빠르고 컴퓨팅 파워를 절약시켜준다.&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;stride&lt;br /&gt;
filter를 움직이는 보폭을 의미한다. stride가 1이면 filter를 한칸씩 움직이며 특징을 추출한다. stride가 작을수록 세세하게 필터링하게된다.&lt;/li&gt;
  &lt;li&gt;padding&lt;br /&gt;
채널은 필연적으로 입력데이터보다 작아지게 되는데 특성추출이 반복되어 차원이 0이 되는 것을 방지하는 방법이다. 데이터 가장자리에 0, 평균값 등 다양한 특정값을 채워넣어 차원축소를 방지한다. 패딩을 해주면 데이터를 골고루 사용하게 되고 합성곱 연산 이후에도 차원이 유지된다.&lt;/li&gt;
  &lt;li&gt;filter&lt;br /&gt;
weight들의 집합으로 데이터의 특징을 추출하는 window 역할을 한다.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;(2) pooling&lt;br /&gt;
채널의 피처맵 차원을 축소하는 방법으로 합성곱 연산 이후 레이어를 풀링하여 더 작은 피처맵을 추출한다.&lt;/p&gt;

&lt;h2 id=&quot;4-rnn&quot;&gt;4. RNN&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;RNN&lt;br /&gt;
은닉층의 출력데이터가 다시 자기자신 노드로 순환되어 입력값으로 사용된다. 이런 특성 때문에 시계열 데이터를 다루는 데에 좋은 성능을 낸다. 예를 들어, 주가데이터, 음성데이터, 텍스트데이터 같이 앞 뒤 데이터간의 연관성이 있는 데이터셋에 사용될 수 있다.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;LSTM&lt;br /&gt;
RNN의 문제 중 하나는 시계열 데이터를 다룰 때, 정보의 위치가 멀수록 역전파를 할 때 경사가 0에 수렴하기 때문에 모델 성능에 악영향을 미친다는 것이다. 
“나는 한국에서 한국어를 배운다”라는 문장에서 “한국어”라는 단어를 예측한다고 할때 문장에서 핵심적인 정보를 제공하는 단어는 “한국”이다. 이처럼 “한국”과 “한국어”의 위치가 가까이에 있다면 문맥을 연결하기가 쉽지만 멀리 위치해있다면 문맥을 연결하기 힘들어 성능이 저하된다. 역전파 과정에서 과거와 현재의 거리가 멀어질수록 gradient 값이 소실되는 gradient vanishing 문제가 대두되어 LSTM의 개념이 고안되었다.&lt;br /&gt;
이전 정보를 얼마나 잊을지, 현재 정보를 어느정도로 반영할지, 정보를 밖으로 얼마나 출력할지를 결정하는 Gate를 추가하여 RNN을 보완한다.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;GRU&lt;br /&gt;
LSTM의 게이트 숫자를 줄여 모델을 구조적으로 단순화했다는 점이 다르다. forget과 input gate를 합치고 output gate를 생략하였다.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;5-gan&quot;&gt;5. GAN&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;DCGAN&lt;br /&gt;
생성모델의 일종으로 실제로 값어치가 있는 그럴싸한 데이터를 생성해내는데 그 존재 이유가 있다. 생성자 모델과 감별자 모델 두가지가 서로 대립하여 서로 경쟁적으로 학습하여 모델의 성능을 향상시키는 컨셉이다.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;CycleGAN&lt;br /&gt;
데이터의 원래 형태는 유지하면서 세부 특성만 교체할 수 있게 하는 모델이다. 각각 두 개의 생성자와 감별자를 두어 특성을 추출한다. (말, 얼룩말), (오렌지, 사과) 이미지 쌍을 입력하여 학습을 할 때는 말을 얼룩말으로 바꾼 이미지는 얼룩말인지 아닌지 판단하여 얼룩말이라고 판단되도록, 또 변경된 이미지를 다시 말로 바꿔서 원본과 비교함으로써 다른 속성들은 그대로 유지되었는지 판단하며 그 차이를 줄이는 방향으로 학습한다.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;6-autoencoder&quot;&gt;6. Autoencoder&lt;/h2&gt;

&lt;p&gt;오토인코더는 인코더와 디코더로 구성되어있다. 인코더는 입력 데이터를 압축하여 벡터화 하게되고 디코더는 압축된 벡터를 다시 원본데이터와 유사하게 복원하는 역할을 한다.&lt;br /&gt;
원래의 입력데이터 x와 압축, 복원을 거친 x’의 차이를 손실로 정의하고 손실이 줄어들 수 있도록 역전파를 통해 학습이 이루어진다. 입력데이터를 벡터화하는 과정에서 입력 데이터를 정의할 수 있는 잠재변수가 만들어지는데 이 잠재변수의 크기가 커지면 데이터를 구분할 수 있는 더 다양한 특징들이 추출된다. 반대로 잠재변수의 크기를 줄이면 특징의 수는 줄어들지만 더 핵심적으로 데이터를 구분할 수 있는 특징들이 추출된다.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;Encoder : 입력 데이터의 특징을 담고있는 Latent 로 데이터를 압축시키는 단계.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Decoder : 압축된 Latent 를 입력데이터와 유사하게 출력하기위해 확장하는 단계.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;오토인코더의 용도&lt;/p&gt;
    &lt;ul&gt;
      &lt;li&gt;노이즈 제거&lt;/li&gt;
      &lt;li&gt;특성 추출&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;7-attention&quot;&gt;7. Attention&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;Transformer&lt;br /&gt;
자연어처리 분야에서 사용되는 기술로 문장 데이터를 한번에 입력하고 순서정보를 입력해 번역하는 모델. 크게 음성을 텍스트로 변환하거나 텍스트를 음성으로, 또는 텍스트를 다른 언어로 번역하는 모델 등이 있다.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;BERT&lt;br /&gt;
&lt;img src=&quot;https://user-images.githubusercontent.com/70134676/104848544-cbf00200-5928-11eb-8dc2-faf07c786b37.png&quot; alt=&quot;image&quot; /&gt;&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;구글에서 개발하고 사전훈련시킨 자연어처리에 사용되는 범용 언어모델이다. 총 33억 개의 코퍼스(bookcorpus : 8억, wikipedia : 25억)를 사용하여 개인이 구축하기 힘든정도의 방대한 양의 코퍼스(의미뭉치)로 상당한 시간 사전학습을 마쳐놓아 상당한 성능의 모델을 바로 가져다 쓸 수 있게 구현 해놓다.&lt;/p&gt;

&lt;p&gt;자연어를 처리할 때, 데이터가 충분하다면 Embedding 과정이 성능에 큰 영향을 미치는데 이 말은 단어의 의미를 잘 간직할 수 있도록 벡터로 표현하는 것이 중요하다는 의미이다. 이 Embedding 과정에 구글의 강력한 BERT를 이용하는 것이고 사용하고자 하는 문제에 맞도록 파인튜닝을 진행한 후에 분류에 적용하게 된다.&lt;/p&gt;</content>

      
      
      
      
      

      <author>
          <name>woongE</name>
        
        
      </author>

      

      
        <category term="ai" />
      

      
        <summary type="html">#deeplearning #용어 #개념</summary>
      

      
      
    </entry>
  
    <entry>
      <title type="html">NLP - 딥러닝 모델의 범용적 활용가능성 분석</title>
      <link href="http://localhost:4000/versatility" rel="alternate" type="text/html" title="NLP - 딥러닝 모델의 범용적 활용가능성 분석" />
      <published>2021-01-14T04:00:00+09:00</published>
      <updated>2021-01-14T04:00:00+09:00</updated>
      <id>http://localhost:4000/versatility</id>
      <content type="html" xml:base="http://localhost:4000/versatility">&lt;p&gt;#BERT #textmining #deeplearning&lt;/p&gt;

&lt;p&gt;&lt;span class=&quot;table-of-contents-list&quot;&gt;목차 &lt;/span&gt;&lt;/p&gt;
&lt;ul class=&quot;table-of-contents-list&quot;&gt;
    &lt;li&gt;&lt;a href=&quot;./tadgan-modelstructure&quot;&gt;Time seriesAnomaly Detection Generative Adversarial Networks-Model Structure &lt;/a&gt;&lt;/li&gt;  
    &lt;li&gt;&lt;a href=&quot;./tadgan-lossfunction&quot;&gt;Time seriesAnomaly Detection Generative Adversarial Networks-loss function &lt;/a&gt;&lt;/li&gt;  
    &lt;li&gt;&lt;a href=&quot;./tadgan-intro&quot;&gt;Intro to the Time Series Anomaly Detection &lt;/a&gt;&lt;/li&gt;  
    &lt;li&gt;&lt;a href=&quot;./text-ads-filtering&quot;&gt;Text 데이터 광고 필터링을 위한 분류 모델 구축 &lt;/a&gt;&lt;/li&gt;
    &lt;li&gt;&lt;a href=&quot;./bert-sentiment-keyword&quot;&gt;사용자 리뷰 핵심어 추출 및 감성분석 &lt;/a&gt;&lt;/li&gt;
    &lt;li&gt;&lt;a href=&quot;./deeplearning-summary&quot;&gt;Deep Learning - 핵심 개념 &amp;amp; 용어 &lt;/a&gt;&lt;/li&gt;
    &lt;li&gt;&lt;a href=&quot;./versatility&quot;&gt;NLP - 딥러닝 모델의 범용적 활용가능성 분석 &lt;/a&gt;&lt;/li&gt;
    &lt;li&gt;&lt;a href=&quot;./bank-marketing&quot;&gt;Bank marketing 정기예금 가입여부 예측 &lt;/a&gt;&lt;/li&gt;
    &lt;li&gt;&lt;a href=&quot;./roccurve&quot;&gt;ROC curve에 대해 알아보자 &lt;/a&gt;&lt;/li&gt;
    &lt;li&gt;&lt;a href=&quot;./machinelearning-linearregression&quot;&gt;연어의 회귀본능이 아닌 선형회귀에 대해 알아보자 &lt;/a&gt;&lt;/li&gt;  
    
        
    
    


    &lt;!-- &lt;li&gt;&lt;a href=&quot;./proposal&quot;&gt;코스레스토랑에 보내는 제안서 &lt;/a&gt;&lt;/li&gt; --&gt;
    &lt;!-- &lt;li&gt;&lt;a href=&quot;./proposal&quot;&gt;코스레스토랑에 보내는 제안서 &lt;/a&gt;&lt;/li&gt; --&gt;
    &lt;!-- &lt;li&gt;&lt;a href=&quot;./proposal&quot;&gt;코스레스토랑에 보내는 제안서 &lt;/a&gt;&lt;/li&gt; --&gt;
    &lt;!-- &lt;li&gt;&lt;a href=&quot;./proposal&quot;&gt;코스레스토랑에 보내는 제안서 &lt;/a&gt;&lt;/li&gt; --&gt;
    &lt;!-- &lt;li&gt;&lt;a href=&quot;./proposal&quot;&gt;코스레스토랑에 보내는 제안서 &lt;/a&gt;&lt;/li&gt; --&gt;
    &lt;!-- &lt;li&gt;&lt;a href=&quot;./proposal&quot;&gt;코스레스토랑에 보내는 제안서 &lt;/a&gt;&lt;/li&gt; --&gt;
    &lt;!-- &lt;li&gt;&lt;a href=&quot;./proposal&quot;&gt;코스레스토랑에 보내는 제안서 &lt;/a&gt;&lt;/li&gt; --&gt;
    &lt;!-- &lt;li&gt;&lt;a href=&quot;./proposal&quot;&gt;코스레스토랑에 보내는 제안서 &lt;/a&gt;&lt;/li&gt; --&gt;
    &lt;!-- &lt;li&gt;&lt;a href=&quot;./proposal&quot;&gt;코스레스토랑에 보내는 제안서 &lt;/a&gt;&lt;/li&gt; --&gt;
    &lt;!-- &lt;li&gt;&lt;a href=&quot;./proposal&quot;&gt;코스레스토랑에 보내는 제안서 &lt;/a&gt;&lt;/li&gt; --&gt;
    &lt;!-- &lt;li&gt;&lt;a href=&quot;./proposal&quot;&gt;코스레스토랑에 보내는 제안서 &lt;/a&gt;&lt;/li&gt; --&gt;
    &lt;!-- &lt;li&gt;&lt;a href=&quot;./proposal&quot;&gt;코스레스토랑에 보내는 제안서 &lt;/a&gt;&lt;/li&gt; --&gt;
    &lt;!-- &lt;li&gt;&lt;a href=&quot;./proposal&quot;&gt;코스레스토랑에 보내는 제안서 &lt;/a&gt;&lt;/li&gt; --&gt;
    &lt;!-- &lt;li&gt;&lt;a href=&quot;./proposal&quot;&gt;코스레스토랑에 보내는 제안서 &lt;/a&gt;&lt;/li&gt; --&gt;
    &lt;!-- &lt;li&gt;&lt;a href=&quot;./proposal&quot;&gt;코스레스토랑에 보내는 제안서 &lt;/a&gt;&lt;/li&gt;     --&gt;









&lt;!-- 
    &lt;li&gt;&lt;a href=&quot;./test&quot;&gt;시험용 포스팅입니다 - 출력되어라&lt;/a&gt;&lt;/li&gt;
    &lt;li&gt;&lt;a href=&quot;./test&quot;&gt;시험용 포스팅입니다 - 출력되어라&lt;/a&gt;&lt;/li&gt;
    &lt;li&gt;&lt;a href=&quot;./test&quot;&gt;시험용 포스팅입니다 - 출력되어라&lt;/a&gt;&lt;/li&gt;
    &lt;li&gt;&lt;a href=&quot;./test&quot;&gt;시험용 포스팅입니다 - 출력되어라&lt;/a&gt;&lt;/li&gt;
    &lt;li&gt;&lt;a href=&quot;./test&quot;&gt;시험용 포스팅입니다 - 출력되어라&lt;/a&gt;&lt;/li&gt;
    &lt;li&gt;&lt;a href=&quot;./test&quot;&gt;시험용 포스팅입니다 - 출력되어라&lt;/a&gt;&lt;/li&gt;
    &lt;li&gt;&lt;a href=&quot;./test&quot;&gt;시험용 포스팅입니다 - 출력되어라&lt;/a&gt;&lt;/li&gt;
    &lt;li&gt;&lt;a href=&quot;./test&quot;&gt;시험용 포스팅입니다 - 출력되어라&lt;/a&gt;&lt;/li&gt;
    &lt;li&gt;&lt;a href=&quot;./test&quot;&gt;시험용 포스팅입니다 - 출력되어라&lt;/a&gt;&lt;/li&gt;
    &lt;li&gt;&lt;a href=&quot;./test&quot;&gt;시험용 포스팅입니다 - 출력되어라&lt;/a&gt;&lt;/li&gt;
    &lt;li&gt;&lt;a href=&quot;./test&quot;&gt;시험용 포스팅입니다 - 출력되어라&lt;/a&gt;&lt;/li&gt;
    &lt;li&gt;&lt;a href=&quot;./test&quot;&gt;시험용 포스팅입니다 - 출력되어라&lt;/a&gt;&lt;/li&gt;
    &lt;li&gt;&lt;a href=&quot;./test&quot;&gt;시험용 포스팅입니다 - 출력되어라&lt;/a&gt;&lt;/li&gt;
    &lt;li&gt;&lt;a href=&quot;./test&quot;&gt;시험용 포스팅입니다 - 출력되어라&lt;/a&gt;&lt;/li&gt;
    &lt;li&gt;&lt;a href=&quot;./test&quot;&gt;시험용 포스팅입니다 - 출력되어라&lt;/a&gt;&lt;/li&gt;
    &lt;li&gt;&lt;a href=&quot;./test&quot;&gt;시험용 포스팅입니다 - 출력되어라&lt;/a&gt;&lt;/li&gt;
    &lt;li&gt;&lt;a href=&quot;./test&quot;&gt;시험용 포스팅입니다 - 출력되어라&lt;/a&gt;&lt;/li&gt;
    &lt;li&gt;&lt;a href=&quot;./test&quot;&gt;시험용 포스팅입니다 - 출력되어라&lt;/a&gt;&lt;/li&gt;
    &lt;li&gt;&lt;a href=&quot;./test&quot;&gt;시험용 포스팅입니다 - 출력되어라&lt;/a&gt;&lt;/li&gt;
    &lt;li&gt;&lt;a href=&quot;./test&quot;&gt;시험용 포스팅입니다 - 출력되어라&lt;/a&gt;&lt;/li&gt;
    &lt;li&gt;&lt;a href=&quot;./test&quot;&gt;시험용 포스팅입니다 - 출력되어라&lt;/a&gt;&lt;/li&gt;
  --&gt;
    
&lt;/ul&gt;

&lt;h1 id=&quot;section-4-project---text-emotion-classification&quot;&gt;Section 4 Project - Text emotion classification&lt;/h1&gt;

&lt;p&gt;Deep Learning 섹션의 마무리 프로젝트는 텍스트 감정분석 모델의 범용적인 활용 가능성을 검증해보는 것으로 결정했다. 
아직까지는 일주일이라는 짧은 시간동안 데이터셋과 레퍼런스 코드를 찾고 그것들을 내 환경에서 완벽하게 구현해내는 것들이 익숙하지 않아 주제 선정에 어느정도 제약이 따를 수밖에 없었다. 그래서 평소 관심있던 분야 중, 내가 지금 일주일 안에 어느정도의 완성도 이상을 결과물로 낼 수 있을 것 같은 모델과 데이터셋을 선정했다. 
내가 정한 분야와 데이터셋에서 적절한 가설을 세우고 그 가설을 검증하는데 필요한 딥러닝 파이프라인을 구축하는 것이 프로젝트의 목표였다.
가설 설정을 위해 끊임없이 질문을 던지고 그 질문을 어떻게 해결해야할지, 검증은 어떻게 해야 할지를 고민하며 밤을 지새웠던 지난 일주일 간의 고군분투기를 적어본다.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://github.com/lwoongh38/project4_NLP_BERT&quot; title=&quot;프로젝트링크&quot;&gt;Github repo&lt;/a&gt;&lt;/p&gt;

&lt;h2 id=&quot;abstract&quot;&gt;abstract&lt;/h2&gt;

&lt;p&gt;기업은 최소한의 자원을 투자하여 최대의 이익을 보기 위하여 노력한다. 본 프로젝트는 제품이나 서비스에 대한 감상평을 남기는 리뷰와 같은 텍스트 데이터를 활용하여 해당 제품이나 서비스에 대한 감정이 긍정적인지, 부정적인지 분류하는 감정분류를 실시한다.
감정 분류를 실시할 때, 매번 새로운 모델링을 하지않고 분야별로 한 번의 파인튜닝을 실시한 모델로 범용적 활용 가능성에 대해 고찰한다.&lt;/p&gt;

&lt;hr /&gt;

&lt;h2 id=&quot;1-서론&quot;&gt;1. 서론&lt;/h2&gt;

&lt;p&gt;&lt;img src=&quot;https://user-images.githubusercontent.com/70134676/104591610-495d0d80-56b0-11eb-8315-5dc77180693b.png&quot; width=&quot;90%&quot; height=&quot;100%&quot; /&gt;&lt;/p&gt;

&lt;p&gt;마케팅, 누구라도 한 번쯤은 들어봤을 만한 단어입니다.
마케팅은 목적하는 바를 달성하기 위해 유도하는 모든 활동을 말합니다.
우리가 흔하게 이야기하는 마케팅은 경제적인 관점에서의 소비를 유도하기 위해 하는 활동들입니다.&lt;/p&gt;

&lt;p&gt;과거에는 기업이 제품을 만들어 소비의 방향을 선도하고 소비자는 만들어진 기업의 소비 트렌드를 따르던 패턴이었습니다. 하지만 현재에 이르러서는 경제적인 여유와 의사표현수단이 다양해지면서 소비자가 트렌드를 선도하고 기업이 이에 맞는 제품이나 서비스를 제공하는 형태로 경제활동의 패러다임이 크게 변화했습니다.&lt;br /&gt;
이런 환경속에서 마케팅 분야의 가장 큰 관심사는 “소비자나 사용자의 마음을 파악하는 것”이 되었습니다. 소비자의 마음을 알아야 알맞은 서비스나 제품을 만들 수 있고 또 만들어진 상품을 잘 팔 수 있기 때문입니다.&lt;/p&gt;

&lt;h3 id=&quot;11-가설-설정&quot;&gt;1.1 가설 설정&lt;/h3&gt;
&lt;p&gt;최근 기업들은 소셜미디어나 리뷰 텍스트 등을 기반으로 감정을 분석하여 마케팅에 활용하는 사례들이 점점 많아지고 있습니다.&lt;br /&gt;
감정분석을 하는 프로세스를 크게 나누어보면,&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;감정을 분석하고자 하는 데이터를 정제&lt;/li&gt;
  &lt;li&gt;데이터의 고유특성을 고려하여 분류모델 설계&lt;/li&gt;
  &lt;li&gt;생성한 모델에 데이터를 학습&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;이 과정은 보기엔 단순해보일지 모르나 데이터의 복잡성이나 크기에 따라 상당히 많은 시간을 소요하게 됩니다. 문제는 데이터가 바뀔 때마다 저 모든 과정을 다시 거쳐야 한다는 것입니다.&lt;/p&gt;

&lt;p&gt;그래서 저는 자연어 처리분야에서 소비자의 텍스트데이터를 활용하여 감성분류를 할 때, 다양한 분야에 범용적으로 활용할 수 있는 모델이 있으면 좋지 않을까 라는 생각을 해보게 됐습니다.&lt;/p&gt;

&lt;p&gt;이 프로젝트를 진행하면서 검증할 저의 가설을 세워봤습니다.&lt;/p&gt;
&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;“같은 언어를 사용하고 감정을 이진 분류 한다는 가정 하에, 한 분야에 대해 학습한 모델이 다른 분야에 사용될 수 있다”
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;
&lt;p&gt;이런 가설을 생각해보게 된 이유는 제가 감정분류를 할 때 사용할 모델을 찾아보다가 구글의 BERT라는 모델에 대해 알게되었고 이를 사용하기로 마음먹었기 때문입니다.&lt;br /&gt;
구글이라는 큰 기업에서 방대한 양의 코퍼스로 사전학습을 시켜놓았기 때문에 비슷한 형태의 데이터에 수행할 내용이 같다면 성능차이가 크지 않을 것이라는 생각에서 이런 가설을 세우게 되었습니다.&lt;/p&gt;

&lt;h2 id=&quot;2-modeling&quot;&gt;2. Modeling&lt;/h2&gt;

&lt;h3 id=&quot;21-모델-결정---bert&quot;&gt;2.1 모델 결정 - BERT&lt;/h3&gt;

&lt;p&gt;&lt;img src=&quot;https://img1.daumcdn.net/thumb/R1280x0/?scode=mtistory2&amp;amp;fname=https%3A%2F%2Fblog.kakaocdn.net%2Fdn%2FcEoPYe%2FbtqBW0v9pJo%2FxM7PQl9BL0XAKX9fYuphw1%2Fimg.png&quot; width=&quot;90%&quot; height=&quot;100%&quot; /&gt;&lt;/p&gt;

&lt;p&gt;간단하게 BERT를 설명해보면,&lt;br /&gt;
구글에서 개발하고 사전훈련시킨 자연어처리에 사용되는 범용 언어모델입니다. 총 33억 개의 코퍼스(bookcorpus : 8억, wikipedia : 25억)를 사용하여 개인이 구축하기 힘든정도의 방대한 양의 코퍼스(의미뭉치)로 상당한 시간 사전학습을 마쳐놓아 상당한 성능의 모델을 바로 가져다 쓸 수 있게 구현 해놓았습니다.&lt;/p&gt;

&lt;p&gt;자연어를 처리할 때, 데이터가 충분하다면 Embedding 과정이 성능에 큰 영향을 미치게 됩니다. 이 말은 단어의 의미를 잘 간직할 수 있도록 벡터로 표현하는 것이 중요하다는 의미입니다. 이 Embedding 과정에 구글의 강력한 BERT를 이용하는 것이고 제가 사용하고자 하는 문제에 맞도록 파인튜닝을 진행한 후에 분류에 적용하게 됩니다.&lt;/p&gt;

&lt;h3 id=&quot;22-데이터셋-결정---네이버쇼핑-리뷰-네이버영화-리뷰-steam-리뷰&quot;&gt;2.2 데이터셋 결정 - 네이버쇼핑 리뷰, 네이버영화 리뷰, steam 리뷰&lt;/h3&gt;

&lt;p&gt;프로젝트를 진행하기 위하여 제가 수집한 데이터는 세 종류입니다.&lt;br /&gt;
steam 리뷰는 게임을 판매, 구동시켜주는 steam 플랫폼의 리뷰로 추천, 비추천 두 가지로 평가되며,&lt;br /&gt;
네이버 쇼핑, 영화 리뷰는 별점 5점으로 평가된 데이터로 중간값인 3점은 제외되고 1,2,4,5 점의 리뷰만 수집된 데이터로 1,2 점은 부정, 4,5점은 긍정으로 다시 분류하였습니다. 각 데이터는 긍정과 부정 비율을 동등하게 맞추어 샘플링 되었기 때문에 추후 평가에 사용될 기준모델의 정확도는 50%입니다.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;네이버 쇼핑 리뷰&lt;br /&gt;
&lt;img src=&quot;https://user-images.githubusercontent.com/70134676/104597410-a2c93a80-56b8-11eb-988e-94e685d71d4a.png&quot; alt=&quot;image&quot; /&gt;&lt;br /&gt;
쇼핑데이터는 물품에 대한 만족, 불만족이 비교적 텍스트에 단순하게 나타나 있는 모습을 볼 수 있습니다.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;네이버 영화 리뷰&lt;br /&gt;
&lt;img src=&quot;https://user-images.githubusercontent.com/70134676/104597460-b379b080-56b8-11eb-830a-b95895e07b18.png&quot; alt=&quot;image&quot; /&gt;&lt;br /&gt;
영화데이터는 영화에 대한 정보가 함축적으로 들어있거나 조금 더 다양한 어휘가 사용되고 있습니다.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;steam 리뷰&lt;br /&gt;
&lt;img src=&quot;https://user-images.githubusercontent.com/70134676/104597471-b674a100-56b8-11eb-98c7-3e3206cd1a62.png&quot; alt=&quot;image&quot; /&gt;&lt;br /&gt;
스팀데이터의 경우는 비어, 속어, 은어 등이 다양하게 나타나있음을 볼 수 있습니다.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;3-검증&quot;&gt;3. 검증&lt;/h2&gt;

&lt;h3 id=&quot;31-실험-흐름&quot;&gt;3.1 실험 흐름&lt;/h3&gt;
&lt;p&gt;&lt;img src=&quot;https://user-images.githubusercontent.com/70134676/104598142-91ccf900-56b9-11eb-886d-c4652a5c335a.png&quot; alt=&quot;image&quot; /&gt;&lt;/p&gt;

&lt;p&gt;실험의 전체적인 흐름은,&lt;/p&gt;
&lt;ol&gt;
  &lt;li&gt;방대한 양의 코퍼스로 사전학습을 마친 모델을 불러오고&lt;/li&gt;
  &lt;li&gt;사용하길 원하는 데이터로 파인 튜닝을 실시합니다.&lt;/li&gt;
  &lt;li&gt;이후에 해당 태스크를 수행하게 됩니다.&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;따라서 저는 3가지 데이터를 각각 파인튜닝한 모델 3가지와 3가지 데이터 모두를 사용하여 파인튜닝한 모델까지 총 4개의 모델을 생성하고, 각 모델에 자신이 학습하지 않은 나머지 데이터셋을 분류시켜 정확도를 평가할 것입니다. 제가 세운 가설이 맞으려면 각 모델이 다른 데이터셋을 분류할 때도 정확도의 차이가 크게 나지 않아야 될 것입니다.&lt;/p&gt;

&lt;p&gt;시간적인 제약때문에 BERT를 사용한 모델 중 한국어의 리뷰모델을 분석할 때 성능이 좋았다는 모델의 파라미터를 서칭해서 그대로 대입한 후 모든 실험을 같은 조건 하에서 진행했습니다.&lt;/p&gt;

&lt;h3 id=&quot;32-실험-결과&quot;&gt;3.2 실험 결과&lt;/h3&gt;
&lt;p&gt;&lt;img src=&quot;https://user-images.githubusercontent.com/70134676/104598731-436c2a00-56ba-11eb-9b6b-79e152c162bd.png&quot; alt=&quot;image&quot; /&gt;&lt;/p&gt;

&lt;p&gt;기본 bert 모델에서 파인튜닝을 다르게 한 4가지의 모델을 만들었고  모델별 분류 정확도를 정리했습니다.&lt;/p&gt;

&lt;p&gt;모델별 분류 정확도를 정리했습니다.
당연할지도 모르겠지만 각 모델들이 학습에 사용된 데이터에 대해 분류할 때 예를들면 
쇼핑데이터 학습 모델은 쇼핑데이터를, 
영화데이터 학습 모델은 영화데이터를,
스팀데이터 학습 모델은 스팀데이터를 예측할 때
가장 높은 정확도를 보였습니다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://user-images.githubusercontent.com/70134676/104598874-6d255100-56ba-11eb-802a-7a7a620cebc9.png&quot; alt=&quot;image&quot; /&gt;&lt;/p&gt;

&lt;p&gt;특이한점을 살펴보면, 쇼핑모델의 경우는 학습한 데이터와 다른 분야의 데이터를 테스트했을 때
유독 정확도가 많이 떨어졌습니다.&lt;br /&gt;
쇼핑데이터를 추출해봤을 때, 전반적으로 영화리뷰나 스팀리뷰에 비해 리뷰내용이 단순하여 다른 리뷰를 분류 할때 정확도가 떨어진 것이 아닌가 추측됩니다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://user-images.githubusercontent.com/70134676/104598985-91812d80-56ba-11eb-8b90-9cbc1fed06c0.png&quot; alt=&quot;image&quot; /&gt;&lt;/p&gt;

&lt;p&gt;스팀모델의 경우도 특이한점이 발견됐는데 분야를 가리지 않고 정확도가 다른 모델들에 비해 낮게 나왔습니다. 이는 스팀 모델의 경우 데이터셋이 다른 모델보다 적기 때문이거나 구글에서 사전 훈련에 사용한 코퍼스에 비어, 속어, 은어는 없기 때문일 수도 있을 것 같습니다.&lt;/p&gt;

&lt;p&gt;또 다른 특이점은
스팀모델만 유독 학습데이터와 다른 분야의 데이터에서도 성능하락 폭이 크지 않았다는 점입니다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://user-images.githubusercontent.com/70134676/104599067-b07fbf80-56ba-11eb-94c1-1b647a44eac7.png&quot; alt=&quot;image&quot; /&gt;&lt;/p&gt;

&lt;p&gt;모든 데이터를 넣어 finetuning 한 ALL모델의 경우는 언뜻 생각하기엔 훈련데이터를 다 모아서 학습했으므로 전반적으로 성능이 오를 것으로 예상했으나 평가 정확도는 각 데이터만 훈련한 모델들과 같았습니다.&lt;br /&gt;
다만 스팀데이터의 경우엔 스팀데이터만 학습시킨 경우보다 정확도가 5%정도 상승했는데
이는 쇼핑, 영화데이터를 통해 학습한 내용 중 어떤 부분이 스팀데이터를 예측하는데 도움을 주었음을 알 수 있습니다.&lt;/p&gt;

&lt;h2 id=&quot;4-결론&quot;&gt;4. 결론&lt;/h2&gt;
&lt;p&gt;범용적인 모델을 만들 수 있을 것이라고 예상했던 저의 가설은 기준을 어디에 두느냐에 따라 완전히 틀리지도, 맞지도 않는 가설이 된 것 같습니다.&lt;br /&gt;
한 가지 데이터로 학습한 후 다른 분야 데이터를 분류했을 때, 대부분 성능이 적게는 3에서 많게는 23% 까지 차이가 났지만 그렇다고 하더라도 베이스라인 정확도 50%는 모두 큰 폭으로  상회하는 수치이기 때문입니다.&lt;/p&gt;

&lt;p&gt;각 분야에 특화된 모델보다는 정확도가 떨어지지만 이진분류와 같은 단순한 문제에서 일정부분 성능을 포기하더라도 학습을 별개로 진행할 필요가 없다는 점에서 범용모델로 사용할 수 있을 정도는 될 것이라고 생각했습니다.&lt;/p&gt;

&lt;h2 id=&quot;코드스테이츠의-데이터사이언티스트-코스-네-번째-섹션을-마무리하며&quot;&gt;코드스테이츠의 데이터사이언티스트 코스 네 번째 섹션을 마무리하며..&lt;/h2&gt;

&lt;p&gt;이번 섹션 프로젝트를 진행하며 가장 힘든 것은 가설을 설정하는 부분이었다. 대학원에서도 논문을 작성할 때 가장 힘든 것이 바로 ‘어떤 질문을 해야하는가’인데 이번 프로젝트는 마치 소논문 하나를 작성하는 것 같은 느낌을 받았던 프로젝트였다.&lt;/p&gt;

&lt;p&gt;가장 다뤄보고 싶었던 자율주행 자동차에 사용되는 윤리적 판단 모델은 이제 연구가 활발히 이루어지고 있는 분야여서 구현할 수 있는 레퍼런스모델과 코드들을 구하기가 쉽지 않아 눈물을 머금고 후퇴했다. 바로 Plan B로 생각했던 NLP분야에 대해 서칭하여 BERT라는 모델에 대해 알게되고, 처음에는 내가 배웠던 keras 프레임워크로 구현을 시도했는데 계속 내가 사용할 데이터셋에 적용하여 학습시키면 20분정도 학습이 진행되다가 오류를 뱉어내었다. 이것도 후퇴해야하나 싶을 때, 혹시모르니 해보자 싶어 도전했던 pytorch 프레임워크로 구현된 레퍼런스 모델과 코드를 활용하여 모델 구현에 성공했다. 
배우지 못했던 프레임워크라서 도전할 생각조차 안해봤었지만 막상 구현에 성공하고 보니 일단 해보려는 시도가 중요하다는 사실을 다시한번 몸으로 배울 수 있었다.&lt;/p&gt;

&lt;p&gt;짧다면 짧고 길다면 긴 4달 남짓한 시간동안 이 분야에 대해 공부하고 몸으로 부딪히며 자꾸 느끼게 되는 것은 정말 지식이 너무나도 방대한 분야라는 것이다. 나는 이 모든 지식들을 알 수는 없고 어차피 새로운 기술이 워낙 빠르게 등장하기 때문에 내가 당장은 이해할 수 없더라도 새로운 것을 받아들여 빠르게 내가 원하는 데이터에 맞게 적용할 수 있는 능력이 정말 중요하다는 것을 새삼 느낀다. 실제로 이런 과정을 거쳐 이번 프로젝트를 마친 지금, 파이토치에 대해 아주 작은 것 하나라도 전보다는 더 알게되지 않았을까.&lt;br /&gt;
몰랐던 기술이라도 꼭 구현할 수 없는 것은 아니라는 사실을 배웠다는 것이 이번 프로젝트에서 배운 가장 큰 수확이 아닐까 생각한다.&lt;/p&gt;</content>

      
      
      
      
      

      <author>
          <name>woongE</name>
        
        
      </author>

      

      
        <category term="ai" />
      

      
        <summary type="html">#BERT #textmining #deeplearning</summary>
      

      
      
    </entry>
  
    <entry>
      <title type="html">Twitter Web Application</title>
      <link href="http://localhost:4000/twitter-web-application" rel="alternate" type="text/html" title="Twitter Web Application" />
      <published>2020-12-14T01:40:00+09:00</published>
      <updated>2020-12-14T01:40:00+09:00</updated>
      <id>http://localhost:4000/twitter-web-application</id>
      <content type="html" xml:base="http://localhost:4000/twitter-web-application">&lt;p&gt;#Twitter #application #softwareengineering&lt;/p&gt;

&lt;p&gt;&lt;span class=&quot;table-of-contents-list&quot;&gt;목차 &lt;/span&gt;&lt;/p&gt;
&lt;ul class=&quot;table-of-contents-list&quot;&gt;
    &lt;li&gt;&lt;a href=&quot;./python-class&quot;&gt;Software Engineering - Python - &quot;Class&quot; 이녀석 &lt;/a&gt;&lt;/li&gt;
    &lt;li&gt;&lt;a href=&quot;./twitter-web-application&quot;&gt;Software Engineering - Twitter Web Application &lt;/a&gt;&lt;/li&gt;
    &lt;!-- &lt;li&gt;&lt;a href=&quot;./proposal&quot;&gt;코스레스토랑에 보내는 제안서 &lt;/a&gt;&lt;/li&gt; --&gt;
    &lt;!-- &lt;li&gt;&lt;a href=&quot;./proposal&quot;&gt;코스레스토랑에 보내는 제안서 &lt;/a&gt;&lt;/li&gt; --&gt;
    &lt;!-- &lt;li&gt;&lt;a href=&quot;./proposal&quot;&gt;코스레스토랑에 보내는 제안서 &lt;/a&gt;&lt;/li&gt; --&gt;
    &lt;!-- &lt;li&gt;&lt;a href=&quot;./proposal&quot;&gt;코스레스토랑에 보내는 제안서 &lt;/a&gt;&lt;/li&gt; --&gt;
    &lt;!-- &lt;li&gt;&lt;a href=&quot;./proposal&quot;&gt;코스레스토랑에 보내는 제안서 &lt;/a&gt;&lt;/li&gt; --&gt;
    &lt;!-- &lt;li&gt;&lt;a href=&quot;./proposal&quot;&gt;코스레스토랑에 보내는 제안서 &lt;/a&gt;&lt;/li&gt; --&gt;
    &lt;!-- &lt;li&gt;&lt;a href=&quot;./proposal&quot;&gt;코스레스토랑에 보내는 제안서 &lt;/a&gt;&lt;/li&gt; --&gt;
    &lt;!-- &lt;li&gt;&lt;a href=&quot;./proposal&quot;&gt;코스레스토랑에 보내는 제안서 &lt;/a&gt;&lt;/li&gt; --&gt;
    &lt;!-- &lt;li&gt;&lt;a href=&quot;./proposal&quot;&gt;코스레스토랑에 보내는 제안서 &lt;/a&gt;&lt;/li&gt; --&gt;
    &lt;!-- &lt;li&gt;&lt;a href=&quot;./proposal&quot;&gt;코스레스토랑에 보내는 제안서 &lt;/a&gt;&lt;/li&gt; --&gt;
    &lt;!-- &lt;li&gt;&lt;a href=&quot;./proposal&quot;&gt;코스레스토랑에 보내는 제안서 &lt;/a&gt;&lt;/li&gt; --&gt;
    &lt;!-- &lt;li&gt;&lt;a href=&quot;./proposal&quot;&gt;코스레스토랑에 보내는 제안서 &lt;/a&gt;&lt;/li&gt; --&gt;
    &lt;!-- &lt;li&gt;&lt;a href=&quot;./proposal&quot;&gt;코스레스토랑에 보내는 제안서 &lt;/a&gt;&lt;/li&gt; --&gt;
    &lt;!-- &lt;li&gt;&lt;a href=&quot;./proposal&quot;&gt;코스레스토랑에 보내는 제안서 &lt;/a&gt;&lt;/li&gt; --&gt;
    &lt;!-- &lt;li&gt;&lt;a href=&quot;./proposal&quot;&gt;코스레스토랑에 보내는 제안서 &lt;/a&gt;&lt;/li&gt; --&gt;
    &lt;!-- &lt;li&gt;&lt;a href=&quot;./proposal&quot;&gt;코스레스토랑에 보내는 제안서 &lt;/a&gt;&lt;/li&gt; --&gt;
    &lt;!-- &lt;li&gt;&lt;a href=&quot;./proposal&quot;&gt;코스레스토랑에 보내는 제안서 &lt;/a&gt;&lt;/li&gt;     --&gt;   









&lt;!-- 
    &lt;li&gt;&lt;a href=&quot;./test&quot;&gt;시험용 포스팅입니다 - 출력되어라&lt;/a&gt;&lt;/li&gt;
    &lt;li&gt;&lt;a href=&quot;./test&quot;&gt;시험용 포스팅입니다 - 출력되어라&lt;/a&gt;&lt;/li&gt;
    &lt;li&gt;&lt;a href=&quot;./test&quot;&gt;시험용 포스팅입니다 - 출력되어라&lt;/a&gt;&lt;/li&gt;
    &lt;li&gt;&lt;a href=&quot;./test&quot;&gt;시험용 포스팅입니다 - 출력되어라&lt;/a&gt;&lt;/li&gt;
    &lt;li&gt;&lt;a href=&quot;./test&quot;&gt;시험용 포스팅입니다 - 출력되어라&lt;/a&gt;&lt;/li&gt;
    &lt;li&gt;&lt;a href=&quot;./test&quot;&gt;시험용 포스팅입니다 - 출력되어라&lt;/a&gt;&lt;/li&gt;
    &lt;li&gt;&lt;a href=&quot;./test&quot;&gt;시험용 포스팅입니다 - 출력되어라&lt;/a&gt;&lt;/li&gt;
    &lt;li&gt;&lt;a href=&quot;./test&quot;&gt;시험용 포스팅입니다 - 출력되어라&lt;/a&gt;&lt;/li&gt;
    &lt;li&gt;&lt;a href=&quot;./test&quot;&gt;시험용 포스팅입니다 - 출력되어라&lt;/a&gt;&lt;/li&gt;
    &lt;li&gt;&lt;a href=&quot;./test&quot;&gt;시험용 포스팅입니다 - 출력되어라&lt;/a&gt;&lt;/li&gt;
    &lt;li&gt;&lt;a href=&quot;./test&quot;&gt;시험용 포스팅입니다 - 출력되어라&lt;/a&gt;&lt;/li&gt;
    &lt;li&gt;&lt;a href=&quot;./test&quot;&gt;시험용 포스팅입니다 - 출력되어라&lt;/a&gt;&lt;/li&gt;
    &lt;li&gt;&lt;a href=&quot;./test&quot;&gt;시험용 포스팅입니다 - 출력되어라&lt;/a&gt;&lt;/li&gt;
    &lt;li&gt;&lt;a href=&quot;./test&quot;&gt;시험용 포스팅입니다 - 출력되어라&lt;/a&gt;&lt;/li&gt;
    &lt;li&gt;&lt;a href=&quot;./test&quot;&gt;시험용 포스팅입니다 - 출력되어라&lt;/a&gt;&lt;/li&gt;
    &lt;li&gt;&lt;a href=&quot;./test&quot;&gt;시험용 포스팅입니다 - 출력되어라&lt;/a&gt;&lt;/li&gt;
    &lt;li&gt;&lt;a href=&quot;./test&quot;&gt;시험용 포스팅입니다 - 출력되어라&lt;/a&gt;&lt;/li&gt;
    &lt;li&gt;&lt;a href=&quot;./test&quot;&gt;시험용 포스팅입니다 - 출력되어라&lt;/a&gt;&lt;/li&gt;
    &lt;li&gt;&lt;a href=&quot;./test&quot;&gt;시험용 포스팅입니다 - 출력되어라&lt;/a&gt;&lt;/li&gt;
    &lt;li&gt;&lt;a href=&quot;./test&quot;&gt;시험용 포스팅입니다 - 출력되어라&lt;/a&gt;&lt;/li&gt;
    &lt;li&gt;&lt;a href=&quot;./test&quot;&gt;시험용 포스팅입니다 - 출력되어라&lt;/a&gt;&lt;/li&gt;
  --&gt;
    
&lt;/ul&gt;

&lt;h1 id=&quot;section-3-project---twitter-application&quot;&gt;Section 3 Project - Twitter Application&lt;/h1&gt;

&lt;p&gt;Software Engineering 섹션의 마무리 프로젝트 주제는 Flask를 이용하여 웹 어플리케이션 만들기. 트위터 API를 활용하여 데이터를 받아오고 DB를 구축하여 저장, 다시 저장된 데이터를 불러와 각종 기능들을 구현하는 것이 최종 목표였다.
섹션 내내 나를 혼돈속으로 밀어넣었던 각종 개발 툴들이 어떻게 활용되고 그것들의 퍼즐을 맞춰 최종적으로 어떤 어플리케이션이 탄생했는지 소개하려고 한다.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://github.com/lwoongh38/portfolio&quot; title=&quot;프로젝트링크&quot;&gt;Github repo&lt;/a&gt;&lt;/p&gt;

&lt;h2 id=&quot;무슨-기능을-가진-어플리케이션인가&quot;&gt;무슨 기능을 가진 어플리케이션인가??&lt;/h2&gt;
&lt;hr /&gt;

&lt;p&gt;트위터 어플리케이션은 메인페이지와 다섯가지의 기능을 가진 각 페이지를 합쳐 총 여섯개의 페이지로 구성되어 있다.
각각의 기능을 살펴보면,&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;Home : 가장 처음 접속할 때 보이는 대문과 같은 페이지로 아래에 기술될 기능들을 사용하며 데이터베이스가 구축되면 user 테이블 전체를 쿼리해서 확인할 수 있도록 구성되어 있다.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Add : 어플리케이션에서 정보를 조회하고싶은 트위터 유저의 screen name을 입력하면 해당 유저의 ID, Username, Full Name, Location을 API를 통해 받아와서 데이터베이스의 User 테이블에 저장한다. 또한 해당 유저의 트윗 기록(ID, Text, User ID)도 Tweet 테이블에 저장하며 User 테이블에 저장된 내용을 페이지 하단부에 쿼리하여 출력한다.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Get : Add페이지에서 조회하여 저장된 유저의 Tweet 테이블을 쿼리하여 하단부에 출력한다.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Delete : 데이터베이스에 저장된 유저의 User, Tweet 테이블의 데이터를 삭제할 수 있다.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Update : 저장된 데이터베이스에서 유저의 Full Name을 변경할 수 있다.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Predict : 트윗 내용을 제시했을 때, 두 명의 트위터 유저 사이에서 누가 해당 트윗을 작성했을 것인지를 예측할 수 있는 기능이다.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;어플리케이션-개발-과정&quot;&gt;어플리케이션 개발 과정&lt;/h2&gt;
&lt;hr /&gt;
&lt;h3 id=&quot;개발에-사용된-모듈-및-패키지&quot;&gt;개발에 사용된 모듈 및 패키지&lt;/h3&gt;
&lt;ul&gt;
  &lt;li&gt;Tweepy : 트위터 API를 파이썬에서 활용가능하게 해주는 모듈.&lt;/li&gt;
  &lt;li&gt;Flask : 파이썬으로 웹 어플리케이션을 개발하기 위한 프레임워크. API 어플리케이션을 만들기 위한 각종 편의 기능들을 제공한다.&lt;/li&gt;
  &lt;li&gt;FLASK SQLAlchemy : ORM(Object Relational Mapper)의 한 종류로 Flask 프레임워크에서 데이터베이스와의 상호작용을 파이썬과 비슷한 객체형식으로 가능하게 해주는 모듈.&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;1-__init__py--modelspy-생성&quot;&gt;1. __init__.py &amp;amp; models.py 생성&lt;/h3&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;strong&gt;init&lt;/strong&gt;.py : 어플리케이션을 initialize하기 위한 파일로 블루프린트를 사용하여 함수를 여러 곳으로 확장하고 분산하여 구현하였다. 블루프린트를 사용하지 않아도 웹서버 구현이 가능하지만 함수들을 분리하여 관리하면 어플리케이션에 기능이 많아지면 파일이 길어져 관리가 힘들어지기 때문에 분산관리의 이점이 드러나게 된다.
사용자가 웹페이지에서 url을 입력했을 때, 해당 url과 route 파일이 상호 연결될 수 있도록 해준다.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;models.py : Flask_SQLAlchmy를 활용하여 DB에 데이터를 저장하는 형식을 지정하는 파일이다. 테이블, 컬럼 명 등을 지정하고 테이블간의 관계도 이곳에서 정해주게 된다.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;2-데이터베이스-연결&quot;&gt;2. 데이터베이스 연결&lt;/h3&gt;

&lt;p&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;__init__.py&lt;/code&gt;와 &lt;code class=&quot;highlighter-rouge&quot;&gt;models.py&lt;/code&gt; 파일을 작성했다면 어플리케이션에서 사용자가 입력한 정보를 저장할 데이터베이스를 구축하고 연결해야 한다.
데이터베이스는 간단히 코드 세줄로 구축 및 연결이 가능하다.&lt;/p&gt;

&lt;div class=&quot;language-py highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;c&quot;&gt;# 데이터베이스 구축&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;FLASK_APP&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;twitter_app&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;flask&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;db&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;init&lt;/span&gt;
&lt;span class=&quot;c&quot;&gt;# 데이터베이스 테이블 생성&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;FLASK_APP&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;twitter_app&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;flask&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;db&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;migrate&lt;/span&gt;
&lt;span class=&quot;c&quot;&gt;#데이터베이스 테이블에 세부 컬럼 생성&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;FLASK_APP&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;twitter_app&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;flask&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;db&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;upgrade&lt;/span&gt;

&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;h3 id=&quot;3-routes--templates-생성&quot;&gt;3. routes &amp;amp; templates 생성&lt;/h3&gt;

&lt;p&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;routes&lt;/code&gt;폴더와 &lt;code class=&quot;highlighter-rouge&quot;&gt;templates&lt;/code&gt;폴더는 각각의 파일들을 관리하게 되는데 &lt;code class=&quot;highlighter-rouge&quot;&gt;routes&lt;/code&gt;파일에는 어플리케이션의 기능 구현에 대한 코드를 작성하게 되고 templates 폴더에 담겨지는 html 파일은 &lt;code class=&quot;highlighter-rouge&quot;&gt;routes&lt;/code&gt; 파일로 구현된 기능을 웹페이지에 어떻게 뿌려줄지를 결정하는 역할을 한다.
쉽게 말하면 &lt;code class=&quot;highlighter-rouge&quot;&gt;routes&lt;/code&gt;는 기능구현 파일, &lt;code class=&quot;highlighter-rouge&quot;&gt;html&lt;/code&gt;파일은 웹에 어떻게 보여질지를 구성하는 파일이다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://user-images.githubusercontent.com/70134676/102013133-8ec8ab00-3d91-11eb-90db-f038ff1e5a65.png&quot; alt=&quot;Home&quot; /&gt;&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;main_routes.py &amp;amp; index.html : Home 웹페이지에 접속하면 처음으로 보이는 페이지로 index.html로 User 테이블의 정보를 전달한다.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;img src=&quot;https://user-images.githubusercontent.com/70134676/102013282-5b3a5080-3d92-11eb-8050-4d9dc4095b43.png&quot; alt=&quot;Add&quot; /&gt;&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;add_routes.py &amp;amp; add.html : = 사용자가 입력한 트위터 유저의 username을 add.html을 통하여 radd_routes.py로 전달하고 해당 유저의 정보와 트윗기록들을 User 테이블과 Tweet 테이블에  저장한다.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;img src=&quot;https://user-images.githubusercontent.com/70134676/102013390-35fa1200-3d93-11eb-85f0-2691ea5b5790.png&quot; alt=&quot;Get&quot; /&gt;&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;get_routes.py &amp;amp; get.html : 사용자가 입력한 트위터 유저의 username을 get.html을 통하여 get_routes.py로 전달하고 Tweet 테이블에 저장된 데이터중 해당 username 과 일치하는 레코드들을 쿼리하여 get.html 에 전달한다. get_html은 전달받은 레코드를 웹페이지에 출력한다.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;img src=&quot;https://user-images.githubusercontent.com/70134676/102013503-f4b63200-3d93-11eb-8042-7e5f4c7bbd1c.png&quot; alt=&quot;Delete&quot; /&gt;&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;delete_routes.py &amp;amp; delete.html :  사용자가 입력한 트위터 유저의 username을 delete.html을 통하여 delete_routes.py로 전달하고 해당 유저에 대한 User, Tweet 테이블 내의 정보를 모두 삭제한다.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;img src=&quot;https://user-images.githubusercontent.com/70134676/102013538-2deea200-3d94-11eb-9f70-cd9d68447f29.png&quot; alt=&quot;Update&quot; /&gt;&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;update_routes.py &amp;amp; update.html : add를 통해 저장된 데이터 중 사용자가 입력한 트위터 유저의 FullName을 delete.html을 통하여 delete_routes.py로 전달하고 User 테이블 내의 FullName을 업데이트 한다.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;img src=&quot;https://user-images.githubusercontent.com/70134676/102013636-c5ec8b80-3d94-11eb-873a-d4835b9f2bb7.png&quot; alt=&quot;Predict&quot; /&gt;&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;predict_routes.py = predict.html : 사용자가 입력한 트윗 내용에 대해 add를 통해 저장된 두 명의 트위터 유저 사이에서 누가 입력 트윗을 작성했을지 예측한다.
로지스틱회귀모델을 적용하여 데이터베이스에 저장된 유저의 트윗내용으로 학습하여 입력된 트윗내용을 예측하게 된다.&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;4-어플리케이션-구동여부-확인-후-배포&quot;&gt;4. 어플리케이션 구동여부 확인 후 배포&lt;/h3&gt;
&lt;p&gt;어플리케이션을 본격적으로 웹에 배포하기 전에 로컬환경에서 앱의 구동여부를 확인해야 한다.
하지만 보통 개발과정에서 셀 수 없을 정도로 많은 에러를 접하게 되기 때문에 수시로 어플리케이션을 구동하여 각각의 파일들의 상호작용이 원활한지 확인하는 과정을 거치게 된다.
로컬에서 기능이 문제없이 작동한다면 웹에 배포하기 위하여 클라우드 플랫폼을 준비해야 한다.
나는 이번에 개발한 어플리케이션을 heroku라는 플랫폼을 활용하여 배포할 것이다.&lt;/p&gt;

&lt;hr /&gt;
&lt;h2 id=&quot;코드스테이츠의-데이터사이언티스트-코스-세번째-섹션을-마무리하며&quot;&gt;코드스테이츠의 데이터사이언티스트 코스 세번째 섹션을 마무리하며..&lt;/h2&gt;

&lt;p&gt;항상 프로젝트를 진행하면서 이 프로젝트는 추후 현업에서 어떻게 사용할 수 있을까에 대한 고민을 하게 된다. 그 고민은 일주일 동안 열정을 쏟을 나의 프로젝트에 대해 더 애착을 가지게 해주고 나아가 더 나은 결과물을 얻을 수 있는 큰 동기부여가 된다.&lt;/p&gt;

&lt;p&gt;이번 프로젝트는 데이터 사이언티스트의 주 업무라고 할 수 있는 데이터를 다루는 과정에서 친절하게 데이터가 주어지지 않았을 때를 대비한 훈련이라고 생각되었다. 공식적으로 제공하는 API를 활용하여 데이터베이스를 구축하고 축적된 데이터를 바탕으로 데이터 사이언티스트 본연의 임무를 가능하게 해주는 일종의 준비단계인 셈이다. 어떤 환경에서 일하게 될지 모르기 때문에 웹에서 직접 데이터베이스를 구축하고 데이터베이스를 활용하여 기능을 구현하는 연습은 좋은 데이터사이언티스트가 되는데도 큰 도움을 줄 것이다.&lt;/p&gt;

&lt;p&gt;섹션에서 배웠던 모듈들만 활용한 제한적인 범위 내에서 수행된 프로젝트였지만 프로젝트를 진행하며 3주동안 배운 지식들이 퍼즐이 맞춰지는 듯한 신기한 경험과 더불어 각각의 패키지와 라이브러리들이 어떤 역할을 하는지 확실하게 이해할 수 있는 시간이었다. 더불어 에러메시지를 대하는 태도도 조금은 의연해진 것 같다. 하지만 여전히 나만의 코드를 독창적으로 작성할 수 없다는 한계점을 여지없이 드러내며 스스로에게 숙제를 안겨주기도 한 시원섭섭한 프로젝트였다.&lt;/p&gt;</content>

      
      
      
      
      

      <author>
          <name>woongE</name>
        
        
      </author>

      

      
        <category term="softwareengineering" />
      

      
        <summary type="html">#Twitter #application #softwareengineering</summary>
      

      
      
    </entry>
  
    <entry>
      <title type="html">Python - “Class” 이녀석</title>
      <link href="http://localhost:4000/python-class" rel="alternate" type="text/html" title="Python - &quot;Class&quot; 이녀석" />
      <published>2020-11-14T01:40:00+09:00</published>
      <updated>2020-11-14T01:40:00+09:00</updated>
      <id>http://localhost:4000/python-class</id>
      <content type="html" xml:base="http://localhost:4000/python-class">&lt;p&gt;#Python #Class #softwareengineering&lt;/p&gt;

&lt;p&gt;&lt;span class=&quot;table-of-contents-list&quot;&gt;목차 &lt;/span&gt;&lt;/p&gt;
&lt;ul class=&quot;table-of-contents-list&quot;&gt;
    &lt;li&gt;&lt;a href=&quot;./python-class&quot;&gt;Software Engineering - Python - &quot;Class&quot; 이녀석 &lt;/a&gt;&lt;/li&gt;
    &lt;li&gt;&lt;a href=&quot;./twitter-web-application&quot;&gt;Software Engineering - Twitter Web Application &lt;/a&gt;&lt;/li&gt;
    &lt;!-- &lt;li&gt;&lt;a href=&quot;./proposal&quot;&gt;코스레스토랑에 보내는 제안서 &lt;/a&gt;&lt;/li&gt; --&gt;
    &lt;!-- &lt;li&gt;&lt;a href=&quot;./proposal&quot;&gt;코스레스토랑에 보내는 제안서 &lt;/a&gt;&lt;/li&gt; --&gt;
    &lt;!-- &lt;li&gt;&lt;a href=&quot;./proposal&quot;&gt;코스레스토랑에 보내는 제안서 &lt;/a&gt;&lt;/li&gt; --&gt;
    &lt;!-- &lt;li&gt;&lt;a href=&quot;./proposal&quot;&gt;코스레스토랑에 보내는 제안서 &lt;/a&gt;&lt;/li&gt; --&gt;
    &lt;!-- &lt;li&gt;&lt;a href=&quot;./proposal&quot;&gt;코스레스토랑에 보내는 제안서 &lt;/a&gt;&lt;/li&gt; --&gt;
    &lt;!-- &lt;li&gt;&lt;a href=&quot;./proposal&quot;&gt;코스레스토랑에 보내는 제안서 &lt;/a&gt;&lt;/li&gt; --&gt;
    &lt;!-- &lt;li&gt;&lt;a href=&quot;./proposal&quot;&gt;코스레스토랑에 보내는 제안서 &lt;/a&gt;&lt;/li&gt; --&gt;
    &lt;!-- &lt;li&gt;&lt;a href=&quot;./proposal&quot;&gt;코스레스토랑에 보내는 제안서 &lt;/a&gt;&lt;/li&gt; --&gt;
    &lt;!-- &lt;li&gt;&lt;a href=&quot;./proposal&quot;&gt;코스레스토랑에 보내는 제안서 &lt;/a&gt;&lt;/li&gt; --&gt;
    &lt;!-- &lt;li&gt;&lt;a href=&quot;./proposal&quot;&gt;코스레스토랑에 보내는 제안서 &lt;/a&gt;&lt;/li&gt; --&gt;
    &lt;!-- &lt;li&gt;&lt;a href=&quot;./proposal&quot;&gt;코스레스토랑에 보내는 제안서 &lt;/a&gt;&lt;/li&gt; --&gt;
    &lt;!-- &lt;li&gt;&lt;a href=&quot;./proposal&quot;&gt;코스레스토랑에 보내는 제안서 &lt;/a&gt;&lt;/li&gt; --&gt;
    &lt;!-- &lt;li&gt;&lt;a href=&quot;./proposal&quot;&gt;코스레스토랑에 보내는 제안서 &lt;/a&gt;&lt;/li&gt; --&gt;
    &lt;!-- &lt;li&gt;&lt;a href=&quot;./proposal&quot;&gt;코스레스토랑에 보내는 제안서 &lt;/a&gt;&lt;/li&gt; --&gt;
    &lt;!-- &lt;li&gt;&lt;a href=&quot;./proposal&quot;&gt;코스레스토랑에 보내는 제안서 &lt;/a&gt;&lt;/li&gt; --&gt;
    &lt;!-- &lt;li&gt;&lt;a href=&quot;./proposal&quot;&gt;코스레스토랑에 보내는 제안서 &lt;/a&gt;&lt;/li&gt; --&gt;
    &lt;!-- &lt;li&gt;&lt;a href=&quot;./proposal&quot;&gt;코스레스토랑에 보내는 제안서 &lt;/a&gt;&lt;/li&gt;     --&gt;   









&lt;!-- 
    &lt;li&gt;&lt;a href=&quot;./test&quot;&gt;시험용 포스팅입니다 - 출력되어라&lt;/a&gt;&lt;/li&gt;
    &lt;li&gt;&lt;a href=&quot;./test&quot;&gt;시험용 포스팅입니다 - 출력되어라&lt;/a&gt;&lt;/li&gt;
    &lt;li&gt;&lt;a href=&quot;./test&quot;&gt;시험용 포스팅입니다 - 출력되어라&lt;/a&gt;&lt;/li&gt;
    &lt;li&gt;&lt;a href=&quot;./test&quot;&gt;시험용 포스팅입니다 - 출력되어라&lt;/a&gt;&lt;/li&gt;
    &lt;li&gt;&lt;a href=&quot;./test&quot;&gt;시험용 포스팅입니다 - 출력되어라&lt;/a&gt;&lt;/li&gt;
    &lt;li&gt;&lt;a href=&quot;./test&quot;&gt;시험용 포스팅입니다 - 출력되어라&lt;/a&gt;&lt;/li&gt;
    &lt;li&gt;&lt;a href=&quot;./test&quot;&gt;시험용 포스팅입니다 - 출력되어라&lt;/a&gt;&lt;/li&gt;
    &lt;li&gt;&lt;a href=&quot;./test&quot;&gt;시험용 포스팅입니다 - 출력되어라&lt;/a&gt;&lt;/li&gt;
    &lt;li&gt;&lt;a href=&quot;./test&quot;&gt;시험용 포스팅입니다 - 출력되어라&lt;/a&gt;&lt;/li&gt;
    &lt;li&gt;&lt;a href=&quot;./test&quot;&gt;시험용 포스팅입니다 - 출력되어라&lt;/a&gt;&lt;/li&gt;
    &lt;li&gt;&lt;a href=&quot;./test&quot;&gt;시험용 포스팅입니다 - 출력되어라&lt;/a&gt;&lt;/li&gt;
    &lt;li&gt;&lt;a href=&quot;./test&quot;&gt;시험용 포스팅입니다 - 출력되어라&lt;/a&gt;&lt;/li&gt;
    &lt;li&gt;&lt;a href=&quot;./test&quot;&gt;시험용 포스팅입니다 - 출력되어라&lt;/a&gt;&lt;/li&gt;
    &lt;li&gt;&lt;a href=&quot;./test&quot;&gt;시험용 포스팅입니다 - 출력되어라&lt;/a&gt;&lt;/li&gt;
    &lt;li&gt;&lt;a href=&quot;./test&quot;&gt;시험용 포스팅입니다 - 출력되어라&lt;/a&gt;&lt;/li&gt;
    &lt;li&gt;&lt;a href=&quot;./test&quot;&gt;시험용 포스팅입니다 - 출력되어라&lt;/a&gt;&lt;/li&gt;
    &lt;li&gt;&lt;a href=&quot;./test&quot;&gt;시험용 포스팅입니다 - 출력되어라&lt;/a&gt;&lt;/li&gt;
    &lt;li&gt;&lt;a href=&quot;./test&quot;&gt;시험용 포스팅입니다 - 출력되어라&lt;/a&gt;&lt;/li&gt;
    &lt;li&gt;&lt;a href=&quot;./test&quot;&gt;시험용 포스팅입니다 - 출력되어라&lt;/a&gt;&lt;/li&gt;
    &lt;li&gt;&lt;a href=&quot;./test&quot;&gt;시험용 포스팅입니다 - 출력되어라&lt;/a&gt;&lt;/li&gt;
    &lt;li&gt;&lt;a href=&quot;./test&quot;&gt;시험용 포스팅입니다 - 출력되어라&lt;/a&gt;&lt;/li&gt;
  --&gt;
    
&lt;/ul&gt;

&lt;h3 id=&quot;software-engineering&quot;&gt;Software Engineering…&lt;/h3&gt;
&lt;hr /&gt;
&lt;p&gt;통계, 선형대수, ML 섹션을 거치면서 늘 사용해왔던 파이썬.
노트북 파일(ipynb)로 편하게 이용해왔지만 기본적인 지식이 부족하여 수시로 뱉어내는 에러를 해결 하는 과정은 녹록치 않았다.
그래서 그동안 에러를 볼 때마다 이번 섹션인 Software Engineering 섹션을 내심 기다려왔다. 그리고 마침내 두둥등장.
기본적인 개발환경에 대한 이해부터 직접 설정해보기도 하고 파이썬에 대해 조금 더 자세하게 들여다 본 한 주가 지났다. 이번 주에 배웠던 내용중에 이해가 잘 가지 않아 고생했던 Python의 &lt;strong&gt;Class&lt;/strong&gt;가 바로 오늘의 주제다. 시작해보자.&lt;/p&gt;

&lt;h2 id=&quot;class&quot;&gt;Class?&lt;/h2&gt;
&lt;p&gt;다른 프로그래밍 언어인 C에는 클래스가 없다는데 그럼 클래스는 왜 필요한 걸까? 
흔히 클래스의 개념을 설명할 때 계산기로 많이 설명하는데 이보다 적절한 예시를 찾지 못했기 때문에 계산기를 예로 들어보겠다.
계산기에 &lt;code class=&quot;highlighter-rouge&quot;&gt;1+1&lt;/code&gt;을 입력하면 계산기는 &lt;code class=&quot;highlighter-rouge&quot;&gt;2&lt;/code&gt;라는 값을 출력한다. 여기서 바로 &lt;code class=&quot;highlighter-rouge&quot;&gt;+3&lt;/code&gt;를 누르면 &lt;code class=&quot;highlighter-rouge&quot;&gt;5&lt;/code&gt;를 출력해준다. 첫번째 계산에서 출력된 결과인 &lt;code class=&quot;highlighter-rouge&quot;&gt;2&lt;/code&gt;를 어딘가에 저장해 두었고, 덕분에 우리는 바로 &lt;code class=&quot;highlighter-rouge&quot;&gt;+3&lt;/code&gt;만 눌러서 &lt;code class=&quot;highlighter-rouge&quot;&gt;5&lt;/code&gt;라는 결과를 받아볼 수 있다. 이렇게만 보면 뭐가 특별한가 싶지만 동시에 더 많은 계산을 하고, 더 복잡한 계산을 할수록 이전의 계산결과 저장을 위해 더 많은 메모리를 필요로 한다. 
클래스라는 개념은 이렇게 많은 메모리가 필요한 상황을 타개하기 위한 것이다. 클래스를 잘 사용하면 메모리를 효율적으로 사용할 수 있다는 이야기. 여기까지 보면 클래스가 왜 필요한지에 대한 근본적인 궁금증은 해결이 되었을 것 같다.&lt;/p&gt;

&lt;p&gt;클래스에 대해 공부하다보면 클래스와 객체가 있다는데 이는 또 무엇인가…
쉽게 이야기하면 클래스는 붕어빵틀, 객체는 붕어빵이라고 생각하면 된다. 
우리는 붕어빵틀(&lt;code class=&quot;highlighter-rouge&quot;&gt;class&lt;/code&gt;)를 이용해서 붕어빵(object)을 계속 찍어낼 수 있다. 클래스와 객체를 이해할 때 가장 중요한 것은 찍어낸 붕어빵들 간에는 고유한 특성을 가진다는 것이다.(&lt;del&gt;한 개의 붕어빵의 꼬리를 베어먹었다고 해서 다른 붕어빵의 꼬리가 잘려나가지 않는 것처럼&lt;/del&gt;) 서로서로 독립되어있다는 이야기.&lt;/p&gt;

&lt;div class=&quot;language-py highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;k&quot;&gt;class&lt;/span&gt; &lt;span class=&quot;nc&quot;&gt;BreadFrame&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;pass&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;이렇게 빵틀이라는 클래스가 있으면&lt;/p&gt;

&lt;div class=&quot;language-py highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;n&quot;&gt;soboro&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;BreadFrame&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;garlicbread&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;BreadFrame&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;
&lt;p&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;soboro&lt;/code&gt;, &lt;code class=&quot;highlighter-rouge&quot;&gt;garlicbread&lt;/code&gt; 처럼 많은 빵들을 찍어낼 수 있다.
&lt;code class=&quot;highlighter-rouge&quot;&gt;BreadFrame&lt;/code&gt;라는 빵틀의 결과물인 &lt;code class=&quot;highlighter-rouge&quot;&gt;soboro&lt;/code&gt;, &lt;code class=&quot;highlighter-rouge&quot;&gt;garlicbread&lt;/code&gt;가 바로 객체가 되는 것이다.
여기서 인스턴스라는 용어와 객체가 헷갈릴 수 있는데 요점만 딱 말하자면 &lt;strong&gt;`soboro`는 객체(object)이자 `BreadFrame`의 인스턴스이다.&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;남들은 쉽다고 하겠지만 나는 골머리를 앓았던 계산기를 클래스를 활용하며 만들어보자.&lt;/p&gt;

&lt;div class=&quot;language-py highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;k&quot;&gt;class&lt;/span&gt; &lt;span class=&quot;nc&quot;&gt;Calcula&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;__init__&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;a&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;b&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
        &lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;a&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;a&lt;/span&gt;
        &lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;b&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;b&lt;/span&gt;

    &lt;span class=&quot;k&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;addition&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
        &lt;span class=&quot;k&quot;&gt;return&lt;/span&gt; &lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;a&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+&lt;/span&gt; &lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;b&lt;/span&gt;
    
    &lt;span class=&quot;k&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;subtraction&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
        &lt;span class=&quot;k&quot;&gt;return&lt;/span&gt; &lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;a&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;b&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;h2 id=&quot;상속&quot;&gt;상속&lt;/h2&gt;
&lt;p&gt;다음은 클래스의 상속에 관한 내용이다.
흔히 재산을 상속받는 다는 얘기로 많이 들어보았을 상속은 클래스에서도 같은 의미로 적용된다.
앞의 계산기에서 덧셈, 뺄셈만 가능한 불완전한 계산기를 만들었는데(&lt;del&gt;클래스 이름이 Calcula인 이유&lt;/del&gt;) 이 두가지 기능만 넣은 이유가 있다.
바로 이 상속 개념을 적용하여 곱셈, 나눗셈 기능을 추가해 볼 것이다.&lt;/p&gt;

&lt;div class=&quot;language-py highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;k&quot;&gt;class&lt;/span&gt; &lt;span class=&quot;nc&quot;&gt;Calculator&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Calcula&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;__init__&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;a&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;b&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
        &lt;span class=&quot;nb&quot;&gt;super&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;__init__&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;a&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;b&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

    &lt;span class=&quot;k&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;multiplication&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
        &lt;span class=&quot;k&quot;&gt;return&lt;/span&gt; &lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;a&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;*&lt;/span&gt; &lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;b&lt;/span&gt;

    &lt;span class=&quot;k&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;division&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
        &lt;span class=&quot;k&quot;&gt;return&lt;/span&gt; &lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;a&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;/&lt;/span&gt; &lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;b&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;이렇게 완성된 계산기 &lt;code class=&quot;highlighter-rouge&quot;&gt;Calculator&lt;/code&gt;를 확인해보면&lt;/p&gt;

&lt;div class=&quot;language-py highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;c&quot;&gt;# 인스턴스 생성&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;result&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;Calculator&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;
&lt;div class=&quot;language-py highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;c&quot;&gt;# input&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;result&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;addition&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;
&lt;span class=&quot;c&quot;&gt;# output&lt;/span&gt;
&lt;span class=&quot;mi&quot;&gt;3&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;
&lt;p&gt;기존의 &lt;code class=&quot;highlighter-rouge&quot;&gt;Calcula&lt;/code&gt;로부터 덧셈기능을 잘 상속받은 것을 확인할 수 있고,&lt;/p&gt;
&lt;div class=&quot;language-py highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;c&quot;&gt;# input&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;result&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;multiplication&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;
&lt;span class=&quot;c&quot;&gt;# output&lt;/span&gt;
&lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;
&lt;p&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;Calculator&lt;/code&gt; 클래스에서 추가한 곱셈기능도 무사히(?) 작동하고 있는 것을 확인할 수 있다.&lt;/p&gt;

&lt;p&gt;여기서&lt;/p&gt;
&lt;div class=&quot;language-py highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;c&quot;&gt;# 생성자 함수&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;__init__&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;a&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;b&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
    &lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;a&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;a&lt;/span&gt;
    &lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;b&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;b&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;
&lt;p&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;__init__&lt;/code&gt;은 생성자 함수로, 해당 클래스(붕어빵틀)로 생성된 인스턴스(붕어빵)의 초기값을 생성하는 함수다. 객체가 생성되는 동시에 자동으로 호출된다.&lt;/p&gt;

&lt;p&gt;파이썬을 사용하면서 가장 기본이 되는 class에 대해서 알아보았다.
파이썬을 계속 쓰는 한 사용할 일이 많을 것 같고 더 심오한 내용도 많을 것 같지만 현재 이해할 수 있는 선에서 적어보았다. 일단 벌려놓아야 수습을 한다는 코치님들의 말을 믿고 일단 class에 대해 우선 포스팅을 해놓고 더 알아갈 때마다 와서 내용을 보완해야 할 것 같다.&lt;/p&gt;

&lt;hr /&gt;
&lt;h3 id=&quot;코드스테이츠의-데이터사이언티스트-코스-세번째-섹션의-첫-주를-마무리하며&quot;&gt;코드스테이츠의 데이터사이언티스트 코스 세번째 섹션의 첫 주를 마무리하며..&lt;/h3&gt;

&lt;p&gt;이번 섹션을 너무 겁없이 기다려왔던 것이 아닌가.. 싶을 정도로 어려운 한주였다.
그동안 편하게 사용해왔던 &lt;code class=&quot;highlighter-rouge&quot;&gt;shift + enter&lt;/code&gt;를 뒤로하고 본격적으로 CLI(Command Line Interface)를 활용하여 학습을 진행하면서 느낀점을 몇글자 적어보자면,&lt;/p&gt;

&lt;p&gt;힘들었지만 그래도 장점이라면,
검은 바탕의 힌 글씨를 보고있노라면 예전부터 대중매체에서 익숙하게 봐왔던 해커가 된 것 마냥 컴퓨터를 뭔가 전문적(?)으로 이용하고 있다는 느낌을 준다는 것이고 환경을 하나씩 갖추어가며 검은바탕에서 코드를 끄적이는 기분이 어렵고 힘들면서도 이상하게 괜찮다.&lt;/p&gt;

&lt;p&gt;단점이라면, 그것을 제외한 모든것인 것 같지만 마냥 싫은 느낌은 또 아니다.
그동안 컴퓨터를 사용하면서 GUI(Graphic User Interface)에 익숙해져있던 나는 마우스로 더블클릭하면 간단히 해결되는 폴더 열어보기 등 모든 과정을 명령어를 입력해야하는 환경에 놓여졌다. 검은화면을 보면서 어찌해야할지 몰라 방황하는 시간이 많아졌고 명령어를 찾기위해 검색을 하는 시간도 훨씬 잦아졌다. 개발환경을 세팅하면서 밤을 지새운 것은 덤이다. 하지만 이런 과정이 나를 담금질하는 과정이라고 생각하니 힘들어도 이겨내야겠다는 마음을 먹게 되더라.
굳세어지자.&lt;/p&gt;</content>

      
      
      
      
      

      <author>
          <name>woongE</name>
        
        
      </author>

      

      
        <category term="softwareengineering" />
      

      
        <summary type="html">#Python #Class #softwareengineering</summary>
      

      
      
    </entry>
  
    <entry>
      <title type="html">Bank marketing 정기예금 가입여부 예측</title>
      <link href="http://localhost:4000/bank-marketing" rel="alternate" type="text/html" title="Bank marketing 정기예금 가입여부 예측" />
      <published>2020-11-07T04:00:00+09:00</published>
      <updated>2020-11-07T04:00:00+09:00</updated>
      <id>http://localhost:4000/bank-marketing</id>
      <content type="html" xml:base="http://localhost:4000/bank-marketing">&lt;p&gt;#randomforest #XGB #machinelearning&lt;/p&gt;

&lt;p&gt;&lt;span class=&quot;table-of-contents-list&quot;&gt;목차 &lt;/span&gt;&lt;/p&gt;
&lt;ul class=&quot;table-of-contents-list&quot;&gt;
    &lt;li&gt;&lt;a href=&quot;./tadgan-modelstructure&quot;&gt;Time seriesAnomaly Detection Generative Adversarial Networks-Model Structure &lt;/a&gt;&lt;/li&gt;  
    &lt;li&gt;&lt;a href=&quot;./tadgan-lossfunction&quot;&gt;Time seriesAnomaly Detection Generative Adversarial Networks-loss function &lt;/a&gt;&lt;/li&gt;  
    &lt;li&gt;&lt;a href=&quot;./tadgan-intro&quot;&gt;Intro to the Time Series Anomaly Detection &lt;/a&gt;&lt;/li&gt;  
    &lt;li&gt;&lt;a href=&quot;./text-ads-filtering&quot;&gt;Text 데이터 광고 필터링을 위한 분류 모델 구축 &lt;/a&gt;&lt;/li&gt;
    &lt;li&gt;&lt;a href=&quot;./bert-sentiment-keyword&quot;&gt;사용자 리뷰 핵심어 추출 및 감성분석 &lt;/a&gt;&lt;/li&gt;
    &lt;li&gt;&lt;a href=&quot;./deeplearning-summary&quot;&gt;Deep Learning - 핵심 개념 &amp;amp; 용어 &lt;/a&gt;&lt;/li&gt;
    &lt;li&gt;&lt;a href=&quot;./versatility&quot;&gt;NLP - 딥러닝 모델의 범용적 활용가능성 분석 &lt;/a&gt;&lt;/li&gt;
    &lt;li&gt;&lt;a href=&quot;./bank-marketing&quot;&gt;Bank marketing 정기예금 가입여부 예측 &lt;/a&gt;&lt;/li&gt;
    &lt;li&gt;&lt;a href=&quot;./roccurve&quot;&gt;ROC curve에 대해 알아보자 &lt;/a&gt;&lt;/li&gt;
    &lt;li&gt;&lt;a href=&quot;./machinelearning-linearregression&quot;&gt;연어의 회귀본능이 아닌 선형회귀에 대해 알아보자 &lt;/a&gt;&lt;/li&gt;  
    
        
    
    


    &lt;!-- &lt;li&gt;&lt;a href=&quot;./proposal&quot;&gt;코스레스토랑에 보내는 제안서 &lt;/a&gt;&lt;/li&gt; --&gt;
    &lt;!-- &lt;li&gt;&lt;a href=&quot;./proposal&quot;&gt;코스레스토랑에 보내는 제안서 &lt;/a&gt;&lt;/li&gt; --&gt;
    &lt;!-- &lt;li&gt;&lt;a href=&quot;./proposal&quot;&gt;코스레스토랑에 보내는 제안서 &lt;/a&gt;&lt;/li&gt; --&gt;
    &lt;!-- &lt;li&gt;&lt;a href=&quot;./proposal&quot;&gt;코스레스토랑에 보내는 제안서 &lt;/a&gt;&lt;/li&gt; --&gt;
    &lt;!-- &lt;li&gt;&lt;a href=&quot;./proposal&quot;&gt;코스레스토랑에 보내는 제안서 &lt;/a&gt;&lt;/li&gt; --&gt;
    &lt;!-- &lt;li&gt;&lt;a href=&quot;./proposal&quot;&gt;코스레스토랑에 보내는 제안서 &lt;/a&gt;&lt;/li&gt; --&gt;
    &lt;!-- &lt;li&gt;&lt;a href=&quot;./proposal&quot;&gt;코스레스토랑에 보내는 제안서 &lt;/a&gt;&lt;/li&gt; --&gt;
    &lt;!-- &lt;li&gt;&lt;a href=&quot;./proposal&quot;&gt;코스레스토랑에 보내는 제안서 &lt;/a&gt;&lt;/li&gt; --&gt;
    &lt;!-- &lt;li&gt;&lt;a href=&quot;./proposal&quot;&gt;코스레스토랑에 보내는 제안서 &lt;/a&gt;&lt;/li&gt; --&gt;
    &lt;!-- &lt;li&gt;&lt;a href=&quot;./proposal&quot;&gt;코스레스토랑에 보내는 제안서 &lt;/a&gt;&lt;/li&gt; --&gt;
    &lt;!-- &lt;li&gt;&lt;a href=&quot;./proposal&quot;&gt;코스레스토랑에 보내는 제안서 &lt;/a&gt;&lt;/li&gt; --&gt;
    &lt;!-- &lt;li&gt;&lt;a href=&quot;./proposal&quot;&gt;코스레스토랑에 보내는 제안서 &lt;/a&gt;&lt;/li&gt; --&gt;
    &lt;!-- &lt;li&gt;&lt;a href=&quot;./proposal&quot;&gt;코스레스토랑에 보내는 제안서 &lt;/a&gt;&lt;/li&gt; --&gt;
    &lt;!-- &lt;li&gt;&lt;a href=&quot;./proposal&quot;&gt;코스레스토랑에 보내는 제안서 &lt;/a&gt;&lt;/li&gt; --&gt;
    &lt;!-- &lt;li&gt;&lt;a href=&quot;./proposal&quot;&gt;코스레스토랑에 보내는 제안서 &lt;/a&gt;&lt;/li&gt;     --&gt;









&lt;!-- 
    &lt;li&gt;&lt;a href=&quot;./test&quot;&gt;시험용 포스팅입니다 - 출력되어라&lt;/a&gt;&lt;/li&gt;
    &lt;li&gt;&lt;a href=&quot;./test&quot;&gt;시험용 포스팅입니다 - 출력되어라&lt;/a&gt;&lt;/li&gt;
    &lt;li&gt;&lt;a href=&quot;./test&quot;&gt;시험용 포스팅입니다 - 출력되어라&lt;/a&gt;&lt;/li&gt;
    &lt;li&gt;&lt;a href=&quot;./test&quot;&gt;시험용 포스팅입니다 - 출력되어라&lt;/a&gt;&lt;/li&gt;
    &lt;li&gt;&lt;a href=&quot;./test&quot;&gt;시험용 포스팅입니다 - 출력되어라&lt;/a&gt;&lt;/li&gt;
    &lt;li&gt;&lt;a href=&quot;./test&quot;&gt;시험용 포스팅입니다 - 출력되어라&lt;/a&gt;&lt;/li&gt;
    &lt;li&gt;&lt;a href=&quot;./test&quot;&gt;시험용 포스팅입니다 - 출력되어라&lt;/a&gt;&lt;/li&gt;
    &lt;li&gt;&lt;a href=&quot;./test&quot;&gt;시험용 포스팅입니다 - 출력되어라&lt;/a&gt;&lt;/li&gt;
    &lt;li&gt;&lt;a href=&quot;./test&quot;&gt;시험용 포스팅입니다 - 출력되어라&lt;/a&gt;&lt;/li&gt;
    &lt;li&gt;&lt;a href=&quot;./test&quot;&gt;시험용 포스팅입니다 - 출력되어라&lt;/a&gt;&lt;/li&gt;
    &lt;li&gt;&lt;a href=&quot;./test&quot;&gt;시험용 포스팅입니다 - 출력되어라&lt;/a&gt;&lt;/li&gt;
    &lt;li&gt;&lt;a href=&quot;./test&quot;&gt;시험용 포스팅입니다 - 출력되어라&lt;/a&gt;&lt;/li&gt;
    &lt;li&gt;&lt;a href=&quot;./test&quot;&gt;시험용 포스팅입니다 - 출력되어라&lt;/a&gt;&lt;/li&gt;
    &lt;li&gt;&lt;a href=&quot;./test&quot;&gt;시험용 포스팅입니다 - 출력되어라&lt;/a&gt;&lt;/li&gt;
    &lt;li&gt;&lt;a href=&quot;./test&quot;&gt;시험용 포스팅입니다 - 출력되어라&lt;/a&gt;&lt;/li&gt;
    &lt;li&gt;&lt;a href=&quot;./test&quot;&gt;시험용 포스팅입니다 - 출력되어라&lt;/a&gt;&lt;/li&gt;
    &lt;li&gt;&lt;a href=&quot;./test&quot;&gt;시험용 포스팅입니다 - 출력되어라&lt;/a&gt;&lt;/li&gt;
    &lt;li&gt;&lt;a href=&quot;./test&quot;&gt;시험용 포스팅입니다 - 출력되어라&lt;/a&gt;&lt;/li&gt;
    &lt;li&gt;&lt;a href=&quot;./test&quot;&gt;시험용 포스팅입니다 - 출력되어라&lt;/a&gt;&lt;/li&gt;
    &lt;li&gt;&lt;a href=&quot;./test&quot;&gt;시험용 포스팅입니다 - 출력되어라&lt;/a&gt;&lt;/li&gt;
    &lt;li&gt;&lt;a href=&quot;./test&quot;&gt;시험용 포스팅입니다 - 출력되어라&lt;/a&gt;&lt;/li&gt;
  --&gt;
    
&lt;/ul&gt;

&lt;h1 id=&quot;section-2-project---bank-marketing-정기예금-가입여부-예측하기&quot;&gt;Section 2 Project - Bank marketing 정기예금 가입여부 예측하기&lt;/h1&gt;

&lt;h2 id=&quot;abstract&quot;&gt;abstract&lt;/h2&gt;
&lt;ul&gt;
  &lt;li&gt;이윤을 추구하는 기업의 입장에서는 최소한으로 투자하고 최대한의 효과를 보기위해 부단히 노력하기 마련이다. 때문에 자사의 제품이나 서비스에 대한 마케팅을 실시할 때도 불필요한 지출은 줄이고 꼭 필요한 비용만을 이용하여 최대효율을 달성하는 것을 지향한다.&lt;br /&gt;
본 프로젝트는 은행에서 정기예금에 대한 마케팅을 실시할 때, 무작위로 대상을 선정하기보다 &lt;strong&gt;정기예금을 신청할 것 같은&lt;/strong&gt; 대상을 효율적으로 분별하여 마케팅을 실시할 수 있도록 한다. 핀포인트 마케팅이 가능하도록 하는 예측모델을 만들어보고 이를 개선시키는 것을 목적으로 진행하였다.&lt;/li&gt;
&lt;/ul&gt;

&lt;hr /&gt;

&lt;h2 id=&quot;1-시나리오&quot;&gt;1. 시나리오&lt;/h2&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;포르투갈의 한 은행에서 새롭게 출시하는 정기예금 상품을 기존 고객들을 대상으로 영업을 할 계획을 세우고 있다.
기존에는 무작위로 대상을 선별하여 연락하는 방법을 고수하였으나 이때 임시로 텔레마케터를 고용하는데 드는 비용이나 
마케팅에 성실하게 응답하는 고객이 많이 없다는 점이 문제로 지적된다. 그래서 이 문제를 해결하기 위해 
은행에서 수집한 고객 데이터를 바탕으로 마케팅이 성공할 수 있을지 여부를 분류해주는 예측모델을 만들어달라고 의뢰하였다. 
이렇게 우리는 제공받은 고객데이터를 바탕으로 &quot;정기예금 신청여부&quot;를 예측해주는 모델을 제작하게 되는데...
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;h3 id=&quot;11-제공받은-데이터&quot;&gt;1.1 제공받은 데이터&lt;/h3&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;bank client data: (은행 고객관련 특성)
- 나이
- 직업
- 결혼 여부
- 교육수준
- 채무불이행 여부
- 주택 담보 대출 여부
- 개인 대출 여부

related with the last contact of the current campaign: (현재 캠페인의 마지막 연락 관련 특성)
- 접촉 수단
- 마지막 연락(월)
- 마지막 연락(요일)
- 마지막 접촉 기간(초) : leakage 주의
other attributes: (기타 특성)
- 캠페인 중 고객에게 연락한 횟수
- 이전 캠페인에서 고객과 마지막으로 연락 후 경과일
- 캠페인 이전 고객에게 연락횟수
- 이전 마케팅 캠페인 결과

social and economic context attributes(사회, 경제적 특성)
- 고용 변동률-분기별
- 소비자 물가지수-월별
- 소비자 신뢰지수
- Euribo(유럽연합 내 12개국 시중은행간 금리)
- 직원 수(분기)

- 정기예금 신청 여부
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;h3 id=&quot;12-예측모델-정의&quot;&gt;1.2 예측모델 정의&lt;/h3&gt;
&lt;p&gt;&lt;img src=&quot;https://user-images.githubusercontent.com/70134676/98316542-e8fd8000-201d-11eb-8d2c-e36759b7675c.png&quot; alt=&quot;image&quot; /&gt;&lt;/p&gt;

&lt;p&gt;내가 만들게 될 모델은 데이터를 입력받았을 때 정기예금 신청여부를 “예”, “아니오”로 대답하는 이진분류 모델이 될 것이고 예측모델의 타겟이 되는 정기예금 신청여부를 확인해본 결과 yes와 no가 약 1:9 의 비율로 상당히 불균형한 것을 알 수 있다.
이런 경우, 전부 ‘no’로 예측해도 정확도가 90%에 가깝기 때문에 정확도를 지표로 삼기보다 다른 평가지표를 활용해야 할 것이다.
accuracy나 confusion matrix의 경우는 클래스불균형 문제에서 퍼포먼스를 왜곡할 가능성이 있다는 연구결과를 보게되었고 이때 추천할 수 있는 방법으로 클래스 불균형 여부에 상관없이 일정한 퍼포먼스 인덱스를 제공해주는 AUC, F1-score 두 가지를 평가에 활용할 예정이다. 또한 평가방법으로는 Cross validation을 선택하였는데 이는 전체적인 데이터셋의 크기가 크지 않다고 판단했기 때문이다.&lt;/p&gt;

&lt;h2 id=&quot;2-eda&quot;&gt;2. EDA&lt;/h2&gt;
&lt;p&gt;데이터는 상당히 잘 정제되어있는 상태였으며 간단한 전처리과정을 거치고 바로 EDA 작업에 들어갔다.
가장 먼저 각각의 개별적인 특성에 따라 정기예금 신청여부를 얼마나 잘 구별할 수 있는지 알아보기위해 특성별 타겟분포를 시각화해본 결과 중에 타겟의 분포에 따라 뚜렷한 차이를 보여 타겟 예측에 도움을 줄 것 같은 특성들과 별로 차이가 없어 별 도움을 줄 것같지 않은 특성을 몇가지 소개한다.
먼저 이전의 마케팅 결과에 대한 특성이 존재하여 예측에 큰 힘을 싣어줄 것으로 기대했으나&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://user-images.githubusercontent.com/70134676/98320489-6f1dc480-2026-11eb-9654-717ffcd11410.png&quot; alt=&quot;image&quot; /&gt;&lt;/p&gt;

&lt;p&gt;분포를 살펴본 결과 대부분 데이터가 없는 것으로 볼 때, 이전의 마케팅에 참여한 적이 없는 신규고객들이 대다수인 것을 알 수 있었다.&lt;/p&gt;

&lt;p&gt;마지막 접촉(월)에 따른 타겟 분포를 살펴보면&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://user-images.githubusercontent.com/70134676/98320067-71335380-2025-11eb-97bf-c01e0c13c923.png&quot; alt=&quot;image&quot; /&gt;&lt;/p&gt;

&lt;p&gt;5,6,7,8월 같은 특정 월에 집중적으로 가입자 수가 많은 것을 알 수 있다.&lt;/p&gt;

&lt;p&gt;결혼 여부에 따른 타겟 분포를 살펴보면,&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://user-images.githubusercontent.com/70134676/98318311-b6558680-2021-11eb-892e-f525e8c06ec8.png&quot; alt=&quot;image&quot; /&gt;&lt;/p&gt;

&lt;p&gt;결혼상태에 따라서 정기예금 신청여부도 기혼, 미혼, 이혼 상태 등의 순서로 예금 가입여부가 차이가 난다는 것을 볼 수 있다.&lt;/p&gt;

&lt;hr /&gt;

&lt;p&gt;반면 나이에 따른 타겟분포를 살펴보면,&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://user-images.githubusercontent.com/70134676/98318516-1ba97780-2022-11eb-8523-baa5144c9796.png&quot; alt=&quot;image&quot; /&gt;&lt;/p&gt;

&lt;p&gt;정기예금을 신청한 경우와 그렇지 않은 경우 모두 평균연령이 비슷한 30대 후반인 것을 알 수 있었다. 따라서 이 특성은 타겟을 분류하는데 별로 좋은 특성이 아닐 것이라고 생각했다.&lt;/p&gt;

&lt;p&gt;마지막 접촉(요일) 특성 또한&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://user-images.githubusercontent.com/70134676/98320345-14846880-2026-11eb-81fe-4b816b1075e8.png&quot; alt=&quot;image&quot; /&gt;&lt;/p&gt;

&lt;p&gt;특정 요일에 편향되지 않고 고르게 타겟의 분포가 형성되어 있는 것을 보면 타겟을 분류하는데 좋은 특성이 아니라고 추측할 수 있다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://user-images.githubusercontent.com/70134676/98320848-169af700-2027-11eb-967b-26913b4b8ce9.png&quot; width=&quot;350&quot; height=&quot;300&quot; /&gt; &lt;img src=&quot;https://user-images.githubusercontent.com/70134676/98320852-1ac71480-2027-11eb-9117-317393bb4377.png&quot; width=&quot;350&quot; height=&quot;300&quot; /&gt;&lt;/p&gt;

&lt;p&gt;소비자 신뢰지수나 유럽 시중은행의 금리 지수(euribo) 에 따른 타겟의 분포도 차이가 있어 EDA단계에서 의미를 찾지는 못했으나 예측모델을 만드는데 긍정적인 역할을 할 수 있을 것이라고 생각되었다.&lt;/p&gt;

&lt;h2 id=&quot;3-모델링&quot;&gt;3. 모델링&lt;/h2&gt;
&lt;p&gt;모델링은  작업은&lt;/p&gt;
&lt;ol&gt;
  &lt;li&gt;좋은 성능을 보여주는 앙상블 모델 중&lt;/li&gt;
  &lt;li&gt;bagging모델의 대표주자 랜덤포레스트분류모델(이하 RF)과 boosting모델의 대표주자 그레디언트부스팅(이하 XGB) 분류모델 두가지를 비교하여&lt;/li&gt;
  &lt;li&gt;더 좋은 성능을 보여주는 모델 한가지를 선택한 후 성능개선&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;순서로 진행하였다.
먼저 RF 모델과 XGB 모델을 동일하게 랜덤서치CV를 통하여 최적파라미터를 찾아준 다음 성능을 파악한 결과&lt;/p&gt;
&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;RF 
auc : 0.778
f1 : 0.353
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;
&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;XGB 
auc : 0.804
f1 : 0.366
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;
&lt;p&gt;XGB 모델이 모든 성능지표에서 성능상 우위를 가지는 것을 볼 수 있었다. 이렇게 예측모델의 기법은 XGB모델로 결정하고
성능개선 작업을 시작했다.&lt;/p&gt;

&lt;h3 id=&quot;31-성능개선&quot;&gt;3.1 성능개선&lt;/h3&gt;

&lt;p&gt;XGB 모델은 RF모델보다 하이퍼파라미터의 튜닝에 민감하게 반응하는 모델기법이다. 그래서 큰 기대를 가지고 모델성능 개선작업에 착수하였다.&lt;/p&gt;

&lt;p&gt;성능개선 작업은&lt;/p&gt;
&lt;ol&gt;
  &lt;li&gt;특성의 특징에 따라 인코딩 방법을 섞어서 진행&lt;/li&gt;
  &lt;li&gt;타겟의 클래스를 비슷하게 보정
두 단계로 진행해보았다.&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;먼저 인코딩방법은 기존에는 사용해봤을 때 무난한 성능을 보여주었던 ordinal 인코더를 사용했었다.
하지만 새로 인코딩을 실시할 때는,
범주가 적고 범주간에 우선순위가 없는 특성에는 OneHotEncoder를 사용하였고
그 외의 특성에는 TargetEncoder를 사용하여 성능을 향상시키려고 시도하였다.
인코딩 작업이 완료된 후의 성능은&lt;/p&gt;
&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;XGB 
auc : 0.807
f1 : 0.375
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;
&lt;p&gt;으로 소폭의 성능향상이 이루어졌다.&lt;/p&gt;

&lt;p&gt;두번째로 성능을 향상시키는 아이디어는 오버샘플링으로 모든 예측모델은 타겟의 클래스가 50:50일 때 가장 성능이 좋다는 점에 착안한 방법이다.
이 방법에도 언더샘플링과 오버샘플링 두가지 방법이 있으나 언더샘플링은 데이터양이 줄어들기 때문에 학습 시 실행시간을 줄일 수 있다는 장점이 있지만 분류모델을 만들 때 유용하게 사용될 수 있는 데이터가 유실될 우려가 있고 임의적으로 뽑은 샘플이 편향되거나 모집단을 대표하지 않을 수 있어 최종적으로 테스트셋에 적용했을 때 결과가 좋지 않을 수 있다.&lt;/p&gt;

&lt;p&gt;오버샘플링은 소수클래스를 임의로 복제하여 수를 늘리는 방법인데 소수 클래스를 판별할 때 과적합의 우려가 있으나 전반적으로 언더샘플링보다 뛰어난 효과를 보여주므로 오버샘플링 기법으로 클래스의 비율을 맞출 것이다.
이 중에서도 Synthetic Minority Over-sampling Technique (SMOTE) 알고리즘은 소수클래스로부터 일부를 선택하여 인접한 데이터 샘플을 찾아 그 사이에 새로운 데이터를 생성하는 방법으로 임의로 오버샘플링을 했을 때보다 과적합을 완화시킬 수 있다.&lt;/p&gt;

&lt;p&gt;따라서 오버샘플링 중에 SMOTE라는 방법으로 오버샘플링을 진행하였다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://user-images.githubusercontent.com/70134676/98322673-5b289180-202b-11eb-9b79-133305fd4d0c.png&quot; width=&quot;350&quot; height=&quot;300&quot; /&gt; &lt;img src=&quot;https://user-images.githubusercontent.com/70134676/98322676-5d8aeb80-202b-11eb-980b-8663db87f460.png&quot; width=&quot;350&quot; height=&quot;300&quot; /&gt;&lt;/p&gt;

&lt;p&gt;좌측은 샘플링을 실시하기 전, 우측은 샘플링을 실시한 이후의 클래스 분포를 보여준다.&lt;/p&gt;

&lt;p&gt;샘플링 이후의 데이터로 다시 모델을 돌렸을 때의 성능은&lt;/p&gt;
&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;XGB 
auc : 0.976
f1 : 0.898
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;
&lt;p&gt;auc 스코어와 f1스코어 모두 대폭 성능이 향상된 것을 볼 수 있었고
모델의 클래스 분포에 따라 같은 모델이라도 입력되는 학습데이터셋에 따라서 성능차이가 나는지를 확인할 수 있었다.&lt;/p&gt;

&lt;h2 id=&quot;4-결과-해석&quot;&gt;4. 결과 해석&lt;/h2&gt;
&lt;h3 id=&quot;41-특성-중요도permutation-importance-순열-중요도&quot;&gt;4.1 특성 중요도(Permutation Importance, 순열 중요도)&lt;/h3&gt;

&lt;p&gt;&lt;img src=&quot;https://user-images.githubusercontent.com/70134676/98323252-bd35c680-202c-11eb-8652-debab6d0e654.png&quot; alt=&quot;image&quot; /&gt;&lt;/p&gt;

&lt;p&gt;완성된 모델이 판단하기에 정기예금 가입 여부에 큰 영향을 주는 특성은 무엇이었는지를 알아보기 위해 Permutation Importance를 구해보았다.
상위 3개의 특성이 분류결과에 영향을 많이 주는 것을 알 수 있었고 PDP 를 그려서 특성이 어떤 영향을 주었는지를 파악해보기로 하였다.&lt;/p&gt;

&lt;h3 id=&quot;42-pdppartial-dependence-plot-부분-의존성-그림&quot;&gt;4.2 PDP(Partial Dependence plot, 부분 의존성 그림)&lt;/h3&gt;

&lt;p&gt;&lt;img src=&quot;https://user-images.githubusercontent.com/70134676/98326002-87481080-2033-11eb-9fd5-36d125edc6e7.png&quot; alt=&quot;image&quot; /&gt;&lt;/p&gt;

&lt;p&gt;capaign은 마케팅 캠페인 중에 고객에게 연락한 횟수에 대한 특성이다.
캠페인 기간중에 연락을 한 횟수가 늘어날수록 0(정기예금 가입 안함)으로 예측한다고 해석할 수 있다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://user-images.githubusercontent.com/70134676/98326539-d93d6600-2034-11eb-90f4-337882df6393.png&quot; alt=&quot;image&quot; /&gt;&lt;/p&gt;

&lt;p&gt;다음은 순열중요도에서 상위 두가지, campaign, nr.employed 특성과 동시에 타겟과의 관계를 살펴보았다.
캠페인 기간중에 연락을 한 횟수 2회까지는 소폭 1로 예측할 확률이 상승하지만 이후부터는 횟수가 늘어날수록 0(정기예금 가입 안함)으로 예측한다고 해석할 수 있다.
연락을 적게할수록 직원숫자가 적을수록, 직원 숫자가 적을수록 정기예금에 가입할 확률이 높다고 해석할 수 있다.
해당특성을 pdp로 살펴보기 전에는 연락을 적게 할수록 정기예금 가입을 하지 않는다고 생각했는데 오히려 연락횟수가 적을수록 정기예금을 가입하는 경우가 많은 것으로 보인다.
연락횟수 1에서 2로 올라갈때는 소폭 증가하는 것을 보면 2회 이상 연락하는 것은 오히려 역효과를 내는 것으로 볼 수 있다.&lt;/p&gt;

&lt;h2 id=&quot;5-결론&quot;&gt;5. 결론&lt;/h2&gt;
&lt;h3 id=&quot;51-model의-기대효과-및-한계&quot;&gt;5.1 Model의 기대효과 및 한계&lt;/h3&gt;
&lt;p&gt;미흡하게 만들어진 모델이지만 우리나라의 여건에 적용시킬 수 있을지 여부를 생각해봤을 때,
유리보(유럽연합 내 12개국 시중은행간 금리)등의 특성은 해당지역의 고유적인 인덱스이므로
그에 상응하는 우리나라의 고유지표를 찾아야 모델을 구축할 수 있을 것이며 바뀐 특성에 따라 성능도 물론 달라질 것이다.
하지만 우리나라의 특징에 맞는 특성들을 잘 교체할 수 있다면 충분히 모델을 구축할 수 있고 최적화여부에 따라 좋은 성능을 낼 수 있을 것 같다.&lt;/p&gt;

&lt;p&gt;모델을 튜닝하며 성능을 개선하는 작업 자체는 성취감도 있고 굉장히 재밌다고 생각했는데
어떻게 성능을 개선해야겠다는 아이디어는 비교적 잘 떠오르는데에 비해 그것을 적용시키는데에는 무수한 오류를 헤쳐나가야 했기때문에 생각했던 모든 아이디어들을 시험해보지 못한 것들이 아쉬웠다.&lt;/p&gt;</content>

      
      
      
      
      

      <author>
          <name>woongE</name>
        
        
      </author>

      

      
        <category term="ai" />
      

      
        <summary type="html">#randomforest #XGB #machinelearning</summary>
      

      
      
    </entry>
  
</feed>
