<!DOCTYPE html>
<html>
<head>

    <!-- Document Settings -->
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge" />

    <!-- Base Meta -->
    <!-- dynamically fixing the title for tag/author pages -->



    <title>Search Result</title>
    <meta name="HandheldFriendly" content="True" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <!-- Styles'n'Scripts -->
    <link rel="stylesheet" type="text/css" href="/assets/built/screen.css" />
    <link rel="stylesheet" type="text/css" href="/assets/built/screen.edited.css" />
    <link rel="stylesheet" type="text/css" href="/assets/built/syntax.css" />

    <!-- custom.css -->
    <link rel="stylesheet" type="text/css" href="/assets/built/custom.css" />

    <!-- Font Awesome -->
    <link rel="stylesheet" href="https://maxcdn.bootstrapcdn.com/font-awesome/4.7.0/css/font-awesome.min.css">

    <!-- 웹폰트 -->
    <link rel='stylesheet' href='//cdn.jsdelivr.net/npm/font-kopub@1.0/kopubdotum.min.css'>

    <!-- syntax.css 추가 -->
    <link rel="stylesheet" type="text/css" href="/assets/built/syntax.css">
  
    <!-- highlight.js -->
    <link rel="stylesheet" href="//cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/styles/default.min.css">
    <style>.hljs { background: none; }</style>

    <!--[if IE]>
        <style>
            p, ol, ul{
                width: 100%;
            }
            blockquote{
                width: 100%;
            }
        </style>
    <![endif]-->
    
    <!-- This tag outputs SEO meta+structured data and other important settings -->
    <meta name="description" content="Data Science 학습일지" />
    <link rel="shortcut icon" href="http://localhost:4000/assets/built/images/atom-icon.png" type="image/png" />
    <link rel="canonical" href="http://localhost:4000/search" />
    <meta name="referrer" content="no-referrer-when-downgrade" />

     <!--title below is coming from _includes/dynamic_title-->
    <meta property="og:site_name" content="Data Scientist 성장기" />
    <meta property="og:type" content="website" />
    <meta property="og:title" content="Search Result" />
    <meta property="og:description" content="Data Science 학습일지" />
    <meta property="og:url" content="http://localhost:4000/search" />
    <meta property="og:image" content="http://localhost:4000/assets/built/images/blog-cove1.jpg" />
    <meta property="article:publisher" content="https://www.facebook.com/" />
    <meta name="twitter:card" content="summary_large_image" />
    <meta name="twitter:title" content="Search Result" />
    <meta name="twitter:description" content="Data Science 학습일지" />
    <meta name="twitter:url" content="http://localhost:4000/" />
    <meta name="twitter:image" content="http://localhost:4000/assets/built/images/blog-cove1.jpg" />
    <meta name="twitter:label1" content="Written by" />
    <meta name="twitter:data1" content="Data Scientist 성장기" />
    <meta name="twitter:site" content="@" />
    <meta name="twitter:creator" content="@" />
    <meta property="og:image:width" content="2000" />
    <meta property="og:image:height" content="666" />

    <script type="application/ld+json">
{
    "@context": "https://schema.org",
    "@type": "Website",
    "publisher": {
        "@type": "Organization",
        "name": "Data Scientist 성장기",
        "logo": "http://localhost:4000/"
    },
    "url": "http://localhost:4000/search",
    "image": {
        "@type": "ImageObject",
        "url": "http://localhost:4000/assets/built/images/blog-cove1.jpg",
        "width": 2000,
        "height": 666
    },
    "mainEntityOfPage": {
        "@type": "WebPage",
        "@id": "http://localhost:4000/search"
    },
    "description": "Data Science 학습일지"
}
    </script>

    <!-- <script type="text/javascript" src="https://demo.ghost.io/public/ghost-sdk.min.js?v=724281a32e"></script>
    <script type="text/javascript">
    ghost.init({
    	clientId: "ghost-frontend",
    	clientSecret: "f84a07a72b17"
    });
    </script> -->

    <meta name="generator" content="Jekyll 3.6.2" />
    <link rel="alternate" type="application/rss+xml" title="Search Result" href="/feed.xml" />


</head>
<body class="page-template">

    <div class="site-wrapper">
        <!-- All the main content gets inserted here, index.hbs, post.hbs, etc -->
        <!-- < default -->
<!-- The tag above means: insert everything in this file
into the {body} of the default.hbs template -->

<!-- The big featured header, it uses blog cover image as a BG if available -->
<header class="site-header outer">
    <div class="inner">
        <nav class="site-nav">
    <div class="site-nav-left">
        
            
                <a class="site-nav-logo" href="http://localhost:4000/">Data Scientist 성장기</a>
            
        
        
            <ul class="nav" role="menu">
    <li class="nav-home" role="menuitem"><a href="/">Home</a></li>
    <li class="nav-about" role="menuitem"><a href="/about/">About</a></li>
    <li class="nav-fundemental" role="menuitem"><a href="/tag/fundemental/">Fundemental</a></li>
    <li class="nav-softwareengineering" role="menuitem"><a href="/tag/softwareengineering/">Software Engineering</a></li>
    <li class="nav-ai" role="menuitem"><a href="/tag/ai/">AI</a></li>
    <li class="nav-til" role="menuitem"><a href="/tag/til/">TIL</a></li>
    <li class="nav-archive" role="menuitem">
        <a href="/archive.html">All Posts</a>
    </li>
    <li class="nav-archive" role="menuitem">
        <a href="/author_archive.html">Tag별 Posts</a>
    </li>
</ul>
        
    </div>
    <div class="site-nav-right">
        <div class="social-links">
            
            
        </div>
        
            <a class="subscribe-button" href="#subscribe">Search</a>
        
    </div>
</nav>

    </div>
</header>

<!-- Everything inside the #post tags pulls data from the post -->
<!-- #post -->

<main id="site-main" class="site-main outer" role="main">
    <div class="inner">

        <article class="post-full  post page no-image">

            <header class="post-full-header">
                <h1 class="post-full-title">Search Result</h1>
            </header>

            

            <section class="post-full-content">
                <form action="/search" method="get" hidden="hidden">
    <label for="search-box"></label>
    <input type="text" id="search-box" name="query">
</form>

<ul class="mylist" id="search-results"></ul>

<script>
    window.store = {
    
    "tadgan-modelstructure": {
        "title": "Time seriesAnomaly Detection Generative Adversarial Networks-Model Structure",
            "author": "woongE",
            "category": "",
            "content": "#deeplearning #TadGAN #modelstructure목차     Time seriesAnomaly Detection Generative Adversarial Networks-Model Structure       Time seriesAnomaly Detection Generative Adversarial Networks-loss function       Intro to the Time Series Anomaly Detection       Text 데이터 광고 필터링을 위한 분류 모델 구축     사용자 리뷰 핵심어 추출 및 감성분석     Deep Learning - 핵심 개념 &amp; 용어     NLP - 딥러닝 모델의 범용적 활용가능성 분석     Bank marketing 정기예금 가입여부 예측     ROC curve에 대해 알아보자     연어의 회귀본능이 아닌 선형회귀에 대해 알아보자                                                                                       Time series Anomaly Detection Generative Adversarial Networks - Model Structure1. TadGAN의 구조지금까지 우리는 TadGAN이 어떤 모델이고, 어떤 loss function을 쓰는 지 알아봤습니다. 하지만 우리는 가장 중요한 걸 모르고 있습니다. 바로 모델 그 자체가 어떻게 작동하는지 입니다. 논문에는 tadgan모델 구조를 다음과 같이 나타냈습니다.모델은 기본적으로 AutoEncoder의 구조와 유사한 구조를 가지고 있습니다. 두 개의 Generator, 두개의 Critic 함수로 이루어졌다는 것을 확인할 수 있습니다. Generator E는 time series sequence를 latent space로 바꿔주는 역할을 하고, Generator G는 latent space를 다시 time series sequence로 변환해주는 역할을 합니다. 여기까지는 기존의 AE와 별반 다를 것이 없어 보입니다. 하지만 Critic X함수는 원본 데이터와 생성된 데이터 중 어떤 데이터가 원본인지를 가려내고, Critic Z 함수는 Generator E가 time series squence를 latent space로 얼마나 잘 맵핑 했는지를 판별 합니다. 더 자세하게 모델이 어떻게 작동하는지를 알기 위해서 TadGAN 공식 github의 이미지를 가지고 설명을 할까 합니다. 모델 이해는 pytorch에 능숙하다는 가정하에 (https://github.com/arunppsg/TadGAN)를 보시는 것을 추천드립니다.전체 모델 구조입니다. E,G는 Encoder과 Decoder를 나타내고, Cx를 통해서 실제 데이터와 Reconstruction 데이터를 판별합니다. 그리고 Cz는 E를 통해 생성된 Latent space와 렌덤으로 생성된 데이터를 가지고 얼마나 잘 맵핑되어 있는 지를 판별합니다. 다음 부터는 각 Phase 별로 어떤 학습이 이루어지는지 볼텐데, 우리가 주목할 것은 학습의 주체가 무엇인지 입니다.Phase 1 입니다. 실제 데이터와 random latent space가 Generator G를 통과한 재생성 데이터를 판별하는 Cx를 훈련 시키게 됩니다.Cx Loss function = critic_score_fake_x - critic_score_valid_x + sqrt[sum(Cx(alpha * x + (1 - alpha) * x_)^2)]Phase 2 입니다. 여기서는 실제 데이터를 맵핑 시킨 latent space와 random latent space를 비교해 Cz가 어떤 데이터가 원본 데이터의 latent space인지 판별하도록 학습합니다.Cz Loss function =critic_score_fake_z - critic_score_valid_z + sqrt[sum(Cx(alpha * z)+ (1 - alpha) * z_)^2)]Phase 3 입니다. 실제 데이터에서 나온 latent space를 가지고 Generator G로 재생성한 time series squence(gen_x)와, random latent space를 가지고 Genrator G로 생성한 time series squence (fake_x)를 Cx로 판별해 실제 데이터를 구별합니다. 이 때 학습 주체는 Generator E 이고, Cx를 최대한 속이는 쪽으로 학습을 진행합니다. 그리고 원본데이터와 random latent space로 생성한 fake_x 데이터의 Cx Score들을 loss function에 추가했는데, Decoder의 학습 정도에 따라 학습 속도를 변화시킨 것을 볼 수 있습니다.Generator E(Encoder) Loss function =mse_loss(x,gen_x)+critic_score_valid_z - critic_score_fake_zPhase 4 입니다. 실제 데이터를 Generator E에 통과 시킨 latent space를 Generator G에 통과시킨 gen_x와 실제 데이터 x를 비교합니다. 이 때 원본 데이터의 latent space와 random latent space를 통과시킨 Cz의 score들을 loss function에 추가했는데, Phase3 처럼 Encoder의 학습 정도에 따라 학습 속도를 변화시킨 것을 볼 수 있습니다.Generator G(Decoder) Loss function =mse_loss(x,gen_x)+critic_score_valid_x - critic_score_fake_x위의 과정은 차례대로 encoder_iteration,decoder_iteration critic_x_iteration, 그리고 critic_z_iteration로 소스코드에 포함되어 있습니다. 코드를 보고 천천히 따라가시면 어떻게 모델이 작동하는 건지 알 수 있습니다.",
        "url": "/tadgan-modelstructure"
    }
    ,
    
    "tadgan-lossfunction": {
        "title": "Time seriesAnomaly Detection Generative Adversarial Networks-loss function",
            "author": "woongE",
            "category": "",
            "content": "#TadGAN #lossfunction #AnomalyDetection목차     Time seriesAnomaly Detection Generative Adversarial Networks-Model Structure       Time seriesAnomaly Detection Generative Adversarial Networks-loss function       Intro to the Time Series Anomaly Detection       Text 데이터 광고 필터링을 위한 분류 모델 구축     사용자 리뷰 핵심어 추출 및 감성분석     Deep Learning - 핵심 개념 &amp; 용어     NLP - 딥러닝 모델의 범용적 활용가능성 분석     Bank marketing 정기예금 가입여부 예측     ROC curve에 대해 알아보자     연어의 회귀본능이 아닌 선형회귀에 대해 알아보자                                                                                       Time series Anomaly Detection Generative Adversarial Networks - Loss Function1. GAN의 Loss Function이번 포스트에서는 TadGAN에 대해서 본격적으로 알아보겠습니다.TadGAN은 이론상으로 훌륭한 방법론이지만 실제로 구현하기가 상당히 까다롭습니다. 이는 GAN을 사용하는 방법론들이 가지는 근본적인 문제점이기도 한데 학습의 어려움이 존재하기 때문입니다. 이에 대한 자세한 이야기는 잘 정리되어있는 포스트가 있어 링크를 첨부합니다.(https://brunch.co.kr/@kakao-it/162)그래서 Time seriesAnomaly Detection Generative Adversarial Networks(이하 TadGAN) 논문에서는 Loss Function을 차별화하여 이런 어려움을 해결하고자 했습니다.먼저 GAN에서 기본적으로 사용하는 loss function으로 부터 어떻게 KL divergence, JSD divergence를 유도하는지를 살펴보면 아래와 같습니다.KL,JSD를 이용해서 f(x)로 표현된 분포를 g(x)의 분포가 되도록 학습을 시켜야 하는데 그러기 위해서는 아래 그림의 왼쪽 부분은 왼쪽 부분 끼리, 오른쪽 부분은 오른쪽 부분끼리 같아지도록 만들어줘야 합니다. 문제는 두 분포에 대해서 x,T(x) 값이 달라지면 KL,JSD는 같은 기준값을 가진 확률들만 계산하도록 고안된 확률거리 함수라서 기준값이 달라지면 항상 무한대의 값, 혹은 일정한 값만 나오게 되면서 학습을 전혀 할 수 없게 됩니다. 그래서 TadGAN에서는 새로운 loss function을 차용하게 되었는데, 그게 바로 wgan-gp 입니다. wgan-gp는 Wasserstein GAN + gradient panelty를 합친 용어인데 먼저 이 loss function을 알려면 Wasserstein distance가 무엇인지 알아야 할 필요가 있습니다.Wasserstein distance?KL,JSD가 가지는 한계는 어떤 확률 집합, 혹은 확률함수등이 주어질 때 항상 같은 값에서만 비교가 가능하다는 것입니다. 만약에 다른 값에 대해서 얼마나 거리가 떨어져 있는 지를 확인한다면, KL과 JSD는 아래의 값만 계속 전달하게 됩니다.하지만 대부분의 무작위로 주어지는 확률 집합, 혹은 확률 함수들은 특수한 경우(값의 범위가 정해져 있는 경우)를 제외하고는 분포가 같을 경우가 거의 없습니다. 값이 정해져 있다면 애초에 관심이 가지 않겠죠. 풀어서 생각해보면 우리는 사과를 다른 모양의 사과로 변환시키는 것(값의 분포가 같은 경우)보다는 사과를 오렌지, 혹은 배등으로 변환시키는 것(값의 분포가 다를 경우)에 더 관심이 있을 것 입니다.우리는 다른 분포를 가진 확률함수에 대해 KL과 JSD가 유의미한 결과를 주지 않는다는 것을 알았습니다. 그래서 TadGAN 논문에서는 다른 분포를 가지는 확률 함수들에 대해서 값을 비교하기 위해 Wasserstein distance를 채택하게 됩니다. Wasserstein distance는 다른 말로 EMD(Eearth Mover distance)라고도 하는데, 쉽게 말해 Euclidean distance라고 생각하시면 됩니다. 이 확률 거리 측정 방법은 KL과 JSD와는 확실히 다릅니다. KL과 JSD가 다른 값에 대해서 서로 같은 확률을 가질 때 무한 대의 값이나 일정값만을 출력한다면, Wasserstein distance는 다른 분포에 대한 거리 정보까지 담고 있습니다.조금 더 쉽게 설명해 보겠습니다. 예를 들어서 X=[2,2,2,2,3,4,4,5], Y=[3,3,4,4,6,6,6,6]이 존재한다고 하겠습니다. 이 때 X와 Y가 각각 원소를 뽑게 될 때 두 집합의 확률이 얼마나 거리가 떨어져 있는 지에 대해서 알아보려고 합니다. 바로 전체를 비교하면 어려울 수 있으니 각 집합에서 X(2)와 Y(6)이 얼마나 떨어져 있는지 한번 비교해 보도록 하겠습니다.각 원소들이 뽑힐 확률이 1/n이라고 했을 때, X가 2를 뽑을 확률은 1/2이고, Y가 6을 뽑을 확률도 역시 1/2 입니다. 그리고 X가 2를 뽑을 확률과 Y가 6을 뽑을 확률에 대한 거리를 계산한다면, 두 경우에 대해서 확률이 동일하기 때문에 위에서 저희가 이미 구한 것 처럼 KL은 inf, JSD는 log2 가 나오지만 Wasserstein distance는 다음과 같이 계산됩니다.즉 확률은 같아도 2를 1/2로 뽑을 확률과 6을 1/2로 뽑을 확률은 정확히 4 만큼의 거리를 가진다는 것을 알 수 있습니다.지금까지 우리는 wasserstein distance를 어떻게 구하는 지에 대해 알아봤습니다. 하지만 이 것은 X(2) -&gt; Y(6) 로 갈 때만의 이야기를 한 것입니다. 우리가 알고 싶은 것은 X와 Y에 대한 확률 거리가 얼마나 떨어져 있는지를 어떻게 하면 정량적으로 나타낼 수 있는가? 입니다.X,Y에 대해서 각 원소들이 뽑힐 확률이 uniform(전부) 균일)이라고 가정할 때, 각 원소들이 뽑힐 확률은 모두 1/n으로 동일 합니다. 그래서 각 원소들이 뽑힐 확률은 Y축에 표시되어 있고, 그 원소의 값들은 X축에 표시되어 있습니다. 그리고 우리는 이제 X를 Y처럼 만들면 되는데, 만드는 방식은 블록 쌓기와 같습니다. 이 블록은 모든 블록의 크기를 더하면 1이 되는 블록입니다. 하지만 이렇게 되면 경우의 수가 너무 많이 발생합니다. 예를 들면 X(2)를 X(6)에 전부 다 옮겨 Y(6) 처럼 만들어주는 방법이 있습니다. 그것도 아니라면 X(3),X(4),X(5)를 X(6)으로 모두 옮긴 다음, X(2)를 잘게 쪼개서 X(3),X(4),X(5)를 채워주는 방법도 있습니다. 이렇게 여러가지의 방법들이 존재하기 때문에 Wasserstein distance에는 제약 조건이 있습니다. 어떻게 옮기든 상관은 없는데, 그 중 최소비용이 드는 방법을 두 함수의 거리로 생각하겠다는 것입니다. 비용 측정은 다음과 같이 합니다.이 계산식은 EM distance, Wasserstein-1 이라고 불리는 거리계산 방법입니다. 언뜻 보면 어려워 보일 수 있지만 집합 r과 집합 g에서 사건이 동시에 발생할 때 가능한 모든 확률들 중에서,L2 norm 공식을 적용했을 때 임의로 선택된 x,y의 확률의 차이와 값의 차이가 가장 작은 것을 Wasserstein distance라고 정하기로 한 것입니다. 그렇다면 이제 우리는 우리가 처음에 말했던, “왼쪽 부분은 왼쪽 부분 끼리, 오른쪽 부분은 오른쪽 부분끼리 같아지도록 만들어줘야”라는 의미가 무엇인지 생각해볼 수 있습니다. 물론 이렇게 확률을 변화시키지 않아도 되지만 최대한 근접하게 확률을 변화해야만 Wasserstein distance를 구하기 수월해 집니다.즉, Wasserstein distance는 어떤 사건이 동시에 발생했을 때, 두 사건이 가지는 확률과 분포가 달라지는 경우가 발생합니다. 분포가 같고 확률만 다르다면 기존의 KL, JSD로 충분히 계산이 가능하겠지만, 분포가 다르면 KL은 무한대로 발산하고 JSD는 log2의 값만 나타내게 됩니다. 하지만 Wasserstein distance는 분포가 다르더라도 분포의 차이, 그리고 확률의 차이를 동시에 고려합니다. 그래서 분포가 다른 두 확률에 대해서 얼마나 차이가 나는 지를 구별하여 나타낼 수 있는 거리 함수입니다.wgan-gp?이제까지 기본적인 GAN loss function의 한계를 알아보았고, 왜 KL, JSD divergnece를 사용해서 학습을 하면 안되는지, 그리고 Wasserstein distance라는 확률거리 함수를 사용해서 어떻게 이를 극복하는지를 했는지를 알아보았습니다. 실제로 tadgan 논문의 5쪽을 보면 일반적으로 GAN에서 사용하는 loss function과, 저자가 학습이 잘 되지 않는 이슈 때문에 wgan-gp를 사용하겠다는 내용이 나옵니다. 그리고 Wasserstein loss는 1-Lipschitz continuous function 조건(llf(x1) − f(x2)ll ≤ Klx1 − x2l, ∀x1, x2 ∈ dom f)을 만족 시키기 때문에 어느정도 기울기에 대한 변화를 제어할 수 있습니다. 여기까지 보면보면 Wasserstein distance를 이용한 Wasserstein loss로 충분해보이지만, 사실은 그렇지 않습니다. Improved Training of Wasserstein GANs:https://arxiv.org/abs/1704.00028)논문을 보면 1-Lipschitz 조건을 만족하긴 하지만, 이것 역시 아직 gredient 변화에 대해서 적절히 반응하지 못한다는 것을 확인할 수 있습니다. 그래서 gradient penalty를 추가 해서 gradient의 변화를 조절합니다.Conclusionloss function을 사용할 때, 기존의 GAN에 적용하던 확률거리함수(KL, JSD) 보다는 Wasserstein distance를 사용해야 모델이 학습을 잘 하게 되는데, 그 이유는 KL, JSD 발산은 서로 다른 값을 뽑을 확률에 대한 계산을 하게 되면 특정 값이나 무한대의 값을 측정하기 때문에 학습이 제대로 이루어지지 않습니다. 하지만 Wasserstein distance는 서로 다른 값을 뽑을 확률에 대해서 그 확률에 상관 없이 가변적인 값을 도출해 내기 때문에 손실 함수의 거리 계산으로 적합하다는 내용 입니다. 이 Wasserstein distance를 사용한 GAN을 wgan이라고 하는데, 이 wgan도 조금 문제가 있어서 wgan-gp라는 손실 함수가 새로 나오게 되었습니다.다음 포스트에서는 TadGAN의 모델구조에 대해서 알아보겠습니다.",
        "url": "/tadgan-lossfunction"
    }
    ,
    
    "tadgan-intro": {
        "title": "Intro to the Time Series Anomaly Detection",
            "author": "woongE",
            "category": "",
            "content": "#TadGAN #Timeseries #AnomalyDetection목차     Time seriesAnomaly Detection Generative Adversarial Networks-Model Structure       Time seriesAnomaly Detection Generative Adversarial Networks-loss function       Intro to the Time Series Anomaly Detection       Text 데이터 광고 필터링을 위한 분류 모델 구축     사용자 리뷰 핵심어 추출 및 감성분석     Deep Learning - 핵심 개념 &amp; 용어     NLP - 딥러닝 모델의 범용적 활용가능성 분석     Bank marketing 정기예금 가입여부 예측     ROC curve에 대해 알아보자     연어의 회귀본능이 아닌 선형회귀에 대해 알아보자                                                                                       Time series Anomaly Detection Generative Adversarial Networks1. Intro이번에 TadGAN: Time Series Anomaly Detection Using Generative Adversarial Networks논문을 읽고 공부하면서 알게 된 내용을 정리하는 포스트입니다. 한 포스트에 담기에는 양이 너무 많을 것 같아 2~3개 정도의 시리즈로 나누어 정리해볼 생각입니다.TadGAN은 machine learning 중 비지도 학습 방식입니다. 입력되는 시계열 데이터와 최대한 유사한 데이터를 모방하도록 학습하고, Threshold에 따라 이상치를 판단하는 이상 탐지 기법입니다. 비지도 학습 이상 탐지 분야는 다양한 목적을 위해 활용되고 있는데 다음과 같이 열거할 수 있습니다.데이터 품질 향상스탠포드 대학의 Andrew ng 교수는 모델을 튜닝하는 것 못지않게 모델의 학습에 이용되는 데이터의 품질을 높이는 것이 모델 성능 향상에 주요한 영향을 미친다고 말했습니다. 따라서 신뢰할 수 있는 데이터를 수집해야 하는데 이 과정에서 데이터 품질을 높이기 위해 필터링하는 작업에 이상치 탐지 모델을 적용하려는 노력이 이어지고 있습니다.비지도 학습방법의 필요성(Label의 부재)현실에 존재하는 데이터의 대부분은 독립변수에 따른 종속변수인 Y Label을 알 수 없는 경우가 많습니다. 이러한 데이터에 Labeling 작업을 통해 지도학습방식으로 모델을 학습시킬 수도 있지만 굉장히 많은 비용(시간, 돈)이 소모될 뿐만 아니라 Labling을 하는데 필요한 기준을 세우는 것도 어렵습니다. 그래서 결국에는 비지도학습에 의한 이상치 탐지방법이 필요합니다.이상치 탐지를 하는 방법은 여러가지가 존재하며 저는 그 중에서도 논문에서 소개한 세가지 방법론을 본격적으로 TadGAN을 얘기하기 전에 소개 하려고 합니다.  1. Proximity-based methods첫번째는 근접도 기반 방법입니다. 대표적으로 clutering 기법이 존재합니다. 각 데이터 요소들간의 거리를 계산하고 이를 바탕으로 근접도를 판단을 합니다. 근접도가 커지면 커질수록 멀리 있다는 뜻이기 때문에 여러가지 제약조건을 통해서 outlier을 결정할 수 있습니다.2. Predict-based methods두번째는 예측 기반 방법입니다. 전통적인 통계방법인 ARIMA가 가장 대표적입니다. 시점 t-1까지의 데이터를 활용해서 시점 t를 예측합니다. 그리고 실제 값이 예측값과 얼마나 다른지를 보고 이상치인지 아닌지를 판별합니다. 세 가지 방법중에서는 난이도가 가장 높은 편이라고 할 수 있습니다.3. Reconstruction-based methods재생성 기반 방법입니다. Auto Encoder가 가장 대표적입니다. 여러개의 특징(다변량 혹은 시계열데이터)을 Encoder을 통해 latent space로 압축시키고, 이후 Decoder을 통과해 원래의 데이터로 복원하게 되는데 이때 Auto Encoder는 그 데이터의 특징을 학습한 상태이므로 이상치가 포함된 데이터라면 복원이 잘 되어있지 않을 것입니다. 이러한 원리로 복원된 데이터와 원본 데이터의 차이를 비교해서 이상치를 추정하게 됩니다.그래서 TadGAN은…위의 방법중에서 TadGAN은 마지막 방법인 Reconsturction-based methods를 통해 이상치를 판별합니다. 조금 더 나아가서 TadGAN은 이런 reconsturction method를 CycleGAN에 사용된 기법을 이용하는 모델입니다. 먼저 CycleGAN에 대해서 이야기해 보자면 CycleGAN은 최초로 등장했을 때 이미지에 대한 변환을 주로 담당했는데, 원본 데이터의 형태를 유지하면서 이미지의 질감이나 스타일을 잘 바꿔줍니다. 예를들면, 초원에서 뛰어다니고 있는 말 사진에서 말을 얼룩말로 바꿔줄 수 있습니다. 중요한 것은 CycleGAN은 이미지의 형태가 완전히 다르면 제대로 기능을 하지 못한다는 것입니다.예를 들면 말에 사람이 타고 있다면 모델은 사람도 말로 인식해서 얼룩말 무늬를 씌워버립니다. 이 것은 이미지를 학습하는 입장에서는 별로 달갑지 않겠지만 시계열 데이터가 속해 있는지 아닌지를 판별할 때는 굉장히 유용합니다. CycleGAN은 AutoEncoder와 굉장히 유사하지만 GAN으로 학습하기 때문에 조금더 폭넓은 데이터들, 그러니까 초원과 말(들)이 존재하는 모든 사진을 하나의 필드로 인식합니다. 이것은 저희가 TadGAN에서 하려는 것과 굉장히 유사합니다. 다양하지만 하나의 도메인에서 비롯된 시계열 데이터를 학습시키면 그 모델은 특정 도메인에 대해서만 좋은 복원률을 보이고, 아니라면 완전히 이상한(ex. 얼룩무늬로 이루어진 사람)복원을 하게 될 것입니다.그래프를 보면 조금 더 쉽게 비교가 가능합니다. 처음 볼 수 있는 그림은 cyclegan의 원본을 시계열 데이터에 맞춰서 변경한 것입니다. 원본 시계열과 재생성된 시계열을 검사하는 Cx가 추가된 것 말고는 굉장히 흡사하게 보입니다. 우리가 여기서 하고 싶은 것은 명확합니다. 앞서 설명드렸던 cycleGAN의 아이디어를 TadGAN으로 가져와서 생각해보면 원본 이미지는 우리가 가지고 있는 시계열 데이터를 의미합니다. 우리가 가장 중요하게 생각하는 것은 시계열 데이터의 다양한 형태를 모델이 익히는 것인데 이미지로 치환해서 생각해보자면 우리가 관심 있는 것은 이미지의 형태이지 질감이나 스타일이 아닙니다. 질감이나 스타일은 여러가지 조건에 따라 달라질 수 있지만, 형태가 달라져버리면(말과 초원만 있는 사진을 학습한 모델에 말을 탄 사람이 초원에 있는 사진을 물어본 경우) 굉장히 다른 형태(얼룩무늬 사람)로 복원을 하기 때문입니다. TadGAN은 바로 이런 점에 착안을 해서 고안된 모델입니다.이번 포스트에서는 이정도로 하고 다음 포스트부터 TadGAN의 구조나 원리 등에 대해 제가 이해한바를 적어볼 예정입니다. 제가 이해한 바가 틀릴 수도 있으니 잘못알고 있는 것이 있다면 지적해주시면 감사하겠습니다.",
        "url": "/tadgan-intro"
    }
    ,
    
    "text-ads-filtering": {
        "title": "Text 데이터 광고 필터링을 위한 분류 모델 구축",
            "author": "woongE",
            "category": "",
            "content": "#deeplearning #NLP #BERT #classification #filtering목차     Time seriesAnomaly Detection Generative Adversarial Networks-Model Structure       Time seriesAnomaly Detection Generative Adversarial Networks-loss function       Intro to the Time Series Anomaly Detection       Text 데이터 광고 필터링을 위한 분류 모델 구축     사용자 리뷰 핵심어 추출 및 감성분석     Deep Learning - 핵심 개념 &amp; 용어     NLP - 딥러닝 모델의 범용적 활용가능성 분석     Bank marketing 정기예금 가입여부 예측     ROC curve에 대해 알아보자     연어의 회귀본능이 아닌 선형회귀에 대해 알아보자                                                                                       Text 데이터 광고 필터링을 위한 분류 모델 구축abstract본 프로젝트는 시계열(text) 데이터를 분석하기 위한 전처리 과정에서 사용자가 원하지 않는 정보를 자동적으로 필터링하여 전처리 진행 속도를 상승시키는 것을 목적으로 한다.Hate speech 분류에 사용되는 BiLSTM, CharCNN 등의 알고리즘을 활용 가능한지 파악하고 이를 차용하여 인스타그램의 텍스트 데이터에서 광고를 분류하는데 적용하고 각 모델의 성능을 비교한다.Github repo1. Intro코로나 바이러스 감염증이 확산된 후 2020년 세계 소매분야 e커머스 매출액은 약 4,414조원으로 전년도 매출 3494조원에 비해 920조원 가량 늘어났다고 합니다.(분석:미디어투자기업 Group M, 출처:e커머스 전망 보고서) 언택트 소비가 활성화 되면서 더욱 많은 소비자들이 상품평이나 구매 후기 등에 의존하게 되고 이에 따라 기업들은 구매후기 작성에 따른 혜택을 늘려가고 있는데 이를 악용하는 사례 역시 늘어나고 있는 추세입니다.구매후기 데이터는 소비자들 뿐만 아니라 기업의 마케팅을 위해서도 활용되는데 이를 위해서는 데이터가 순수한 소비자의 후기를 제외한  이른바 ‘뒷광고’, ‘거짓 후기’ 등으로 오염되어있는 데이터를 정제하는 과정이 필요합니다. 하지만 이 과정에서 많은 시간과, 경제적인 자원이 소모됩니다. 따라서 대량의 시계열(text) 데이터를 빠른시간안에 효율적으로 정제하기 위하여 광고나 거짓후기를 필터링할 수 있는 기술이 필요합니다.저는 대학원 연구실에서 인스타그램의 후기 데이터를 바탕으로 마케팅 현황을 분석하는 프로젝트에 참여한 경험이 있습니다.해당 프로젝트의 데이터 전처리 단계에서 사람이 일일이 읽어보고 광고여부를 판단하여 제거하는 과정을 수행했는데 시간이 매우 오래걸리고 작업자를 지치게 하는 과정이었어요. 따라서 시계열(text)데이터에서 광고를 필터링할 수 있는 모델이 있다면 데이터 전처리 과정의 소요시간을 대폭 감소시킬 수 있을 것이라는 생각이 들어 프로젝트를 기획하게 되었습니다.인스타그램.현재 국내에서 가장 많이 쓰는 sns 2위를 차지하고 있고 세계적으로도 10억명 이상이 사용할 정도로 절대다수의 이용자들이 자신의 디지털 소통창구로 인스타그램을 활용하고 있습니다. 이런 막강한 유저풀을 활용한 콘텐츠 마케팅이 눈길을 끌고 있습니다.2021년 인스타그램에서 발표한 통계자료를 보면 사용자의 약 81%가 제품이나 서비스를 검색하기 위해 인스타그램을 사용하고 있고 1억 3천만명의 유저들이 매달 쇼핑에 관련된 게시글을 보고있습니다. 또 비즈니스계정 사용자는 하루에 평균 한 건의 게시물을 게재하고 있다고 합니다.다음은 인스타그램에서 화장품을 검색했을 때 노출되는 게시글입니다.빨간색으로 강조되어있는 부분을 보면 이 게시글들은 광고인 것을 알 수 있습니다.문제는 일반 이용자의 경우는 해당 화장품의 솔직한 후기가 궁금하여 게시글을 검색하는 경우가 대부분인데 일일이 게시글을 열어보고 골라내야한다는 점과,기업의 입장에서는 사용자들의 솔직한 후기를 제품의 개선점이나 신제품출시에 대한 지표를 사용할텐데 텍스트파일을 일일이 읽어보고 광고를 골라내야한다는 점에서 소비자와 기업 모두 인스타그램 텍스트데이터를 활용하는데 있어 어려움을 겪고 있습니다.1.1 가설 설정그래서 저는,1. Hate speech 탐지에 쓰이는 방법을 광고 분류 모델에도 적용할 수 있을 것이다.2. Hate speech 탐지에 쓰이는 이상탐지 메커니즘을 활용하여 광고성 문구를 판별하는 모델을 생성, 0.9 이상의 정확도를 달성할 수 있을 것이다.라는 가설을 세우고 여러 모델을 제작하여 모델의 성능을 비교해보기로 했습니다.  인스타그램 텍스트데이터를 입력하여 전처리, 토큰화하고, 레이블을 입력합니다.  각 모델별로 광고성, 비광고성 글에 나타나는 특징에 따라 광고 여부를 판단할 수 있도록 학습시킵니다.인스타그램의 텍스트데이터를 사용하여 광고를 분류할 수 있는 모델을 만드는 것이 최종 목표입니다.2. Modeling2.1 모델 결정제가 구축할 파이프라인에 사용될 모델 중 하나는 버트입니다.다층의 레이어를 활용해서 문장을 구성하고 있는 토큰 사이의 의미를 잘 추출해낼 수 있습니다.간단하게 버트가 학습하는 과정을 살펴보면 문장의 일부 단어를 mask토큰으로 바꾸고 가려진 단어를 예측하도록 하는 첫번째 학습을 통해 문맥을 파악하는 능력을 기르게 되고또 다음 문장이 올바른 문장인지 맞추는 학습을 통해 문장 사이의 관계를 학습하게 됩니다.저는 버트 모델중에서도 한글에 특화되어있는 skt에서 한글로 학습시킨 kobert 을 선택했습니다.두번째 모델은 bilstm입니다.Bilstm에 대해 예를 들어 설명하면,\" 나는 __을 뒤집어 쓰고 펑펑 울었다. \"위의 문장에서 빈칸에 어떤 단어가 들어갈 지 유추할때, 한국어를 자유자재로 사용하는 사람은 빈칸에 들어갈 말이 이불이라는 것을 쉽게 알 수 있습니다. 그런데 이 문장의 경우, 빈칸을 유추 할때, 빈칸 앞보다는 빈칸 뒤에 나오는 단어들이 더 중요 합니다. ‘나는’ 뒤에 나올 수 있는 단어는 수없이 많은 반면에, ‘뒤집어 쓰고 펑펑 울었다’ 앞에 나올 수 있는 단어는 흔치 않기 때문 입니다. 이 예제에서 볼 수 있듯이, 텍스트 데이터는 정방향 추론 못지 않게 역방향 추론도 유의미한 결과를 낼 수 있습니다. 그래서 lstm을 다음과 같이 정방향 학습, 역방향 학습을 할 수 있도록 설계한 것이 bilstm입니다.2.2 데이터 수집저는 인스타로더를 사용해서 인스타그램 텍스트데이터를 확보했습니다.하지만 정제가 되어있지 않기 때문에 주어진 시간을 효율적으로 활용하기 위해 광고일 확률이 높은 해시태그와 광고가 아닐 확률이 높은 해시태그를 선정해서 크롤링한 다음, 광고 필터링 리스트를 제작해서 라벨링을 진행했습니다.검증, 테스트용 데이터셋은 일반적으로 사람이 광고를 분류하는 기준을 적용하기 위해서 약 3만개의 데이터를 직접 분류하는 방법으로 라벨링했습니다.학습용 데이터셋은 제품이나 서비스 분야를 가리지 않고 적용시킬 수 있도록 #광고아님, #찐후기, #내돈내산, #홍보, #협찬 등과 같이 되도록 일반적인 해시태그로 데이터를 추출했고검증, 테스트용 데이터셋은 광고와 광고가 아닌 일반 후기를 수집하기 수월한 #설화수, #미용기기 프라엘을 선정해서 이것을 해시태그로 수집했습니다.2.3 데이터 전처리인스타그램 텍스트데이터를 크롤링하는 패키지를 사용해서 얻어지는 raw데이터는 위와 같고, 1차적으로 이모티콘을 제거하고 줄바꿈과 같은 제어문자를 제거했습니다.모든 특수 문자를 한번에 지우지 않은 이유는, 필터링을 통해 라벨링을 할 때 예를 들어 #광고 해시태그를 필터링 하고 싶을 때 #이 없으면 광고로 필터링을 진행해야 하는데 이 때 광고아님도 같이 포함되어 필터링이 되기 때문에 1차 전처리 완료하고 라벨링을 한다음, 이어서 전처리를 진행했습니다.다음은 라벨링을 완료하고 2차전처리로 특수기호까지 제거해준 모습입니다.띄어쓰기가 되어있지 않은 데이터 같은 경우는 그냥 전처리를 하게 되면 토큰화를 할 때 의미단위로 코퍼스가 분리되지 않는 문제가 있어서 따로 처리해줬습니다.최종적으로 전처리가 완료되면 위와 같은 방식으로 정제되는 것을 볼 수 있습니다.3. 검증3.1 데이터 EDA데이터셋의 단어 통계를 확인하고싶어 한국어 형태소 분석기인 Mecab 패키지를 사용해서 간단하게 EDA를 진행해봤습니다. 토크나이저를 사용하고, 각 학습용 데이터셋에 대해서 라벨별로 최빈 단어들을 모아본 결과, 다음과 같은 결과를 얻을 수 있었습니다. 광고 데이터셋의 최빈단어목록에서 ‘협찬’ 같은 단어가 발견되었지만 그 외에는 크게 의미가 있는 결과를 보여주진 못했습니다.혹시 데이터의 길이로 판별이 가능한지 알아본 결과 광고성 글은 비 광고성 글의 길이보다 평균 두배 정도가 차이가 났습니다. 하지만 두 레이블 모두  겹치는 구간이 꽤 발생하기 때문에 문장의 길이로 단정짓기는 힘들다고 판단했습니다.4. 실험 결과 및 결론완성된 두가지 모델입니다.정밀도와 재현율, f1 스코어를 평가지표로 결정했고 이중에 재현율을 좀더 중요하게 활용하기로 했습니다.광고가 아닌 글들을 광고라고 판단하는 경우가 다소 있더라도, 실제 광고 글을 잘 분류해내는 것이 모델이 달성해야 할 최우선 목표라고 생각했기 때문입니다.평가지표상의 성능으로 판단해보면 데이터들의 분포는 오른쪽 그림과 같이 되어있음을 추측해볼 수 있었습니다.이 모델들이 실제 광고를 광고로 올바르게 분류할 확률은 0.9 이상으로 가설로 제시했던 0.6을 상회하는 성능을 보여주었습니다. 이는 실제 광고를 광고라고 예측하는 기능은 모델의 목적에 맞도록 잘 작동하고 있다고 볼 수 있습니다.하지만 재현율에 비해 정밀도나 F1스코어가 많이 낮은것을 보면 광고라고 예측된 것 중에 실제로는 광고가 아니었던 사례들이 꽤 많다는 것을 알 수 있어 모델의 전반적인 성능을 위해 개선작업이 필요하다고 판단됩니다.마지막으로 모델로 데이터 전체를 분류해본 결과입니다.두가지 모델의 성능은 크게 다르지 않았지만 한글을 모델에 적용하는 과정에서 kobert가 전이학습된 모델을 가져와서 사용하기 때문에 형태소분석 등의 번거로운 과정을 생략할 수 있다는 점이 큰 장점으로 다가왔습니다.프로젝트를 마무리하며..저는 이번 프로젝트에서 인스타그램의 텍스트 데이터 분류를 위한 광고 필터링 모델을 제작하는 것을 계획했습니다. 기술적인 어려움도 있었지만 데이터를 라벨링하는 과정에서 수동으로 일일이 광고인 것과 아닌 것을 분류해야하는 작업에서 시간적인 어려움을 겪었습니다. 전체 프로젝트에 할당된 시간중 거의 90%에 가까운 시간이 데이터를 크롤링하고, 정제하고, 라벨링하는데 소요되었습니다.여태껏 정제되어있는 데이터만을 프로젝트에서 사용했었는데 데이터셋 하나를 만든다는 것이 얼마나 많은 공이 들어가는 일인지를 알 수 있었습니다. 특히 NLP에 사용되는 데이터셋은 전처리 과정에서 이것저것 신경쓸 것도 많아 쉽지않은 과정이었습니다. 데이터셋을 만드는데 얼마나 많은 공이 들어가는지 알게 되었으니 앞으로는 고마운 마음으로 써야할 것 같네요.이번 프로젝트에서 가장 크게 배운점이라면 바로 윈도우와 맥os에서 한글의 유니코드 정규화가 다르게 이루어진다는 것을 알게 된 것입니다.중2병이 돋은듯한 한글자모분리현상과 일부 특수기호나 이모티콘이 일괄제거가 되지 않아 원인을 파고파고 또 파보니 맥os는 NFD, 윈도우는 NFC라는 방식을 사용한다고 합니다.같은 언어를 운영체제에 따라 다르게 받아들일 수 있다고는 상상도 못해봤기에 기업간의 쓸데없는 자존심 싸움이 사용자들에게 의미없는 수고를 강요한다는 생각이 들었습니다.프로젝트 진행과정에서 전처리와 레이블링 과정에서 굉장히 고생을 많이 했고 또 배운점이 많아 추후에 텍스트 전처리와 레이블링 작업에 대해 별도로 포스팅 해리라 결심해보며 이번 프로젝트를 마무리합니다.",
        "url": "/text-ads-filtering"
    }
    ,
    
    "bert-sentiment-keyword": {
        "title": "사용자 리뷰 핵심어 추출 및 감성분석",
            "author": "woongE",
            "category": "",
            "content": "#deeplearning #NLP #BERT #sentimental classification #keyword extraction목차     Time seriesAnomaly Detection Generative Adversarial Networks-Model Structure       Time seriesAnomaly Detection Generative Adversarial Networks-loss function       Intro to the Time Series Anomaly Detection       Text 데이터 광고 필터링을 위한 분류 모델 구축     사용자 리뷰 핵심어 추출 및 감성분석     Deep Learning - 핵심 개념 &amp; 용어     NLP - 딥러닝 모델의 범용적 활용가능성 분석     Bank marketing 정기예금 가입여부 예측     ROC curve에 대해 알아보자     연어의 회귀본능이 아닌 선형회귀에 대해 알아보자                                                                                       BERT를 활용한 사용자 리뷰 핵심어 추출 및 감성분석abstract본 프로젝트는 텍스트 데이터를 분석하여 리뷰 내용이 상품의 어떤 특성을 특정하고 있는 것인지 추출하고 감성분석을 하여 사용자 리뷰가 어떤 내용에 대해 서술된 것인지, 해당 내용에 대해 긍정적인지 부정적인지를 판단할 수 있도록 하는 파이프라인을 구축하는 것을 목적으로 한다. 따라서 텍스트데이터에서 핵심어를 추출한 결과와 감성분석을 실시한 결과를 통합하여 유의미한 비즈니스 인사이트를 얻어낼 수 있도록 가설을 세우고 이를 리뷰데이터 원문과 추출된 핵심어, 감성분석 결과를 비교하여 신뢰할 만한 결과를 도출해내는지 실험하는 방향으로 프로젝트를 진행한다.Github repo1. Intro리뷰, 온라인 쇼핑을 할 때면 늘 구매후기를 먼저 찾아보게 됩니다. 저 또한 마찬가지인데요.리뷰는 소비자들이 어떤 물건이나 서비스를 구매함에 있어 어느 매체의 정보보다 많은 영향을 끼치고 있습니다.당장 저부터 어떤 재화를 구매하기 전에 항상 리뷰를 검색해서 찾아보는 편인데요.위 그림은 구글에 리뷰와 마케팅을 검색했을 때 나오는 결과입니다.이렇게 소비자에게 중요한 구매지표가 된 리뷰는 이제 생산하는 재화에 대한 상품성 개선, 신상품 기획에 필요한 소비자의 의견 파악이라는, 기업의 입장에서도 마케팅용으로 반드시 활용해야하고 또 활용하고 있을 것이라는 생각이 들어서 이번 프로젝트를 기획하게 되었습니다.현재 대중적인 온라인커머스 기업인 네이버쇼핑의 리뷰입니다.빨간색으로 강조되어있는 부분처럼 판매자가 정해놓은 주제 중에 선택하여 작성하게 되어있습니다. 이처럼 기존의 리뷰데이터는 평점이 재화의 어떤 점에 대한 평가인지 업체가 정해놓은 주제 외에는 구체적으로 알기가 힘듭니다.이런 사례에서 볼 수 있듯이 현재 리뷰를 기업의 입장에서 활용하는데 가장 큰 장해물은 리뷰가 나타내는 내용과 평점이 반드시 일치하지 않는다는 것입니다.오른쪽 그림처럼 모두 평점5점의 리뷰이지만 리뷰마다 다른 특징을 평가하고 있기 때문에 리뷰내용을 분석하여 각 리뷰의 주제를 추출하고 추출된 주제에 대한 감성분석을 실시하는 것이 효율적인 사용자리뷰 활용을 위해 필요하다고 생각했습니다.1.1 가설 설정그래서 제가 세운 가설은 다음과 같습니다.“리뷰데이터에서 키워드 추출 모델을 통해 추출된 핵심어와 감성분석 모델의 분석 결과를 합쳐서 리뷰 원문의 의미와 일치하도록 하는 리뷰 분석 파이프라인을 만들어 80%이상의 정확도를 확보할 수 있다”예를들어 “생각보다 착용감이 좋아요. 사운드가 시원시원합니다”라는 리뷰가 주어졌을 때, 감성분석모델을 통해 해당 문장은 긍정으로 분류하고 키워드 추출모델에 의해서 착용감, 사운드, 좋아요 등의 키워드가 추출되어 최종적으로 해당리뷰는 착용감, 사운드 측면에서 긍정적인 리뷰임을 판별하는 파이프라인을 구축하는 것이 프로젝트의 목표입니다.2. Modeling2.1 모델 결정 - KOBERT제가 프로젝트에 사용될 모델은 버트입니다. 다층의 레이어를 활용해서 문장을 구성하고 있는 토큰 사이의 의미를 잘 추출해낼 수 있습니다.간단하게 버트가 학습하는 과정을 살펴보면 문장의 일부 단어를 mask토큰으로 바꾸고 가려진 단어를 예측하도록 하는 학습을 통해 문맥을 파악하는 능력을 기르게 되고,또 다음 문장이 올바른 문장인지 맞추는 비지도학습을 통해 문장 사이의 관계를 학습하게 됩니다.저는 버트 모델중에서도 한글에 특화되어있는 skt에서 한글로 학습시킨 kobert 모델과 google의 다중언어 bert 모델을 이용하여 프로젝트를 진행했습니다.2.2 데이터셋 결정 - 네이버쇼핑 리뷰, 네이버영화 리뷰, steam 리뷰프로젝트를 진행하기 위하여 제가 수집한 데이터셋입니다.저는 모델의 파인튜닝을 진행하기 위해 다양한 상품에 대한 리뷰 총 20만개의 리뷰를 학습과 검증용 데이터로 사용했고 테스트용 데이터를 별도로 특정 상품군을 선정하여 3600개의 데이터를 별도로 준비했습니다.리뷰 원문과 파이프라인을 거친 텍스트의 내용이 일치하는지의 판단은 문장이 나타내는 디테일한 뉘앙스와 판별 결과를 비교하기 위해서 사람이 직접 읽어보고 올바르게 판별했는지를 비교했습니다.3. 검증3.1 Modeling감성분석 모델은 f1스코어와 정밀도, 재현율 모두 0.9 이상을 기록하며 높은 성능을 나타내고 있습니다.실제 텍스트를 넣고 검증을 해봤을 때,첫번째처럼 부정 긍정이 확실한 경우는 예상처럼 잘 분류를 하는 것을 볼 수 있었습니다.두번째 리뷰는 긍정적인 부분과 부정적인 부분을 같이 적었는데 긍정으로 분류를 하였고 전체적인 문장의 뉘앙스는 긍정이 맞으므로 올바르게 분석하고 있다고 판단했습니다.마지막으로 세번째 또한 긍정적인 부분과 부정적인 부분이 같이 있지만 문장에서 느껴지는 분위기는 부정적이므로 맞게 판단하였습니다.전반적으로 감성분석 모델은 잘 구축이 되었다고 판단했습니다.다음은 키워드 추출 모델인데 키워드 추출 모델의 경우 문장을 키워드 단위로 쪼개고 쪼개진 토큰들을 임베딩모델을 통해 임베딩하여 수치화했습니다. 임베딩된 키워드들은 전체 문장과 키워드 단위로 분리된 벡터 사이의 거리를 이용하여 코사인유사도를 계산했습니다. 원래 문장 벡터와 키워드의 벡터가 유사할수록 전체 문장을 잘 표현하는 단어라고 할 수 있습니다. 계산결과에서 가장 유사한 순서대로 5개의 키워드만 추출했습니다4. 실험 결과 및 결론다음은 전체 결과입니다.파란색으로 강조된 evaluation이 감성분석을 통해 분류된 결과이고Review는 리뷰 원문, review_count는 원문을 키워드단위로 쪼갰을 때 전체 키워드입니다. 마지막으로 keyword가 전체 문장을 대표하는 키워드 5가지입니다.직접 3600개의 데이터를 비교했을 때 감성분석결과는 예상보다 세세한 문장의 분위기까지 잘 잡아내서 매우 만족스러웠는데 키워드 추출 모델의 경우는 생각보다 정확도가 많이 떨어졌습니다.이유를 세 가지 정도로 생각해봤는데,첫번째는 키워드 추출과정입니다. 키워드를 추출하는 업무는 아무래도 문장이 어떻게 토큰화 되느냐가 중요합니다. 이번 프로젝트에서는 시간관계상 코버트에서 제공하는 토크나이저를 사용했는데 실험의 정확도를 위해서는 다른 토크나이저를 사용해서 비교해봐야 결과값을 제대로 판단할 수 있을 것 같습니다.두번째는 주요 키워드 선정 방법입니다. 주요 키워드를 추출하는 방법으로 단순하게 코사인 유사도를 계산해서 거리가 가까운 순으로 주요 키워드를 선정했는데 다양한 방법으로 유사도를 확인하는 과정이 필요할 것 같습니다.마지막 세번째는 모델 자체의 기능입니다.최종 주요 키워드로 선정된 결과를 보면 제가 생각했던 것처럼 심플하게 주요 키워드가 선정되지 않았는데 모델에서 사람이 주요하다고 생각하는 키워드를 제대로 인식할 정도로 학습이 제대로 이루어지지 않은것이 아닌가 하는 의심을 해보게 되었습니다.버트 모델은 기본적으로 버트에서 사전학습된 임베딩 능력을 가져오고 내가 원하는 업무에 맞도록 파인튜닝하는 과정을 거치게 됩니다. 키워드 추출 업무의 경우, 키워드를 주요하다고 판단할 만한 근거를 finetunning을 실시할 때 충분히 학습시켜주지 않아서 이런 결과가 나왔을 수도 있겠다는 생각이 들었습니다.프로젝트를 마무리하며..이번 섹션 프로젝트를 진행하며 가장 힘든 것은 절대적인 시간이 많이 부족해서 현실과 타협을 많이 해야했던 것이었습니다. 연구실 업무, 졸업 논문에 사소하게는 컴퓨터의 환경설정 문제까지 프로젝트 외의 다른 요소 때문에 시간을 뺏겨서 프로젝트를 기한 내에 끝내기 위해 처음 계획했던 것보다는 많은 실험을 하지 못했습니다.  그래도 기존에 PyTorch 라이브러리로 사용해봤던 BERT를 이번에는 익숙한 케라스로 구현하고 또 한글특화모델인 KOBERT를 사용해보면서 나름 모델에 대해 더 이해할 수 있었던 점은 좋았던 것 같습니다.다음 프로젝트에는 이번 프로젝트에서 반드시 해보려고 했던 웹 스크래핑으로 실제로 데이터를 직접 수집해서 전처리부터 끝까지 진정한 의미의 End to End 프로젝트를 해보기로 결심하고 이번 프로젝트를 마무리합니다.",
        "url": "/bert-sentiment-keyword"
    }
    ,
    
    "deeplearning-summary": {
        "title": "Deep Learning - 핵심 개념 &amp; 용어",
            "author": "woongE",
            "category": "",
            "content": "#deeplearning #용어 #개념목차     Time seriesAnomaly Detection Generative Adversarial Networks-Model Structure       Time seriesAnomaly Detection Generative Adversarial Networks-loss function       Intro to the Time Series Anomaly Detection       Text 데이터 광고 필터링을 위한 분류 모델 구축     사용자 리뷰 핵심어 추출 및 감성분석     Deep Learning - 핵심 개념 &amp; 용어     NLP - 딥러닝 모델의 범용적 활용가능성 분석     Bank marketing 정기예금 가입여부 예측     ROC curve에 대해 알아보자     연어의 회귀본능이 아닌 선형회귀에 대해 알아보자                                                                                       Deep Learning Summary - 핵심 개념 &amp; 용어이제는 대세가 된 단어인 딥러닝. 대중적인 관심이 많이 늘어났지만 관련 분야를 공부를 하지 않은 일반인에겐 어려운 개념이나 용어가 많아 이해하기가 쉽지 않다. 그래서 오늘은 내가 배운 내용을 정리할 겸 일반인이 보기에도 이해하기 쉽도록 각 용어와 개념들을 설명해보려고 한다.전체적인 딥러닝 개요도1. 신경망 기초(1) 뉴런인공신경망에서 세포의 역할을 하고 신경망 구조도에서는 노드로 표현된다. 뉴런마다 각각 입력을 받게되고 활성함수를 거쳐 다음 뉴런으로 출력한다.(2) weight(가중치)인공신경망에서 뉴런을 연결하는 선으로 각 가중치들은 입력받은 신호를 필터링하는 역할을 한다.입력받은 값에 어느정도의 가중을 주어 다음 뉴런에 전달할 것인지 해당 데이터의 경중을 결정하는 요소이다.(3) bias(편향)뉴런들마다 각각 입력된 값과 가중치를 곱한 다음 더해주는 상수이다. 각 뉴런이 가지고있는 고유값이다.(4) 활성화함수하나의 뉴런에서 다음 뉴런으로 가중합을 전달할 때, 값을 전달할지 말지를 결정하는 기준이 되는 함수이다. 임계점을 기준으로 출력값에 변화를 주게된다.  sigmoid이진분류를 할 때 사용되며 기본적으로 임계값은 0.5로 이 값을 기준으로 작으면 0으로, 크면 1로 분류한다.  ReLu0을 기준으로 0보다 작을 때는 0을, 클 때는 입력값 자체를 출력하는 기울기가 1인 함수  softmax다중분류문제에 사용되며 0~1 사이의 값을 출력하며 확률값이기 때문에 전부 더하면 1이 된다.  Tanh시그모이드의 변형버전으로 임계값을 기준으로 -1, 1로 분류한다.2. 신경망 학습(1) 역전파 &amp; 경사하강법실제 관측값과 예측값 사이의 오차가 최소가 되도록 역전파 알고리즘은 순전파와 반대로 거슬러올라가며 경사하강법을 사용하여 가중치를 업데이트한다.(2) 손실함수  분류          binary cross-entropy이진분류할 때 사용하는 함수로 0에 가까울수록 좋다.      categorical cross-entropy다중분류에 쓰이는 함수로 0에 가까울 수록 좋다.      sparse cross-entropycategoricalCrossentropy 와 비슷하지만, 레이블이 int 형이라는 점에서 다르다.        회귀          MAE관측값과 예측값의 차이에 절대값을 취하여 모두 합한 후 평균을 계산한 값이다. 실제값과 단위가 똑같아 직관적인 비교가 가능하다.      MSE관측값과 예측값의 차이를 제곱하여 평균을 계산한 값이다. MAE보다 아웃라이어에 민감하다.      (3) optimizer  SGD경사하강법은 lost function를 미분을 통하여 최소값을 찾아가는 과정이다. 경사하강법 에서는 학습률를 중요하게 생각해야하는데, 이것이 너무 작을땐 탐색시간이 한참걸릴수도 있고, 너무 클땐 최소 손실값을 잘못 찾을수도 있기에 (전역 최솟값을 건너뛰고 지역 최솟값을 찾는 문제) 비교적 비효율적이다. 그래서 SGD는 GD 와 다르게, 한번 학습할 때 모든 데이터에 대해 가중치를 조절하는 것이 아니라, 랜덤하게 추출한 일부 데이터에 대해 가중치를 조절한다.  RMSpropAdaGrad 은 학습률을 변수들에 다르게 적용하는 방법이다. 변화가 많았던 변수들엔 학습률을 작게하고, 그렇지 않은 변수들엔 크게 하는것이다. 많은 변화가 있었다는 것은 최적값에 가깝다는 뜻이니 학습률을 작게해서 세밀하게 조정하기 위함이다. 하지만, AdaGrad 로인해 너무 극단적으로 학습률이 작아질때 생기는 학습이 멈추는 문제가 있었는데 이것을 보완한 것이 RMSprop 이다.  AdamMomentum 은 GD 의 학습률로 인한 문제를 기울기의 정도에따라 관성을 적용함으로써 완화한 것이다. Adam 은 Momentum 과 RMSprop 이 가지는 장점을 동시에 가지는 컨셉으로 생겨났다.(4) 학습 규제  dropout노드를 랜덤하게 일시적으로 끊어낸다. 해당 뉴런을 물리적으로 차단하여 학습을 실시, 과적합을 방지한다.  early stopping학습을 진행하면서 사용자가 정한 임계값 이상의 성능 개선이 없으면 학습을 조기종료하여 과적합을 방지한다.  weight decay가중치가 커지는 것을 방지하도록 가중치를 감소시키는 방법으로 과적합을 방지한다.  learning rate decay학습할수록 학습률을 감소시켜 과적합을 방지한다.(5) 메모리 사용 전략  batch모델의 가중치를 업데이트 시킬 때 사용되는 데이터의 단위를 말한다. 1000개로 구성된 데이터가 있을 때, 배치 사이즈를 100으로 설정하면 100개 단위로 가중치를 업데이트 한다. 전체 데이터에 대하여 총 10번의 가중치가 업데이트된다.  epoch &amp; iteration학습 횟수를 의미하며 1000개로 구성된 데이터에서 batch size가 10이고 epoch이 10이면 100번의 가중치 업데이트를 한 사이클로 하여 총 10 사이클을 반복하여 실시한다. 이때 100번의 가중치 없데이트 횟수가 iteration이 된다. 1epoch은 데이터 전체를 한번 사용하는 것을 기준으로 한다.3. CNN(1) convolution합성곱을 통하여 fully connected 신경망보다 가중치의 갯수가 줄어들어 결과적으로 학습시간이 훨씬 빠르고 컴퓨팅 파워를 절약시켜준다.  stridefilter를 움직이는 보폭을 의미한다. stride가 1이면 filter를 한칸씩 움직이며 특징을 추출한다. stride가 작을수록 세세하게 필터링하게된다.  padding채널은 필연적으로 입력데이터보다 작아지게 되는데 특성추출이 반복되어 차원이 0이 되는 것을 방지하는 방법이다. 데이터 가장자리에 0, 평균값 등 다양한 특정값을 채워넣어 차원축소를 방지한다. 패딩을 해주면 데이터를 골고루 사용하게 되고 합성곱 연산 이후에도 차원이 유지된다.  filterweight들의 집합으로 데이터의 특징을 추출하는 window 역할을 한다.(2) pooling채널의 피처맵 차원을 축소하는 방법으로 합성곱 연산 이후 레이어를 풀링하여 더 작은 피처맵을 추출한다.4. RNN      RNN은닉층의 출력데이터가 다시 자기자신 노드로 순환되어 입력값으로 사용된다. 이런 특성 때문에 시계열 데이터를 다루는 데에 좋은 성능을 낸다. 예를 들어, 주가데이터, 음성데이터, 텍스트데이터 같이 앞 뒤 데이터간의 연관성이 있는 데이터셋에 사용될 수 있다.        LSTMRNN의 문제 중 하나는 시계열 데이터를 다룰 때, 정보의 위치가 멀수록 역전파를 할 때 경사가 0에 수렴하기 때문에 모델 성능에 악영향을 미친다는 것이다. “나는 한국에서 한국어를 배운다”라는 문장에서 “한국어”라는 단어를 예측한다고 할때 문장에서 핵심적인 정보를 제공하는 단어는 “한국”이다. 이처럼 “한국”과 “한국어”의 위치가 가까이에 있다면 문맥을 연결하기가 쉽지만 멀리 위치해있다면 문맥을 연결하기 힘들어 성능이 저하된다. 역전파 과정에서 과거와 현재의 거리가 멀어질수록 gradient 값이 소실되는 gradient vanishing 문제가 대두되어 LSTM의 개념이 고안되었다.이전 정보를 얼마나 잊을지, 현재 정보를 어느정도로 반영할지, 정보를 밖으로 얼마나 출력할지를 결정하는 Gate를 추가하여 RNN을 보완한다.        GRULSTM의 게이트 숫자를 줄여 모델을 구조적으로 단순화했다는 점이 다르다. forget과 input gate를 합치고 output gate를 생략하였다.  5. GAN      DCGAN생성모델의 일종으로 실제로 값어치가 있는 그럴싸한 데이터를 생성해내는데 그 존재 이유가 있다. 생성자 모델과 감별자 모델 두가지가 서로 대립하여 서로 경쟁적으로 학습하여 모델의 성능을 향상시키는 컨셉이다.        CycleGAN데이터의 원래 형태는 유지하면서 세부 특성만 교체할 수 있게 하는 모델이다. 각각 두 개의 생성자와 감별자를 두어 특성을 추출한다. (말, 얼룩말), (오렌지, 사과) 이미지 쌍을 입력하여 학습을 할 때는 말을 얼룩말으로 바꾼 이미지는 얼룩말인지 아닌지 판단하여 얼룩말이라고 판단되도록, 또 변경된 이미지를 다시 말로 바꿔서 원본과 비교함으로써 다른 속성들은 그대로 유지되었는지 판단하며 그 차이를 줄이는 방향으로 학습한다.  6. Autoencoder오토인코더는 인코더와 디코더로 구성되어있다. 인코더는 입력 데이터를 압축하여 벡터화 하게되고 디코더는 압축된 벡터를 다시 원본데이터와 유사하게 복원하는 역할을 한다.원래의 입력데이터 x와 압축, 복원을 거친 x’의 차이를 손실로 정의하고 손실이 줄어들 수 있도록 역전파를 통해 학습이 이루어진다. 입력데이터를 벡터화하는 과정에서 입력 데이터를 정의할 수 있는 잠재변수가 만들어지는데 이 잠재변수의 크기가 커지면 데이터를 구분할 수 있는 더 다양한 특징들이 추출된다. 반대로 잠재변수의 크기를 줄이면 특징의 수는 줄어들지만 더 핵심적으로 데이터를 구분할 수 있는 특징들이 추출된다.      Encoder : 입력 데이터의 특징을 담고있는 Latent 로 데이터를 압축시키는 단계.        Decoder : 압축된 Latent 를 입력데이터와 유사하게 출력하기위해 확장하는 단계.        오토인코더의 용도          노이즈 제거      특성 추출      7. Attention      Transformer자연어처리 분야에서 사용되는 기술로 문장 데이터를 한번에 입력하고 순서정보를 입력해 번역하는 모델. 크게 음성을 텍스트로 변환하거나 텍스트를 음성으로, 또는 텍스트를 다른 언어로 번역하는 모델 등이 있다.        BERT  구글에서 개발하고 사전훈련시킨 자연어처리에 사용되는 범용 언어모델이다. 총 33억 개의 코퍼스(bookcorpus : 8억, wikipedia : 25억)를 사용하여 개인이 구축하기 힘든정도의 방대한 양의 코퍼스(의미뭉치)로 상당한 시간 사전학습을 마쳐놓아 상당한 성능의 모델을 바로 가져다 쓸 수 있게 구현 해놓다.자연어를 처리할 때, 데이터가 충분하다면 Embedding 과정이 성능에 큰 영향을 미치는데 이 말은 단어의 의미를 잘 간직할 수 있도록 벡터로 표현하는 것이 중요하다는 의미이다. 이 Embedding 과정에 구글의 강력한 BERT를 이용하는 것이고 사용하고자 하는 문제에 맞도록 파인튜닝을 진행한 후에 분류에 적용하게 된다.",
        "url": "/deeplearning-summary"
    }
    ,
    
    "versatility": {
        "title": "NLP - 딥러닝 모델의 범용적 활용가능성 분석",
            "author": "woongE",
            "category": "",
            "content": "#BERT #textmining #deeplearning목차     Time seriesAnomaly Detection Generative Adversarial Networks-Model Structure       Time seriesAnomaly Detection Generative Adversarial Networks-loss function       Intro to the Time Series Anomaly Detection       Text 데이터 광고 필터링을 위한 분류 모델 구축     사용자 리뷰 핵심어 추출 및 감성분석     Deep Learning - 핵심 개념 &amp; 용어     NLP - 딥러닝 모델의 범용적 활용가능성 분석     Bank marketing 정기예금 가입여부 예측     ROC curve에 대해 알아보자     연어의 회귀본능이 아닌 선형회귀에 대해 알아보자                                                                                       Section 4 Project - Text emotion classificationDeep Learning 섹션의 마무리 프로젝트는 텍스트 감정분석 모델의 범용적인 활용 가능성을 검증해보는 것으로 결정했다. 아직까지는 일주일이라는 짧은 시간동안 데이터셋과 레퍼런스 코드를 찾고 그것들을 내 환경에서 완벽하게 구현해내는 것들이 익숙하지 않아 주제 선정에 어느정도 제약이 따를 수밖에 없었다. 그래서 평소 관심있던 분야 중, 내가 지금 일주일 안에 어느정도의 완성도 이상을 결과물로 낼 수 있을 것 같은 모델과 데이터셋을 선정했다. 내가 정한 분야와 데이터셋에서 적절한 가설을 세우고 그 가설을 검증하는데 필요한 딥러닝 파이프라인을 구축하는 것이 프로젝트의 목표였다.가설 설정을 위해 끊임없이 질문을 던지고 그 질문을 어떻게 해결해야할지, 검증은 어떻게 해야 할지를 고민하며 밤을 지새웠던 지난 일주일 간의 고군분투기를 적어본다.Github repoabstract기업은 최소한의 자원을 투자하여 최대의 이익을 보기 위하여 노력한다. 본 프로젝트는 제품이나 서비스에 대한 감상평을 남기는 리뷰와 같은 텍스트 데이터를 활용하여 해당 제품이나 서비스에 대한 감정이 긍정적인지, 부정적인지 분류하는 감정분류를 실시한다.감정 분류를 실시할 때, 매번 새로운 모델링을 하지않고 분야별로 한 번의 파인튜닝을 실시한 모델로 범용적 활용 가능성에 대해 고찰한다.1. 서론마케팅, 누구라도 한 번쯤은 들어봤을 만한 단어입니다.마케팅은 목적하는 바를 달성하기 위해 유도하는 모든 활동을 말합니다.우리가 흔하게 이야기하는 마케팅은 경제적인 관점에서의 소비를 유도하기 위해 하는 활동들입니다.과거에는 기업이 제품을 만들어 소비의 방향을 선도하고 소비자는 만들어진 기업의 소비 트렌드를 따르던 패턴이었습니다. 하지만 현재에 이르러서는 경제적인 여유와 의사표현수단이 다양해지면서 소비자가 트렌드를 선도하고 기업이 이에 맞는 제품이나 서비스를 제공하는 형태로 경제활동의 패러다임이 크게 변화했습니다.이런 환경속에서 마케팅 분야의 가장 큰 관심사는 “소비자나 사용자의 마음을 파악하는 것”이 되었습니다. 소비자의 마음을 알아야 알맞은 서비스나 제품을 만들 수 있고 또 만들어진 상품을 잘 팔 수 있기 때문입니다.1.1 가설 설정최근 기업들은 소셜미디어나 리뷰 텍스트 등을 기반으로 감정을 분석하여 마케팅에 활용하는 사례들이 점점 많아지고 있습니다.감정분석을 하는 프로세스를 크게 나누어보면,  감정을 분석하고자 하는 데이터를 정제  데이터의 고유특성을 고려하여 분류모델 설계  생성한 모델에 데이터를 학습이 과정은 보기엔 단순해보일지 모르나 데이터의 복잡성이나 크기에 따라 상당히 많은 시간을 소요하게 됩니다. 문제는 데이터가 바뀔 때마다 저 모든 과정을 다시 거쳐야 한다는 것입니다.그래서 저는 자연어 처리분야에서 소비자의 텍스트데이터를 활용하여 감성분류를 할 때, 다양한 분야에 범용적으로 활용할 수 있는 모델이 있으면 좋지 않을까 라는 생각을 해보게 됐습니다.이 프로젝트를 진행하면서 검증할 저의 가설을 세워봤습니다.“같은 언어를 사용하고 감정을 이진 분류 한다는 가정 하에, 한 분야에 대해 학습한 모델이 다른 분야에 사용될 수 있다”이런 가설을 생각해보게 된 이유는 제가 감정분류를 할 때 사용할 모델을 찾아보다가 구글의 BERT라는 모델에 대해 알게되었고 이를 사용하기로 마음먹었기 때문입니다.구글이라는 큰 기업에서 방대한 양의 코퍼스로 사전학습을 시켜놓았기 때문에 비슷한 형태의 데이터에 수행할 내용이 같다면 성능차이가 크지 않을 것이라는 생각에서 이런 가설을 세우게 되었습니다.2. Modeling2.1 모델 결정 - BERT간단하게 BERT를 설명해보면,구글에서 개발하고 사전훈련시킨 자연어처리에 사용되는 범용 언어모델입니다. 총 33억 개의 코퍼스(bookcorpus : 8억, wikipedia : 25억)를 사용하여 개인이 구축하기 힘든정도의 방대한 양의 코퍼스(의미뭉치)로 상당한 시간 사전학습을 마쳐놓아 상당한 성능의 모델을 바로 가져다 쓸 수 있게 구현 해놓았습니다.자연어를 처리할 때, 데이터가 충분하다면 Embedding 과정이 성능에 큰 영향을 미치게 됩니다. 이 말은 단어의 의미를 잘 간직할 수 있도록 벡터로 표현하는 것이 중요하다는 의미입니다. 이 Embedding 과정에 구글의 강력한 BERT를 이용하는 것이고 제가 사용하고자 하는 문제에 맞도록 파인튜닝을 진행한 후에 분류에 적용하게 됩니다.2.2 데이터셋 결정 - 네이버쇼핑 리뷰, 네이버영화 리뷰, steam 리뷰프로젝트를 진행하기 위하여 제가 수집한 데이터는 세 종류입니다.steam 리뷰는 게임을 판매, 구동시켜주는 steam 플랫폼의 리뷰로 추천, 비추천 두 가지로 평가되며,네이버 쇼핑, 영화 리뷰는 별점 5점으로 평가된 데이터로 중간값인 3점은 제외되고 1,2,4,5 점의 리뷰만 수집된 데이터로 1,2 점은 부정, 4,5점은 긍정으로 다시 분류하였습니다. 각 데이터는 긍정과 부정 비율을 동등하게 맞추어 샘플링 되었기 때문에 추후 평가에 사용될 기준모델의 정확도는 50%입니다.      네이버 쇼핑 리뷰쇼핑데이터는 물품에 대한 만족, 불만족이 비교적 텍스트에 단순하게 나타나 있는 모습을 볼 수 있습니다.        네이버 영화 리뷰영화데이터는 영화에 대한 정보가 함축적으로 들어있거나 조금 더 다양한 어휘가 사용되고 있습니다.        steam 리뷰스팀데이터의 경우는 비어, 속어, 은어 등이 다양하게 나타나있음을 볼 수 있습니다.  3. 검증3.1 실험 흐름실험의 전체적인 흐름은,  방대한 양의 코퍼스로 사전학습을 마친 모델을 불러오고  사용하길 원하는 데이터로 파인 튜닝을 실시합니다.  이후에 해당 태스크를 수행하게 됩니다.따라서 저는 3가지 데이터를 각각 파인튜닝한 모델 3가지와 3가지 데이터 모두를 사용하여 파인튜닝한 모델까지 총 4개의 모델을 생성하고, 각 모델에 자신이 학습하지 않은 나머지 데이터셋을 분류시켜 정확도를 평가할 것입니다. 제가 세운 가설이 맞으려면 각 모델이 다른 데이터셋을 분류할 때도 정확도의 차이가 크게 나지 않아야 될 것입니다.시간적인 제약때문에 BERT를 사용한 모델 중 한국어의 리뷰모델을 분석할 때 성능이 좋았다는 모델의 파라미터를 서칭해서 그대로 대입한 후 모든 실험을 같은 조건 하에서 진행했습니다.3.2 실험 결과기본 bert 모델에서 파인튜닝을 다르게 한 4가지의 모델을 만들었고  모델별 분류 정확도를 정리했습니다.모델별 분류 정확도를 정리했습니다.당연할지도 모르겠지만 각 모델들이 학습에 사용된 데이터에 대해 분류할 때 예를들면 쇼핑데이터 학습 모델은 쇼핑데이터를, 영화데이터 학습 모델은 영화데이터를,스팀데이터 학습 모델은 스팀데이터를 예측할 때가장 높은 정확도를 보였습니다.특이한점을 살펴보면, 쇼핑모델의 경우는 학습한 데이터와 다른 분야의 데이터를 테스트했을 때유독 정확도가 많이 떨어졌습니다.쇼핑데이터를 추출해봤을 때, 전반적으로 영화리뷰나 스팀리뷰에 비해 리뷰내용이 단순하여 다른 리뷰를 분류 할때 정확도가 떨어진 것이 아닌가 추측됩니다.스팀모델의 경우도 특이한점이 발견됐는데 분야를 가리지 않고 정확도가 다른 모델들에 비해 낮게 나왔습니다. 이는 스팀 모델의 경우 데이터셋이 다른 모델보다 적기 때문이거나 구글에서 사전 훈련에 사용한 코퍼스에 비어, 속어, 은어는 없기 때문일 수도 있을 것 같습니다.또 다른 특이점은스팀모델만 유독 학습데이터와 다른 분야의 데이터에서도 성능하락 폭이 크지 않았다는 점입니다.모든 데이터를 넣어 finetuning 한 ALL모델의 경우는 언뜻 생각하기엔 훈련데이터를 다 모아서 학습했으므로 전반적으로 성능이 오를 것으로 예상했으나 평가 정확도는 각 데이터만 훈련한 모델들과 같았습니다.다만 스팀데이터의 경우엔 스팀데이터만 학습시킨 경우보다 정확도가 5%정도 상승했는데이는 쇼핑, 영화데이터를 통해 학습한 내용 중 어떤 부분이 스팀데이터를 예측하는데 도움을 주었음을 알 수 있습니다.4. 결론범용적인 모델을 만들 수 있을 것이라고 예상했던 저의 가설은 기준을 어디에 두느냐에 따라 완전히 틀리지도, 맞지도 않는 가설이 된 것 같습니다.한 가지 데이터로 학습한 후 다른 분야 데이터를 분류했을 때, 대부분 성능이 적게는 3에서 많게는 23% 까지 차이가 났지만 그렇다고 하더라도 베이스라인 정확도 50%는 모두 큰 폭으로  상회하는 수치이기 때문입니다.각 분야에 특화된 모델보다는 정확도가 떨어지지만 이진분류와 같은 단순한 문제에서 일정부분 성능을 포기하더라도 학습을 별개로 진행할 필요가 없다는 점에서 범용모델로 사용할 수 있을 정도는 될 것이라고 생각했습니다.코드스테이츠의 데이터사이언티스트 코스 네 번째 섹션을 마무리하며..이번 섹션 프로젝트를 진행하며 가장 힘든 것은 가설을 설정하는 부분이었다. 대학원에서도 논문을 작성할 때 가장 힘든 것이 바로 ‘어떤 질문을 해야하는가’인데 이번 프로젝트는 마치 소논문 하나를 작성하는 것 같은 느낌을 받았던 프로젝트였다.가장 다뤄보고 싶었던 자율주행 자동차에 사용되는 윤리적 판단 모델은 이제 연구가 활발히 이루어지고 있는 분야여서 구현할 수 있는 레퍼런스모델과 코드들을 구하기가 쉽지 않아 눈물을 머금고 후퇴했다. 바로 Plan B로 생각했던 NLP분야에 대해 서칭하여 BERT라는 모델에 대해 알게되고, 처음에는 내가 배웠던 keras 프레임워크로 구현을 시도했는데 계속 내가 사용할 데이터셋에 적용하여 학습시키면 20분정도 학습이 진행되다가 오류를 뱉어내었다. 이것도 후퇴해야하나 싶을 때, 혹시모르니 해보자 싶어 도전했던 pytorch 프레임워크로 구현된 레퍼런스 모델과 코드를 활용하여 모델 구현에 성공했다. 배우지 못했던 프레임워크라서 도전할 생각조차 안해봤었지만 막상 구현에 성공하고 보니 일단 해보려는 시도가 중요하다는 사실을 다시한번 몸으로 배울 수 있었다.짧다면 짧고 길다면 긴 4달 남짓한 시간동안 이 분야에 대해 공부하고 몸으로 부딪히며 자꾸 느끼게 되는 것은 정말 지식이 너무나도 방대한 분야라는 것이다. 나는 이 모든 지식들을 알 수는 없고 어차피 새로운 기술이 워낙 빠르게 등장하기 때문에 내가 당장은 이해할 수 없더라도 새로운 것을 받아들여 빠르게 내가 원하는 데이터에 맞게 적용할 수 있는 능력이 정말 중요하다는 것을 새삼 느낀다. 실제로 이런 과정을 거쳐 이번 프로젝트를 마친 지금, 파이토치에 대해 아주 작은 것 하나라도 전보다는 더 알게되지 않았을까.몰랐던 기술이라도 꼭 구현할 수 없는 것은 아니라는 사실을 배웠다는 것이 이번 프로젝트에서 배운 가장 큰 수확이 아닐까 생각한다.",
        "url": "/versatility"
    }
    ,
    
    "twitter-web-application": {
        "title": "Twitter Web Application",
            "author": "woongE",
            "category": "",
            "content": "#Twitter #application #softwareengineering목차     Software Engineering - Python - \"Class\" 이녀석     Software Engineering - Twitter Web Application                                                                            Section 3 Project - Twitter ApplicationSoftware Engineering 섹션의 마무리 프로젝트 주제는 Flask를 이용하여 웹 어플리케이션 만들기. 트위터 API를 활용하여 데이터를 받아오고 DB를 구축하여 저장, 다시 저장된 데이터를 불러와 각종 기능들을 구현하는 것이 최종 목표였다.섹션 내내 나를 혼돈속으로 밀어넣었던 각종 개발 툴들이 어떻게 활용되고 그것들의 퍼즐을 맞춰 최종적으로 어떤 어플리케이션이 탄생했는지 소개하려고 한다.Github repo무슨 기능을 가진 어플리케이션인가??트위터 어플리케이션은 메인페이지와 다섯가지의 기능을 가진 각 페이지를 합쳐 총 여섯개의 페이지로 구성되어 있다.각각의 기능을 살펴보면,      Home : 가장 처음 접속할 때 보이는 대문과 같은 페이지로 아래에 기술될 기능들을 사용하며 데이터베이스가 구축되면 user 테이블 전체를 쿼리해서 확인할 수 있도록 구성되어 있다.        Add : 어플리케이션에서 정보를 조회하고싶은 트위터 유저의 screen name을 입력하면 해당 유저의 ID, Username, Full Name, Location을 API를 통해 받아와서 데이터베이스의 User 테이블에 저장한다. 또한 해당 유저의 트윗 기록(ID, Text, User ID)도 Tweet 테이블에 저장하며 User 테이블에 저장된 내용을 페이지 하단부에 쿼리하여 출력한다.        Get : Add페이지에서 조회하여 저장된 유저의 Tweet 테이블을 쿼리하여 하단부에 출력한다.        Delete : 데이터베이스에 저장된 유저의 User, Tweet 테이블의 데이터를 삭제할 수 있다.        Update : 저장된 데이터베이스에서 유저의 Full Name을 변경할 수 있다.        Predict : 트윗 내용을 제시했을 때, 두 명의 트위터 유저 사이에서 누가 해당 트윗을 작성했을 것인지를 예측할 수 있는 기능이다.  어플리케이션 개발 과정개발에 사용된 모듈 및 패키지  Tweepy : 트위터 API를 파이썬에서 활용가능하게 해주는 모듈.  Flask : 파이썬으로 웹 어플리케이션을 개발하기 위한 프레임워크. API 어플리케이션을 만들기 위한 각종 편의 기능들을 제공한다.  FLASK SQLAlchemy : ORM(Object Relational Mapper)의 한 종류로 Flask 프레임워크에서 데이터베이스와의 상호작용을 파이썬과 비슷한 객체형식으로 가능하게 해주는 모듈.1. __init__.py &amp; models.py 생성      init.py : 어플리케이션을 initialize하기 위한 파일로 블루프린트를 사용하여 함수를 여러 곳으로 확장하고 분산하여 구현하였다. 블루프린트를 사용하지 않아도 웹서버 구현이 가능하지만 함수들을 분리하여 관리하면 어플리케이션에 기능이 많아지면 파일이 길어져 관리가 힘들어지기 때문에 분산관리의 이점이 드러나게 된다.사용자가 웹페이지에서 url을 입력했을 때, 해당 url과 route 파일이 상호 연결될 수 있도록 해준다.        models.py : Flask_SQLAlchmy를 활용하여 DB에 데이터를 저장하는 형식을 지정하는 파일이다. 테이블, 컬럼 명 등을 지정하고 테이블간의 관계도 이곳에서 정해주게 된다.  2. 데이터베이스 연결__init__.py와 models.py 파일을 작성했다면 어플리케이션에서 사용자가 입력한 정보를 저장할 데이터베이스를 구축하고 연결해야 한다.데이터베이스는 간단히 코드 세줄로 구축 및 연결이 가능하다.# 데이터베이스 구축FLASK_APP=twitter_app flask db init# 데이터베이스 테이블 생성FLASK_APP=twitter_app flask db migrate#데이터베이스 테이블에 세부 컬럼 생성FLASK_APP=twitter_app flask db upgrade3. routes &amp; templates 생성routes폴더와 templates폴더는 각각의 파일들을 관리하게 되는데 routes파일에는 어플리케이션의 기능 구현에 대한 코드를 작성하게 되고 templates 폴더에 담겨지는 html 파일은 routes 파일로 구현된 기능을 웹페이지에 어떻게 뿌려줄지를 결정하는 역할을 한다.쉽게 말하면 routes는 기능구현 파일, html파일은 웹에 어떻게 보여질지를 구성하는 파일이다.  main_routes.py &amp; index.html : Home 웹페이지에 접속하면 처음으로 보이는 페이지로 index.html로 User 테이블의 정보를 전달한다.  add_routes.py &amp; add.html : = 사용자가 입력한 트위터 유저의 username을 add.html을 통하여 radd_routes.py로 전달하고 해당 유저의 정보와 트윗기록들을 User 테이블과 Tweet 테이블에  저장한다.  get_routes.py &amp; get.html : 사용자가 입력한 트위터 유저의 username을 get.html을 통하여 get_routes.py로 전달하고 Tweet 테이블에 저장된 데이터중 해당 username 과 일치하는 레코드들을 쿼리하여 get.html 에 전달한다. get_html은 전달받은 레코드를 웹페이지에 출력한다.  delete_routes.py &amp; delete.html :  사용자가 입력한 트위터 유저의 username을 delete.html을 통하여 delete_routes.py로 전달하고 해당 유저에 대한 User, Tweet 테이블 내의 정보를 모두 삭제한다.  update_routes.py &amp; update.html : add를 통해 저장된 데이터 중 사용자가 입력한 트위터 유저의 FullName을 delete.html을 통하여 delete_routes.py로 전달하고 User 테이블 내의 FullName을 업데이트 한다.  predict_routes.py = predict.html : 사용자가 입력한 트윗 내용에 대해 add를 통해 저장된 두 명의 트위터 유저 사이에서 누가 입력 트윗을 작성했을지 예측한다.로지스틱회귀모델을 적용하여 데이터베이스에 저장된 유저의 트윗내용으로 학습하여 입력된 트윗내용을 예측하게 된다.4. 어플리케이션 구동여부 확인 후 배포어플리케이션을 본격적으로 웹에 배포하기 전에 로컬환경에서 앱의 구동여부를 확인해야 한다.하지만 보통 개발과정에서 셀 수 없을 정도로 많은 에러를 접하게 되기 때문에 수시로 어플리케이션을 구동하여 각각의 파일들의 상호작용이 원활한지 확인하는 과정을 거치게 된다.로컬에서 기능이 문제없이 작동한다면 웹에 배포하기 위하여 클라우드 플랫폼을 준비해야 한다.나는 이번에 개발한 어플리케이션을 heroku라는 플랫폼을 활용하여 배포할 것이다.코드스테이츠의 데이터사이언티스트 코스 세번째 섹션을 마무리하며..항상 프로젝트를 진행하면서 이 프로젝트는 추후 현업에서 어떻게 사용할 수 있을까에 대한 고민을 하게 된다. 그 고민은 일주일 동안 열정을 쏟을 나의 프로젝트에 대해 더 애착을 가지게 해주고 나아가 더 나은 결과물을 얻을 수 있는 큰 동기부여가 된다.이번 프로젝트는 데이터 사이언티스트의 주 업무라고 할 수 있는 데이터를 다루는 과정에서 친절하게 데이터가 주어지지 않았을 때를 대비한 훈련이라고 생각되었다. 공식적으로 제공하는 API를 활용하여 데이터베이스를 구축하고 축적된 데이터를 바탕으로 데이터 사이언티스트 본연의 임무를 가능하게 해주는 일종의 준비단계인 셈이다. 어떤 환경에서 일하게 될지 모르기 때문에 웹에서 직접 데이터베이스를 구축하고 데이터베이스를 활용하여 기능을 구현하는 연습은 좋은 데이터사이언티스트가 되는데도 큰 도움을 줄 것이다.섹션에서 배웠던 모듈들만 활용한 제한적인 범위 내에서 수행된 프로젝트였지만 프로젝트를 진행하며 3주동안 배운 지식들이 퍼즐이 맞춰지는 듯한 신기한 경험과 더불어 각각의 패키지와 라이브러리들이 어떤 역할을 하는지 확실하게 이해할 수 있는 시간이었다. 더불어 에러메시지를 대하는 태도도 조금은 의연해진 것 같다. 하지만 여전히 나만의 코드를 독창적으로 작성할 수 없다는 한계점을 여지없이 드러내며 스스로에게 숙제를 안겨주기도 한 시원섭섭한 프로젝트였다.",
        "url": "/twitter-web-application"
    }
    ,
    
    "python-class": {
        "title": "Python - &quot;Class&quot; 이녀석",
            "author": "woongE",
            "category": "",
            "content": "#Python #Class #softwareengineering목차     Software Engineering - Python - \"Class\" 이녀석     Software Engineering - Twitter Web Application                                                                            Software Engineering…통계, 선형대수, ML 섹션을 거치면서 늘 사용해왔던 파이썬.노트북 파일(ipynb)로 편하게 이용해왔지만 기본적인 지식이 부족하여 수시로 뱉어내는 에러를 해결 하는 과정은 녹록치 않았다.그래서 그동안 에러를 볼 때마다 이번 섹션인 Software Engineering 섹션을 내심 기다려왔다. 그리고 마침내 두둥등장.기본적인 개발환경에 대한 이해부터 직접 설정해보기도 하고 파이썬에 대해 조금 더 자세하게 들여다 본 한 주가 지났다. 이번 주에 배웠던 내용중에 이해가 잘 가지 않아 고생했던 Python의 Class가 바로 오늘의 주제다. 시작해보자.Class?다른 프로그래밍 언어인 C에는 클래스가 없다는데 그럼 클래스는 왜 필요한 걸까? 흔히 클래스의 개념을 설명할 때 계산기로 많이 설명하는데 이보다 적절한 예시를 찾지 못했기 때문에 계산기를 예로 들어보겠다.계산기에 1+1을 입력하면 계산기는 2라는 값을 출력한다. 여기서 바로 +3를 누르면 5를 출력해준다. 첫번째 계산에서 출력된 결과인 2를 어딘가에 저장해 두었고, 덕분에 우리는 바로 +3만 눌러서 5라는 결과를 받아볼 수 있다. 이렇게만 보면 뭐가 특별한가 싶지만 동시에 더 많은 계산을 하고, 더 복잡한 계산을 할수록 이전의 계산결과 저장을 위해 더 많은 메모리를 필요로 한다. 클래스라는 개념은 이렇게 많은 메모리가 필요한 상황을 타개하기 위한 것이다. 클래스를 잘 사용하면 메모리를 효율적으로 사용할 수 있다는 이야기. 여기까지 보면 클래스가 왜 필요한지에 대한 근본적인 궁금증은 해결이 되었을 것 같다.클래스에 대해 공부하다보면 클래스와 객체가 있다는데 이는 또 무엇인가…쉽게 이야기하면 클래스는 붕어빵틀, 객체는 붕어빵이라고 생각하면 된다. 우리는 붕어빵틀(class)를 이용해서 붕어빵(object)을 계속 찍어낼 수 있다. 클래스와 객체를 이해할 때 가장 중요한 것은 찍어낸 붕어빵들 간에는 고유한 특성을 가진다는 것이다.(한 개의 붕어빵의 꼬리를 베어먹었다고 해서 다른 붕어빵의 꼬리가 잘려나가지 않는 것처럼) 서로서로 독립되어있다는 이야기.class BreadFrame:    pass이렇게 빵틀이라는 클래스가 있으면soboro = BreadFrame()garlicbread = BreadFrame()soboro, garlicbread 처럼 많은 빵들을 찍어낼 수 있다.BreadFrame라는 빵틀의 결과물인 soboro, garlicbread가 바로 객체가 되는 것이다.여기서 인스턴스라는 용어와 객체가 헷갈릴 수 있는데 요점만 딱 말하자면 `soboro`는 객체(object)이자 `BreadFrame`의 인스턴스이다.남들은 쉽다고 하겠지만 나는 골머리를 앓았던 계산기를 클래스를 활용하며 만들어보자.class Calcula:    def __init__(self, a, b):        self.a = a        self.b = b    def addition(self):        return self.a + self.b        def subtraction(self):        return self.a - self.b상속다음은 클래스의 상속에 관한 내용이다.흔히 재산을 상속받는 다는 얘기로 많이 들어보았을 상속은 클래스에서도 같은 의미로 적용된다.앞의 계산기에서 덧셈, 뺄셈만 가능한 불완전한 계산기를 만들었는데(클래스 이름이 Calcula인 이유) 이 두가지 기능만 넣은 이유가 있다.바로 이 상속 개념을 적용하여 곱셈, 나눗셈 기능을 추가해 볼 것이다.class Calculator(Calcula):    def __init__(self, a, b):        super().__init__(a, b)    def multiplication(self):        return self.a * self.b    def division(self):        return self.a / self.b이렇게 완성된 계산기 Calculator를 확인해보면# 인스턴스 생성result = Calculator(1, 2)# inputresult.addition()# output3기존의 Calcula로부터 덧셈기능을 잘 상속받은 것을 확인할 수 있고,# inputresult.multiplication()# output2Calculator 클래스에서 추가한 곱셈기능도 무사히(?) 작동하고 있는 것을 확인할 수 있다.여기서# 생성자 함수def __init__(self, a, b):    self.a = a    self.b = b__init__은 생성자 함수로, 해당 클래스(붕어빵틀)로 생성된 인스턴스(붕어빵)의 초기값을 생성하는 함수다. 객체가 생성되는 동시에 자동으로 호출된다.파이썬을 사용하면서 가장 기본이 되는 class에 대해서 알아보았다.파이썬을 계속 쓰는 한 사용할 일이 많을 것 같고 더 심오한 내용도 많을 것 같지만 현재 이해할 수 있는 선에서 적어보았다. 일단 벌려놓아야 수습을 한다는 코치님들의 말을 믿고 일단 class에 대해 우선 포스팅을 해놓고 더 알아갈 때마다 와서 내용을 보완해야 할 것 같다.코드스테이츠의 데이터사이언티스트 코스 세번째 섹션의 첫 주를 마무리하며..이번 섹션을 너무 겁없이 기다려왔던 것이 아닌가.. 싶을 정도로 어려운 한주였다.그동안 편하게 사용해왔던 shift + enter를 뒤로하고 본격적으로 CLI(Command Line Interface)를 활용하여 학습을 진행하면서 느낀점을 몇글자 적어보자면,힘들었지만 그래도 장점이라면,검은 바탕의 힌 글씨를 보고있노라면 예전부터 대중매체에서 익숙하게 봐왔던 해커가 된 것 마냥 컴퓨터를 뭔가 전문적(?)으로 이용하고 있다는 느낌을 준다는 것이고 환경을 하나씩 갖추어가며 검은바탕에서 코드를 끄적이는 기분이 어렵고 힘들면서도 이상하게 괜찮다.단점이라면, 그것을 제외한 모든것인 것 같지만 마냥 싫은 느낌은 또 아니다.그동안 컴퓨터를 사용하면서 GUI(Graphic User Interface)에 익숙해져있던 나는 마우스로 더블클릭하면 간단히 해결되는 폴더 열어보기 등 모든 과정을 명령어를 입력해야하는 환경에 놓여졌다. 검은화면을 보면서 어찌해야할지 몰라 방황하는 시간이 많아졌고 명령어를 찾기위해 검색을 하는 시간도 훨씬 잦아졌다. 개발환경을 세팅하면서 밤을 지새운 것은 덤이다. 하지만 이런 과정이 나를 담금질하는 과정이라고 생각하니 힘들어도 이겨내야겠다는 마음을 먹게 되더라.굳세어지자.",
        "url": "/python-class"
    }
    ,
    
    "bank-marketing": {
        "title": "Bank marketing 정기예금 가입여부 예측",
            "author": "woongE",
            "category": "",
            "content": "#randomforest #XGB #machinelearning목차     Time seriesAnomaly Detection Generative Adversarial Networks-Model Structure       Time seriesAnomaly Detection Generative Adversarial Networks-loss function       Intro to the Time Series Anomaly Detection       Text 데이터 광고 필터링을 위한 분류 모델 구축     사용자 리뷰 핵심어 추출 및 감성분석     Deep Learning - 핵심 개념 &amp; 용어     NLP - 딥러닝 모델의 범용적 활용가능성 분석     Bank marketing 정기예금 가입여부 예측     ROC curve에 대해 알아보자     연어의 회귀본능이 아닌 선형회귀에 대해 알아보자                                                                                       Section 2 Project - Bank marketing 정기예금 가입여부 예측하기abstract  이윤을 추구하는 기업의 입장에서는 최소한으로 투자하고 최대한의 효과를 보기위해 부단히 노력하기 마련이다. 때문에 자사의 제품이나 서비스에 대한 마케팅을 실시할 때도 불필요한 지출은 줄이고 꼭 필요한 비용만을 이용하여 최대효율을 달성하는 것을 지향한다.본 프로젝트는 은행에서 정기예금에 대한 마케팅을 실시할 때, 무작위로 대상을 선정하기보다 정기예금을 신청할 것 같은 대상을 효율적으로 분별하여 마케팅을 실시할 수 있도록 한다. 핀포인트 마케팅이 가능하도록 하는 예측모델을 만들어보고 이를 개선시키는 것을 목적으로 진행하였다.1. 시나리오포르투갈의 한 은행에서 새롭게 출시하는 정기예금 상품을 기존 고객들을 대상으로 영업을 할 계획을 세우고 있다.기존에는 무작위로 대상을 선별하여 연락하는 방법을 고수하였으나 이때 임시로 텔레마케터를 고용하는데 드는 비용이나 마케팅에 성실하게 응답하는 고객이 많이 없다는 점이 문제로 지적된다. 그래서 이 문제를 해결하기 위해 은행에서 수집한 고객 데이터를 바탕으로 마케팅이 성공할 수 있을지 여부를 분류해주는 예측모델을 만들어달라고 의뢰하였다. 이렇게 우리는 제공받은 고객데이터를 바탕으로 \"정기예금 신청여부\"를 예측해주는 모델을 제작하게 되는데...1.1 제공받은 데이터bank client data: (은행 고객관련 특성)- 나이- 직업- 결혼 여부- 교육수준- 채무불이행 여부- 주택 담보 대출 여부- 개인 대출 여부related with the last contact of the current campaign: (현재 캠페인의 마지막 연락 관련 특성)- 접촉 수단- 마지막 연락(월)- 마지막 연락(요일)- 마지막 접촉 기간(초) : leakage 주의other attributes: (기타 특성)- 캠페인 중 고객에게 연락한 횟수- 이전 캠페인에서 고객과 마지막으로 연락 후 경과일- 캠페인 이전 고객에게 연락횟수- 이전 마케팅 캠페인 결과social and economic context attributes(사회, 경제적 특성)- 고용 변동률-분기별- 소비자 물가지수-월별- 소비자 신뢰지수- Euribo(유럽연합 내 12개국 시중은행간 금리)- 직원 수(분기)- 정기예금 신청 여부1.2 예측모델 정의내가 만들게 될 모델은 데이터를 입력받았을 때 정기예금 신청여부를 “예”, “아니오”로 대답하는 이진분류 모델이 될 것이고 예측모델의 타겟이 되는 정기예금 신청여부를 확인해본 결과 yes와 no가 약 1:9 의 비율로 상당히 불균형한 것을 알 수 있다.이런 경우, 전부 ‘no’로 예측해도 정확도가 90%에 가깝기 때문에 정확도를 지표로 삼기보다 다른 평가지표를 활용해야 할 것이다.accuracy나 confusion matrix의 경우는 클래스불균형 문제에서 퍼포먼스를 왜곡할 가능성이 있다는 연구결과를 보게되었고 이때 추천할 수 있는 방법으로 클래스 불균형 여부에 상관없이 일정한 퍼포먼스 인덱스를 제공해주는 AUC, F1-score 두 가지를 평가에 활용할 예정이다. 또한 평가방법으로는 Cross validation을 선택하였는데 이는 전체적인 데이터셋의 크기가 크지 않다고 판단했기 때문이다.2. EDA데이터는 상당히 잘 정제되어있는 상태였으며 간단한 전처리과정을 거치고 바로 EDA 작업에 들어갔다.가장 먼저 각각의 개별적인 특성에 따라 정기예금 신청여부를 얼마나 잘 구별할 수 있는지 알아보기위해 특성별 타겟분포를 시각화해본 결과 중에 타겟의 분포에 따라 뚜렷한 차이를 보여 타겟 예측에 도움을 줄 것 같은 특성들과 별로 차이가 없어 별 도움을 줄 것같지 않은 특성을 몇가지 소개한다.먼저 이전의 마케팅 결과에 대한 특성이 존재하여 예측에 큰 힘을 싣어줄 것으로 기대했으나분포를 살펴본 결과 대부분 데이터가 없는 것으로 볼 때, 이전의 마케팅에 참여한 적이 없는 신규고객들이 대다수인 것을 알 수 있었다.마지막 접촉(월)에 따른 타겟 분포를 살펴보면5,6,7,8월 같은 특정 월에 집중적으로 가입자 수가 많은 것을 알 수 있다.결혼 여부에 따른 타겟 분포를 살펴보면,결혼상태에 따라서 정기예금 신청여부도 기혼, 미혼, 이혼 상태 등의 순서로 예금 가입여부가 차이가 난다는 것을 볼 수 있다.반면 나이에 따른 타겟분포를 살펴보면,정기예금을 신청한 경우와 그렇지 않은 경우 모두 평균연령이 비슷한 30대 후반인 것을 알 수 있었다. 따라서 이 특성은 타겟을 분류하는데 별로 좋은 특성이 아닐 것이라고 생각했다.마지막 접촉(요일) 특성 또한특정 요일에 편향되지 않고 고르게 타겟의 분포가 형성되어 있는 것을 보면 타겟을 분류하는데 좋은 특성이 아니라고 추측할 수 있다. 소비자 신뢰지수나 유럽 시중은행의 금리 지수(euribo) 에 따른 타겟의 분포도 차이가 있어 EDA단계에서 의미를 찾지는 못했으나 예측모델을 만드는데 긍정적인 역할을 할 수 있을 것이라고 생각되었다.3. 모델링모델링은  작업은  좋은 성능을 보여주는 앙상블 모델 중  bagging모델의 대표주자 랜덤포레스트분류모델(이하 RF)과 boosting모델의 대표주자 그레디언트부스팅(이하 XGB) 분류모델 두가지를 비교하여  더 좋은 성능을 보여주는 모델 한가지를 선택한 후 성능개선순서로 진행하였다.먼저 RF 모델과 XGB 모델을 동일하게 랜덤서치CV를 통하여 최적파라미터를 찾아준 다음 성능을 파악한 결과RF auc : 0.778f1 : 0.353XGB auc : 0.804f1 : 0.366XGB 모델이 모든 성능지표에서 성능상 우위를 가지는 것을 볼 수 있었다. 이렇게 예측모델의 기법은 XGB모델로 결정하고성능개선 작업을 시작했다.3.1 성능개선XGB 모델은 RF모델보다 하이퍼파라미터의 튜닝에 민감하게 반응하는 모델기법이다. 그래서 큰 기대를 가지고 모델성능 개선작업에 착수하였다.성능개선 작업은  특성의 특징에 따라 인코딩 방법을 섞어서 진행  타겟의 클래스를 비슷하게 보정두 단계로 진행해보았다.먼저 인코딩방법은 기존에는 사용해봤을 때 무난한 성능을 보여주었던 ordinal 인코더를 사용했었다.하지만 새로 인코딩을 실시할 때는,범주가 적고 범주간에 우선순위가 없는 특성에는 OneHotEncoder를 사용하였고그 외의 특성에는 TargetEncoder를 사용하여 성능을 향상시키려고 시도하였다.인코딩 작업이 완료된 후의 성능은XGB auc : 0.807f1 : 0.375으로 소폭의 성능향상이 이루어졌다.두번째로 성능을 향상시키는 아이디어는 오버샘플링으로 모든 예측모델은 타겟의 클래스가 50:50일 때 가장 성능이 좋다는 점에 착안한 방법이다.이 방법에도 언더샘플링과 오버샘플링 두가지 방법이 있으나 언더샘플링은 데이터양이 줄어들기 때문에 학습 시 실행시간을 줄일 수 있다는 장점이 있지만 분류모델을 만들 때 유용하게 사용될 수 있는 데이터가 유실될 우려가 있고 임의적으로 뽑은 샘플이 편향되거나 모집단을 대표하지 않을 수 있어 최종적으로 테스트셋에 적용했을 때 결과가 좋지 않을 수 있다.오버샘플링은 소수클래스를 임의로 복제하여 수를 늘리는 방법인데 소수 클래스를 판별할 때 과적합의 우려가 있으나 전반적으로 언더샘플링보다 뛰어난 효과를 보여주므로 오버샘플링 기법으로 클래스의 비율을 맞출 것이다.이 중에서도 Synthetic Minority Over-sampling Technique (SMOTE) 알고리즘은 소수클래스로부터 일부를 선택하여 인접한 데이터 샘플을 찾아 그 사이에 새로운 데이터를 생성하는 방법으로 임의로 오버샘플링을 했을 때보다 과적합을 완화시킬 수 있다.따라서 오버샘플링 중에 SMOTE라는 방법으로 오버샘플링을 진행하였다. 좌측은 샘플링을 실시하기 전, 우측은 샘플링을 실시한 이후의 클래스 분포를 보여준다.샘플링 이후의 데이터로 다시 모델을 돌렸을 때의 성능은XGB auc : 0.976f1 : 0.898auc 스코어와 f1스코어 모두 대폭 성능이 향상된 것을 볼 수 있었고모델의 클래스 분포에 따라 같은 모델이라도 입력되는 학습데이터셋에 따라서 성능차이가 나는지를 확인할 수 있었다.4. 결과 해석4.1 특성 중요도(Permutation Importance, 순열 중요도)완성된 모델이 판단하기에 정기예금 가입 여부에 큰 영향을 주는 특성은 무엇이었는지를 알아보기 위해 Permutation Importance를 구해보았다.상위 3개의 특성이 분류결과에 영향을 많이 주는 것을 알 수 있었고 PDP 를 그려서 특성이 어떤 영향을 주었는지를 파악해보기로 하였다.4.2 PDP(Partial Dependence plot, 부분 의존성 그림)capaign은 마케팅 캠페인 중에 고객에게 연락한 횟수에 대한 특성이다.캠페인 기간중에 연락을 한 횟수가 늘어날수록 0(정기예금 가입 안함)으로 예측한다고 해석할 수 있다.다음은 순열중요도에서 상위 두가지, campaign, nr.employed 특성과 동시에 타겟과의 관계를 살펴보았다.캠페인 기간중에 연락을 한 횟수 2회까지는 소폭 1로 예측할 확률이 상승하지만 이후부터는 횟수가 늘어날수록 0(정기예금 가입 안함)으로 예측한다고 해석할 수 있다.연락을 적게할수록 직원숫자가 적을수록, 직원 숫자가 적을수록 정기예금에 가입할 확률이 높다고 해석할 수 있다.해당특성을 pdp로 살펴보기 전에는 연락을 적게 할수록 정기예금 가입을 하지 않는다고 생각했는데 오히려 연락횟수가 적을수록 정기예금을 가입하는 경우가 많은 것으로 보인다.연락횟수 1에서 2로 올라갈때는 소폭 증가하는 것을 보면 2회 이상 연락하는 것은 오히려 역효과를 내는 것으로 볼 수 있다.5. 결론5.1 Model의 기대효과 및 한계미흡하게 만들어진 모델이지만 우리나라의 여건에 적용시킬 수 있을지 여부를 생각해봤을 때,유리보(유럽연합 내 12개국 시중은행간 금리)등의 특성은 해당지역의 고유적인 인덱스이므로그에 상응하는 우리나라의 고유지표를 찾아야 모델을 구축할 수 있을 것이며 바뀐 특성에 따라 성능도 물론 달라질 것이다.하지만 우리나라의 특징에 맞는 특성들을 잘 교체할 수 있다면 충분히 모델을 구축할 수 있고 최적화여부에 따라 좋은 성능을 낼 수 있을 것 같다.모델을 튜닝하며 성능을 개선하는 작업 자체는 성취감도 있고 굉장히 재밌다고 생각했는데어떻게 성능을 개선해야겠다는 아이디어는 비교적 잘 떠오르는데에 비해 그것을 적용시키는데에는 무수한 오류를 헤쳐나가야 했기때문에 생각했던 모든 아이디어들을 시험해보지 못한 것들이 아쉬웠다.",
        "url": "/bank-marketing"
    }
    ,
    
    "roccurve": {
        "title": "ROC curve에 대해 알아보자",
            "author": "woongE",
            "category": "",
            "content": "#평가지표 #ROCcurve목차     Time seriesAnomaly Detection Generative Adversarial Networks-Model Structure       Time seriesAnomaly Detection Generative Adversarial Networks-loss function       Intro to the Time Series Anomaly Detection       Text 데이터 광고 필터링을 위한 분류 모델 구축     사용자 리뷰 핵심어 추출 및 감성분석     Deep Learning - 핵심 개념 &amp; 용어     NLP - 딥러닝 모델의 범용적 활용가능성 분석     Bank marketing 정기예금 가입여부 예측     ROC curve에 대해 알아보자     연어의 회귀본능이 아닌 선형회귀에 대해 알아보자                                                                                       ROC curve에 대해 알아보자머신러닝에 대해 일주일간 다양한 모델들을 공부했다.사용 목적에 따라 많은 종류의 머신러닝 모델이 존재하지만 결국 큰 맥락에서 보자면 머신러닝은 결국 가지고 있는 데이터로 학습을 시키고, 학습시킨 모델을 이용하여 문제를 풀어내려고(예측하려고) 하는 것이다.결국 모델의 성능을 높인다는 의미는 모델이 예측을 잘 할 수 있게 한다는 뜻이고 이를 위해서는 해당 모델을 정확하게 평가할 수 있어야 한다.머신러닝 모델에 대해 공부하다보면 해당 모델이 어떤 매커니즘으로 작동하는지, 배경은 무엇인지 등을 배우게 되지만공통적으로 모델마다 등장하는 섹터가 있다. 그것은 바로…..평가지표다.위에서 말했듯이 제대로 모델의 성능을 알 수 있어야 개선이 가능하기 때문에 모델을 제대로 평가하는 것도 만드는 것만큼이나 중요하다.그래서 이번에는 공부했던 다양한 지표중에 이해가 제일 안갔던 ROC curve에 대해 알아보려고 한다.함께 공부를 시작해보자.ROC curve는 이진분류를 하는 모델의 성능을 평가하는 지표로 사용된다.이진분류는 쉽게 말하면 O,X 문제처럼 두가지 중 하나를 고르는 분류를 말한다.백문이 불여일견이라고 일단 그림을 보자.출처 : http://www.navan.name/roc/구 개의 큰 산 모양을 보이는 그림은 데이터에서 두 클래스의 분포를 나타낸다. (O,X 문제에서 O와 X의 분포)암환자를 진단하는 모델을 만든다고 할 때, 오른쪽은 암환자, 왼쪽은 암환자가 아닌 경우이다.산이 겹치는 부분 가운데의 threshold는 모델을 만든사람이 정하는 변수로 이 임계값을 기준으로 모델은 오른쪽에 있으면 암환자로, 왼쪽에 있으면 암환자가 아니라고 판단을 내리게 된다. 그림의 좌 상단에 위치한 곡선이 ROC curve로 ROC curve는 위의 임계값에 대한 모델의 성능을 표시한 곡선이다. 핵심을 먼저 얘기하고 넘어가자면 저 커브 아래의 면적(AUC, Area Under the Curve)이 클수록 모델의 성능이 뛰어나다는 것을 의미한다.커브에 대해 살펴보려면 우선 축이 무엇인지 알아보자.x축은 FPR y축은 TPR이라고 적혀있는데 이는 각각FPR은 False Positive rateTPR은 True Positive rate을 뜻한다.의미는 같아도 해석을 사람마다 다르게하는 경향이 있으니 위 그림의 구성요소인 actual class와 predicted class에 대한 설명을 하고 넘어가면 좋을 것 같다.먼저 actual class에는 True와 False로 나뉘는데 이는 예측결과가 맞으면 True, 틀리면 False라는 것을 의미한다.다음 predicted class는 Positive와 Negative로 나뉘는데 이는 예측을 긍정적으로 했으면 Positive, 그렇지 않다면 Negative로 표현한다.위로 돌아가서 암환자를 예로 들어보면 TPR은 실제로 암에 걸린사람을 암에 걸렸다고 예측했다는 것이고FPR은 암환자라고 예측했으나 예측결과가 틀린것을 의미한다.TPR이 높다는 의미는 실제로 암이 걸린 환자를 암이 걸렸다고 판단을 잘 내린다는 뜻이고FPR이 높다는 의미는 암이 걸리지 않은 환자도 모두 암으로 판정한다는 의미이다. 암환자를 적게 놓치고 싶다면 임계값을 왼쪽으로 옮겨(낮춰) 빨간 클래스가 임계값 오른쪽에 모두 들어가게 하면 암환자를 무조건 맞출 수 있게 되겠지만 반대로 암환자가 아닌사람을 암환자로 오진하는 경우도 그만큼 늘어날 것이다.반대로 정상인을 암환자로 오진하는 경우를 줄이고 싶다면 임계값을 오른쪽으로 옮겨(올려) 파란 클래스가 임계값 왼쪽에 모두 들어가게하면 되지만 이 경우, 진짜 암에 걸린 환자를 암환자가 아니라고 판단하는 경우도 늘어나게 된다.처음에 핵심을 이야기하며 저 곡선의 아래 면적이 클수록 성능이 좋다고 했는데 이를 다르게 이야기하면 두 클래스를 더 잘 예측하는 모델일수록 곡선은 그림의 좌상단으로 가까워져 사각형에 가깝게 되고 성능이 안좋은 모델일수록 직선에 가까워져곡선 아래 면적이 삼각형에 가깝게 된다.즉 ROC curve는 임계값을 어떻게 설정해야할지 알고 싶을 때 임계값에 대한 정답율과 오답율의 비율을 보는 곡선이라고 이해하면 될 것이다.오늘은 이진분류모델의 임계값을 결정할 때 도움을 줄 수 있는 ROC curve에 대해 알아봤다.다음에도 유익한 주제를 들고 찾아뵐 수 있기를 기대하며 글을 마친다.",
        "url": "/roccurve"
    }
    ,
    
    "machinelearning-linearregression": {
        "title": "연어의 회귀본능이 아닌 선형회귀에 대해 알아보자",
            "author": "woongE",
            "category": "",
            "content": "#선형회귀 #linearregression목차     Time seriesAnomaly Detection Generative Adversarial Networks-Model Structure       Time seriesAnomaly Detection Generative Adversarial Networks-loss function       Intro to the Time Series Anomaly Detection       Text 데이터 광고 필터링을 위한 분류 모델 구축     사용자 리뷰 핵심어 추출 및 감성분석     Deep Learning - 핵심 개념 &amp; 용어     NLP - 딥러닝 모델의 범용적 활용가능성 분석     Bank marketing 정기예금 가입여부 예측     ROC curve에 대해 알아보자     연어의 회귀본능이 아닌 선형회귀에 대해 알아보자                                                                                       연어의 회귀본능이 아닌 선형회귀에 대해 알아보자선형회귀? 그게 뭐야? 회귀라는 말을 연어를 통해서만 들어본 그대.말이 어렵지 사실 그 개념 자체는 이미 우리가 알고있는 것이나 마찬가지다. 최대한 쉽게쉽게 풀어서 설명해볼 터이니오늘도 시작해보자.선형회귀라고 대놓고 주제를 던졌으니 의미를 먼저 알아야겠지?y = ax+b 라는 식이 있다면 입력항인 x와 출력항인 y의 선형 상관관계를 모델링하는 기법이다. 아직도 모호한가? 그러면 이 말을우리 생활 속의 예제로 녹여보자.저 위의 y = ax+b를 시험점수에 대한 식이라고 예를 들면,  y = 물가  x = 휴지가격  a = 가중치(운송비용 등)  b = 기본물가이렇게 정리할 수 있다.위의 그림대로라면 휴지가격을 알 때 물가가 어떠한지를 얻을 수 있다.이것도 간단한 회귀식이다. 이처럼 수식화하지 않았을 뿐 우리 생활에는 다양한 회귀식들이 숨어있다.그럼 여태까지 회귀식을 모르고도 사는데 전혀 문제가 없었는데 갑자기 왜 들고나와서 머리아프게 하는지 궁금해 하는 사람이 있을 것이다.문제는 이처럼 간단한 회귀식이라면 상관이 없겠지만 데이터의 수가 엄청나게 많아지고 특징들이 많아진다면 사람의 머리로는 저런 생각을 하는 것이 불가능 할 것이다. 이를 위해 우리는 계산기(컴퓨터)의 힘을 빌리게 되고 컴퓨터의 좋은 성능을 이용하면 알고있었던 데이터(빨간 점)에서 답을 찾는 것 뿐만이 아닌, 새로운 데이터(x축, 휴지가격)을 바탕으로 정답(y축, 물가)를 예측해볼 수도 있다.이것이 요즘 흔히 하는 말로 머신러닝이라고 하는 개념이다.물론 머신러닝도 지도, 비지도 등 여러가지 갈래로 나뉘어지지만 위에서 설명한 머신러닝은 선형적인 관계를 가진 데이터를 위한 선형회귀 예측모델이 되는 것이다. 빨간점을 활용하여 학습시키면 다른 데이터(이지만 비슷한 선형관계를 가진) 파란점의 휴지가격을 알 때, 물가를 예측해볼 수 있는 것이다. 한마디로 선형회귀는 선형관계를 가진 데이터를 모델링하여 예측하기 위한 머신러닝의 일종이다.선형회귀 기법을 사용하여 데이터에 맞는 예측모델을 만들기 위해서는 회귀직선 필요하다.회귀직선이 말하고자 하는 바는 예측을 할 때 이러한 느낌으로 예측을하면 맞출 수 있을것이라는 일종의 경향성을 의미하고 위의 그림에서는 녹색선이 그 경향을 의미한다.  우리는 데이터로 빨간 점을 가지고 있고 이 데이터가 어떤 경향을 띄는지를 가장 잘 설명해주는 선이 바로 녹색 선이다.이 선을 설명하기 위해서는 예측값과 잔차라는 개념을 알아야 하는데 예측값은 저 녹색선이 추정하는 값이고, 잔차는 관측값(빨간점의 물가)와 예측값(파란점의 물가)의 차이를 말한다.녹색선은 그림의 검은점선으로 표시된 저 관측값과 파란점선의 예측값의 제곱의 합이 최소가 되어야 가장 경향을 잘 설명할 수 있다.여기서 파란점선의 크기와 검은 점선 크기의 제곱의 합을 잔차제곱합이라고 하는데 이 잔차제곱합을 비용함수(cost function)라고 하는데 머신러닝 모델을 만드는 과정에서 비용함수를 최소화 하는 모델(선)을 찾는 과정을 학습이라고 한다.저 녹색 선을 그려서 저 녹색선을 바탕으로 값을 예측하게 하는 과정 전반이 머신러닝 선형회귀 모델링이 되는 것이다.선형회귀 모델은 주어져있지 않은 함수값을 보간하여 예측하는데 유용한데 예를들어 빨간점이 아닌, 없는 데이터 즉 파란점에 해당하는 휴지가격을 알고 있을 때 원래 데이터(빨간점)에는 없지만 녹색선을 이용하여 파란점의 휴지가격을 알 때  물가를 어림잡아 예측해볼 수 있는 것이다.오늘은 머신러닝의 많은 모델 중 하나인 선형회귀에 대해서 알아보았다. 이제는 선형회귀에 대한 말이 나오면 자신있게 대화에 끼어들어보자.앞으로도 다양한 머신러닝의 모델들을 알기쉽게 소개해보리라고 다짐하며 글을 마친다.",
        "url": "/machinelearning-linearregression"
    }
    ,
    
    "project-1": {
        "title": "미세먼지와 공시지가의 상관관계",
            "author": "woongE",
            "category": "",
            "content": "#python #pandas #statistics목차     미세먼지와 공시지가의 상관관계     통계학의 두 축, 빈도주의와 베이지안 관점에 대해 알아보자     코스레스토랑에 보내는 제안서     Git, Github 왜 써야 해?                                                                                             미세먼지와 공시지가의 상관관계abstract  작년까지 말썽을 부린 미세먼지 문제와 현재진행형으로 사회적 이슈가 되고있는 집값문제. 두가지 요소가 서로 어떤 연관성을 가지고 있는지 데이터를 이용하여 분석한다. 서울의 s-dot프로그램의 일환으로 서울전역에 설치된 850개의 미세먼지 센서를 통해 수집된 데이터와 공시지가데이터를 전처리하여 시각화해보고 미세먼지가 많은 지역과 그렇지 않은 지역의 공시지가는 어떤 경향을 보이는지 알아본다. 통계적으로 두 변수가 연관이 있는지 알아보는 것으로 프로젝트를 마무리한다.올해 초부터 Covid-19 바이러스가 세계적으로 대 유행을하면서 많은 분들이 아직까지도 힘든시기를 보내고 있죠.여기까지 읽으셨다면 오늘은 제가 무슨 이야기를 할지 감을 잡으셨을 것 같은데….얘가 오늘은 코로나 관련해서 뭘 하려나보다 싶으셨을거에요. 틀렸습니다.저는 코로나 이야기가 아닌 코로나로 인해 세상이 멈추다시피 헀지만, 반면 그로 인해 좋아진 점은 없을까 생각해봤어요.코로나가 본격적으로 유행한 것은 올해 초부터이고 작년까지 우리의 ‘건강’에 대한 민감한 이슈는 뭐였나요?불과 작년까지만 해도 우리의 건강에 관련된 최대 이슈는 미세먼지가 아니었나 싶어요.매일 밖을 나설때마다 마스크를 쓰고 나가지만 사실 우리가 마스크와 친해졌던 건 보다 먼저 미세먼지가 기승을 부렸기 때문이었죠.오늘은 하나의 주제가 아니라 한개를 더 이야기해볼꺼에요. 이것도 바로 한두달 전까지만해도 매일같이 뉴스에 나오던 이슈에요.그건 바로………………….집값 문제였죠. 아마 관심이 없는 사람도 매일같이 뉴스에서 집값이 올랐다는 말로 도배를 해대니 한두번쯤은 들어봤을거에요.각설하고 저는 미세먼지가 우리의 중요한 화두로 떠오를 무렵 문득 이런생각이 들었어요.살기좋은 동네(집값이 비싼지역)은 미세먼지도 별로 없을까?사실 살기 좋다는 것은 기준이 여러가지가 있을 수 있어서 맞을 것 같기도, 틀릴 것 같기도 하단 말이죠.살기 좋다는 건 무엇보다 여러 인프라가 편리하다는건데 그러면 차량이나 사람이 붐비게 될 것 같으니 미세먼지도 많아질 것 같고, 집값이 비싸다는건 녹지가 주변에 잘 조성되어있다는 의미일수도 있으니까 미세먼지가 별로 없을수도 있다는 생각도 들고요. 반대로 미세먼지가 많아서 집값에 영향을 줄 수도 있겠죠.그래서 오늘의 주제는 이거에요.미세먼지의 농도와 공시지가의 상관관계를 알아보자! 왼쪽 그림은 2019년에 미세먼지 문제가 전년도에 비해서 많이 심각해졌다는 것을 알 수있고오른쪽 그림은 최근 3년동안 집값의 변동을 알 수 있는 자료입니다.따로 보면 알겠는데 이 둘은 어떤 상관관계를 가지고 있을까요?이제 차근차근 저랑 알아가봅시다. 먼저 데이터 분석을 하려면 데이터가 있어야겠죠. 미세먼지 측정데이터는 서울시에서 디지털 행정기반 마련을 위해 실시하고 있는 스마트 도시데이터센서(S-dot) 측정값을 배포한 데이터셋을 사용했습니다. 오른쪽 그림은 센서위치 좌표를 이용해서 제가 시각화를 해본 그림입니다. 코드가 궁금하시면 세부정보를 눌러보세요.```pygeo = '/content/drive/My Drive/Data Science/강의노트/20201005-1009_LS_DS_project week1/Seoul1.geojson'# 서울시 중심부의 위도, 경도center = [37.541, 126.986]# center를 중심으로 하고 zoom 단계에 해당하는 맵 생성map2 = folium.Map(location = center,                  tiles = 'cartodbdark_matter',                  detect_retina=True,                 zoom_start=11)# 1000 개의 데이터만 그려냅니다.for i in location.index[:1000]:    folium.Circle(        location = location.loc[i, ['위도', '경도']],        tooltip = location.loc[i, 'NO'],        radius = 200    ).add_to(map2)map2.save('map2.html')map2```데이터셋의 기간은 센싱 데이터는 2020년 4월, 공시지가는 2020년 공시지가 데이터셋을 가져왔습니다.올해 초 코로나가 가장 심했을 때 중국도 셧다운이 됐었으니 중국으로 인한 영향이 적을 것 같아 4월의 데이터로 선정했습니다.먼저 제가 계획한 분석을 하기 위해 필요한 데이터를 정리해봤습니다.  서울시 미세먼지 센서 데이터 (시리얼번호, 미세먼지, 온도, 습도, 조도, 소음, 진동, 자외선, 풍속, 풍향 등)  서울시 미세먼지 센서 위치정보(시리얼번호, 주소,  좌표값, 설치장소설명)  서울시 공시지가(시군구명, 법정동명, 토지코드, 공시지가, 필지구분코드, 번지수 등)  법정동_행정동 코드 데이터  서울시 행정동을 경계로 한 지도데이터(jeojson)공시지가와 미세먼지데이터를 직접적으로 비교하려면 센서가 위치한 장소와 측정값 이 두가지를 알아야 하겠죠. 측정값 자체는 누구나 볼 수 있게 공개되어 있는데 위치정보는 신청서를 작성하여 담당 공무원에게 보내서 자료를 직접 받아야 합니다. 따라서 위치정보와 측정값이 분리되어있는 자료인 것이죠. 그러면 분리된 이 두 자료를 합쳐야 할텐데 이때 1,2번 데이터에 공통으로 들어가있는 값이 뭐가 있을까요? 네 바로 센서 시리얼번호입니다.이런식으로 각 자료를 합칠 때마다 공통된 열을 기준으로 값을 매핑시킬지 = merge아니면 그냥 따로 자료를 이어붙일지 = concat결정해야 하고 merge 한다면 각 자료의 교집합만 남길것인지 (how = inner) 합집합으로 합칠것인지(how = outer) 잘 고민해야 합니다.추가적인 저의 데이터 전처리과정이 궁금하시다면 세부정보를 눌러주세요.저는 센서의 값을 표만으로는 살펴보기가 힘들기 때문에 지도에 미세먼지 농도의 현황을 표현하고 싶었습니다. 그렇기 때문에 센서데이터를 지도에 표시하기 위해 각 센서의 주소를 시리얼번호를 기반으로 위치정보를 매핑시켜주었고 위치정보에 포함되어있는 주소를 세분화하여 법정동단위를 확보한 후, 공시지가 데이터셋과 법정동명을 기준으로 다시 매핑시켜주었습니다. 여기까지 센서데이터와 공시지가 데이터가 합쳐졌고 이제 이 데이터셋을 지도에 표현하기 위해 지도데이터(geojson)와 합쳐주어야 합니다. 제가 지도 시각화에 사용한 파이썬 라이브러리는 folium인데 이 패키지를 사용하려면 지도데이터와 지도에 표현할 데이터셋을 이어주는 매개체가 필요했고 저는 이 매개체를 행정동코드로 사용했습니다.기존 센서,공시지가 데이터셋의 시군구코드와 법정동 코드를 합치면 행정동코드로 변환할 수 있다는 것을 구글링을 통해 알아낸 후, **법정동_행정동 코드데이터**를 법정동코드를 기준으로 행정동코드를 매핑시켜주었습니다.전처리를 완료하고 미세먼지 농도와 공시지가의 관계를 알아보기 위해 간단하게 산점도를 그려 어떤 연관성이 있을지를 예상해봤습니다.하지만 산점도로는 유의미한 연관성을 찾기 힘들었고 미세먼지와 공시지가의 상관계수(correlation)을 구해보았습니다.상관계수 코드```pyCorrelation = final[['미세먼지(㎍/㎥)','공시지가(원/㎡)']]Correlation.corr()```결과는…..전혀 상관이 없었죠.믿을수가 없어서 지도에 나타내서 제 눈으로 과연 상관이 정말 없는지 확인해보고싶었습니다. 왼쪽이 공시지가, 오른쪽이 미세먼지 농도에 관한 그림입니다.folium 라이브러리를 이용한 지도시각화 코드가 궁금하시다면```pygeo = '/gdrive/My Drive/Data Science/강의노트/20201005-1009_LS_DS_project week1/Seoul1.geojson'# 서울시 중심부의 위도, 경도center = [37.541, 126.986]# center를 중심으로 하고 zoom 단계에 해당하는 맵 생성map3 = folium.Map(location = center,                  tiles = 'Cartodb Positron',                  detect_retina=True,                 zoom_start=11)# Choropleth 레이어를 만들고, 맵 m에 추가 folium.Choropleth(geo_data=geo,                   data=final,                   columns=('행정동코드', '미세먼지(㎍/㎥)'),                   key_on='feature.properties.adm_cd2',                   fill_color='RdPu',                   legend_name='Concentration of fine dust',                   ).add_to(map3)map3.save('map3.html')map3```선형적인 관계가 있으면 두 그림이 같은 경향을 보이거나 대비되는 모습을 보여야 하는데 아닌 것을 보면 확실히 연관성이 없다는 것을 알 수 있었습니다.하지만 여태까지 고생한게 얼만데 이렇게 끝낼 수는 없잖아요?그래서 저는 기왕 전처리해서 밑작업을 끝내놓은 김에 센서데이터를 이용해서 미세먼지 농도가 짙어질 때, 기온이나 자외선, 습도 등의 다른센서데이터는 어떤 경향을 보이는지 알아봤습니다.그러기 위해 미세먼지 농도를 기준으로 정렬하여 상위 다섯개 도시를 뽑아봤습니다.folium 지도시각화 &amp; 마커 표시하기```py# 미세먼지 상위 5개 도시geo = '/gdrive/My Drive/Data Science/강의노트/20201005-1009_LS_DS_project week1/Seoul1.geojson'# 서울시 중심부의 위도, 경도center = [37.541, 126.986]# center를 중심으로 하고 zoom 단계에 해당하는 맵 생성map5 = folium.Map(location = center,                  tiles = 'Cartodb Positron',                  detect_retina=True,                 zoom_start=12)# Choropleth 레이어를 만들고, 맵 m에 추가 folium.Choropleth(geo_data=geo,                   data=final,                   columns=('행정동코드', '미세먼지(㎍/㎥)'),                   key_on='feature.properties.adm_cd2',                   fill_color='RdPu',                   legend_name='fine dust',                   ).add_to(map5)# 마커 객체 생성marker1 = folium.Marker([37.537951, 127.005507],                         popup='Han-nam dong',                         icon=folium.Icon(color='blue'))marker2 = folium.Marker([37.566123, 126.986117],                         popup='Eul-ji-ro 2st',                         icon=folium.Icon(color='blue'))marker3 = folium.Marker([37.471441, 127.105509],                         popup='Se-gok dong 2st',                         icon=folium.Icon(color='blue'))marker4 = folium.Marker([37.573198, 127.003028],                         popup='Hyo-je dong',                         icon=folium.Icon(color='blue'))marker5 = folium.Marker([37.555363, 127.001871],                         popup='Jang Chung dong 2st',                         icon=folium.Icon(color='blue'))# 마커 추가marker1.add_to(map5)marker2.add_to(map5)marker3.add_to(map5)marker4.add_to(map5)marker5.add_to(map5)map5.save('map5.html')map5```그리고 이 다섯개의 도시를 각각 바 그래프로 그려서 미세먼지의 농도에 따른 경향을 살펴봤어요.상위도시 5개 추리기 및 바그래프로 그리기```py# 미세먼지농도 상위 5개 지역#최종데이터에서 미세먼지농도를 기준으로 내림차순, 5개 뽑아 top_dust로 선언top_dust = final.sort_values(by='미세먼지(㎍/㎥)', ascending=False).head(5)#최종데이터에서 그래프로 시각화 할 컬럼 선택하여 데이터프레임 생성top_dust = top_dust[['미세먼지(㎍/㎥)', '법정동명', '진동_z 최대(g)', '자외선(UVI)','조도(lux)', '초미세먼지(㎍/㎥)', '상대습도( %)']]#top5 데이터프레임의 법정동명을 인덱스로 지정top_dust = top_dust.set_index('법정동명')plt.figure(figsize=(12, 8), dpi=120)plt.subplot(3,3,1)plt.bar(top_dust.index, top_dust['미세먼지(㎍/㎥)'], color='#86A99F')plt.xlabel('법정동')plt.ylabel('미세먼지농도')plt.title('Top5 별 미세먼지농도')plt.subplot(3,3,2)plt.bar(top_dust.index, top_dust['초미세먼지(㎍/㎥)'], color='#86A99F')plt.xlabel('법정동')plt.ylabel('초미세먼지농도')plt.title('Top5 별 초미세먼지농도')plt.subplot(3,3,3)plt.bar(top_dust.index, top_dust['진동_z 최대(g)'], color='#86A99F')plt.xlabel('법정동')plt.ylabel('최대 진동')plt.title('Top5 별 최대진동')plt.subplot(3,3,4)plt.bar(top_dust.index, top_dust['자외선(UVI)'], color='#86A99F')plt.xlabel('법정동')plt.ylabel('자외선')plt.title('Top5 별 자외선')plt.subplot(3,3,5)plt.bar(top_dust.index, top_dust['조도(lux)'], color='#86A99F')plt.xlabel('법정동')plt.ylabel('조도')plt.title('Top5 별 조도')plt.subplot(3,3,6)plt.bar(top_dust.index, top_dust['상대습도( %)'], color='#86A99F')plt.xlabel('법정동')plt.ylabel('상대습도')plt.title('Top5 별 상대습도')plt.tight_layout()plt.show()```사실 분석을 시작하기 전에는 특정한 요인, 즉 측정기간동안 미세먼지가 심했던 지역에서는 공사를 했다던지, 행사가 열렸다던지 등의 특별한 이슈가 있을 것 같았고 이런 이슈가 있었다면 진동이나 소음 등의 측정값에서 그 결과를 볼 수 있을 것이라고 판단했거든요? 하지만 시각화를 해보니 미세먼지와 초미세먼지만 약간 비슷한 경향을 보일 뿐, 미세먼지와 다른 요소들간에는 특이점을 찾을 수 없었어요. 가지고있는 데이터를 통해서 얻을 수 있는 인사이트는 크게 없었던 것 같아서 과외로 구글링을 통하여 조사도 해봤지만 공사가 있었거나 행사가 열렸다는 내용을 특별히 발견할 수는 없었죠.전체적으로 해당 분석을 돌이켜보면..조사한 시점 기준으로 미세먼지 농도와 공시지가가 상관이 있는지 여부는 알 수 있었지만 미세먼지의 농도와 공시지가가 서로 어떤 영향을 미치는지는 분석할 수 없어 아쉬웠습니다.미세먼지 농도와 공시지가가 서로 어떤 영향을 주고받는지를 분석하려면 같은 기간의 미세먼지 농도의 변동율, 그리고 공시지가의 변동율이 필요했는데 미세먼지데이터는 올 4월부터 제공되기 시작했고 공시지가는 1년단위로 한번의 데이터밖에 없어서 변동율을 구할 수가 없었거든요. 시간을 너무 많이 사용하고나서야 내가 원하는 정보를 얻기위한 데이터가 중분치 않다는 것을 깨달아서 데이터셋을 변경할 생각도 하지 못헀어요. 하지만 추후에 데이터가 충분해지면 변동율을 이용한 분석을 다시 해봐도 괜찮을 것 같다는 생각을 했습니다.원했던 인사이트들을 얻지 못해서 잠시 허무하긴 했는데 첫 시도부터 만족스럽긴 힘들테니까요. 다음에는 어떻게 하면 좋겠다는 팁을 얻었다는 그 자체로 의미가 있었다고 생각하려고 합니다.(안그러면 허무해서 잠도 못잘 것 같아요.)추후에는 더욱 흥미로운 프로젝트를 소개해보겠습니다. 긴 글을 읽어주셔서 고맙습니다.",
        "url": "/project-1"
    }
    ,
    
    "statistics": {
        "title": "통계학의 두 축, 빈도주의와 베이지안 관점에 대해 알아보자",
            "author": "woongE",
            "category": "",
            "content": "#빈도주의 #베이지안 #통계학목차     미세먼지와 공시지가의 상관관계     통계학의 두 축, 빈도주의와 베이지안 관점에 대해 알아보자     코스레스토랑에 보내는 제안서     Git, Github 왜 써야 해?                                                                                             통계학의 두 관점, 빈도주의와 베이지안에 대해서 알아보자.“빈도주의 통계학과 베이지안 관점의 통계학은 무엇이 다른가?” 라는 질문에, 뭔가 다른 것 같긴 한데 우물쭈물 대답할 말이 마땅히 생각나지 않는 그대는 이 글을 한번쯤 읽어봤으면 좋겠다.수학, 통계 이런거 아무것도 몰라도 쉽게 이해할 수 있게 말로 장황하게 늘어놓았으니, 겁먹지 말고 시작해보자.베이지안 통계학이란?세상의 불확실성을 확률로 설명 가능한  학문이라고 할 수 있다.확률을 정리하면 크게 빈도주의와 베이지안 두가지 관점으로 나눌 수 있다.빈도주의 관점에서는 확률을 상대적인 빈도의 극한값으로 나타내는데  이 말은 동일한 일을 무한히 반복했을 때의 빈도를 의미한다. 빈도주의 관점에서는 동전던지기, 주사위 던지기 등의 무작위 사건의 확률을 설명할 수 있다.동전을 10번 던졌을 때 6번 앞면이 나온다면 이는 상대적 빈도값이 6/10이 되는 것이고, 10000번 던졌을 때 5600번 앞면이 나온다면 5600/10000, 횟수를 무한히 늘려나간다면 이 값은 결국 1/2 = 0.5(50%)에 수렴하게 된다. 이를 앞면이 나올 확률이라고 정의하는 것이다.하지만 빈도주의 관점에서의 확률은 명확한 한계를 가지고 있는데 횟수를 무한히 늘릴 수 없는 사건에 대해서는 확률 계산이 불가능하다는 것이다. 예를들어 내일 비가 내릴 확률은 상대적인 빈도로는 설명할 수 없다. 같은 조건에서 무한히 반복할 수 없기 때문에 빈도를 계산할 수 없어서 확률을 구할 수가 없다. 이처럼 무작위 사건이 아닌 인식론적 불확실성을 설명하기 위해서 새로운 확률의 정의가 필요성이 생겼고,그래서 등장한 것이 베이지안 관점의 확률이다.베이지안 관점에서는 확률을 믿음의 정도(degree of belief)로 정의한다. 가령 “내일 비가 내릴 확률은 70%이다”라는 말처럼 어떠한 사건 발생에 대한 믿음의 정도로 확률을 정의하기 때문에, 빈도주의로 설명할 수 없는 훨씬 다양한 상황에 확률을 부여할 수 있다. 때문에 베이지안 관점의 확률을 불확실성 측정의 도구라고도 이야기한다.베이즈정리를 설명해보면,P(H) 는 사전확률로 가설 H에 대해서 사전에 우리가 믿고있는 정도(사전지식)를 이야기한다. 가능도P(D|H)는 가설 H가 주어졌을 때 데이터 D를 지지하는 정도, 즉 데이터D가 관찰될 가능성에 해당한다.사전확률 P(H) 와 가능도P(D|H)를 곱하여 업데이트된 확률인 사후확률 P(H|D)를 얻는다. 이는 데이터 D가 관찰되었을 때, 가설 H의 확률을 말한다.            모집단을 설명하는 가설 H1,H2, H3, H4가 있다고 가정하고, D라는 데이터가 관찰되었을 때, P(H1      D)를 공식으로 유도해보면 다음과 같다.      이제 공식을 설명해보면,P(H) 는 사전확률로 가설 H에 대해서 사전에 우리가 믿고있는 정도(사전지식)를 이야기한다. P(D|H)에서 가설 H가 주어졌을 때 데이터 D를 지지하는 정도, 즉 데이터D가 관찰될 가능성에 해당한다.사전확률 P(H) 와 가능도P(D|H)를 곱하여 업데이트된 확률인 사후확률 P(H|D)를 얻는다. 이는 데이터 D가 관찰되었을 때, 가설 H의 확률을 말한다.사전확률 H1, H2, H3, H4는 모두 같았지만 D라는 데이터가 관찰되고 난 후, 데이터 D 안에서 H1이 차지하는 비중이 큰 것을 볼 수 있다. 즉, 데이터가 H1을 더 많이 지지하기 때문에 실제 현상에서도 H1이 일어날 확률이 더 높다고 추론할 수 있다.따라서 사전확률은 동일했지만 데이터 D가 주어졌을 때 H1의 확률은 다른 가설들보다 H1이 더 크게 되는 것이다. 즉 데이터가 관찰되고 나서 사전확률이 사후확률로 업데이트된 것이다.이처럼 베이즈 정리는 관찰된 데이터를 토대로 사전확률이 어떻게 사후확률로 업데이트 되는지를 알 수 있게 해주고 데이터는 가능도를 통해 사후확률에 영향을 준다는 것을 알 수 있다.기존의 빈도주의에 의한 통계는 집단이 어떤 분포를 가지는지를 정의하고 계산을 통한 확률에 대해 이야기한다. 하지만 베이지안 관점에서는 사전확률 즉, 경험이 주어지고 추가적인 정보를 통해 확률을 지속적으로 갱신한다.현대에 와서 베이지안 관점의 통계가 더 많은 주목을 받는 이유는 경험(데이터)가 나올수록 예측모델이 더 정확해지기 때문인데 데이터가 넘쳐나는 세상이라 그런 듯 하다. 호랑이 등에 날개를 단 격이라고 할까. 추가로 나중에 머신러닝에 대해 공부할 때, 이런 베이즈정리가 많이 쓰인다고 한다. 이번에 공부하면서 머신러닝에 Naive Bayes Clasify라는 이론도 많이 쓰인다는 것을 알게 되었는데 추후 머신러닝에 대해 공부할 때 이에 대해서도 자세히 소개하기로 다짐하며 글을 마친다.",
        "url": "/statistics"
    }
    ,
    
    "proposal": {
        "title": "코스레스토랑에 보내는 제안서",
            "author": "woongE",
            "category": "",
            "content": "#시각화 #visualization #seaborn #tips목차     미세먼지와 공시지가의 상관관계     통계학의 두 축, 빈도주의와 베이지안 관점에 대해 알아보자     코스레스토랑에 보내는 제안서     Git, Github 왜 써야 해?                                                                                             본 글에 나오는 데이터는 Seaborn 라이브러리의 기본 데이터셋 tips를 활용하였고, 코스레스토랑(코드스테이츠)과 구정환(구일모, 한정민, 김진환) 매니저는 가상의 장소, 인물임을 알려드립니다.코스레스토랑에 보내는 제안서안녕하세요 코드레스토랑 구정환 매니저님. 우선 저를 믿고 서비스 컨설팅 의뢰를 맡겨주셔서 진심으로 감사합니다.보내주신 데이터의 분석 결과나 제안들을 보여드리기에 앞서, 본데이터를 어떻게 활용할 것이고 한계는 어떤것이었는지에 대해 먼저 간단하게 설명드리려고 합니다.먼저 보내주신 데이터를 간단하게 살펴봤을 때, 데이터 안에는 244개의 매출데이터가 있었는데 양을 봤을 때 일주일 중 목요일부터 일요일까지 일주일 정도의 분량일 것으로 생각됩니다. 데이터가 많을수록 매출의 ‘경향성’을 파악하기가 용이하지만 보내주신 데이터를 최대한 활용하여 분석을 실시했습니다. 나름의 근거를 제시했으나 데이터의 양이 부족하였기 때문에 제가 드리는 제언은 절대적인 것이 아니며 레스토랑 경영에 참고지표 정도로 사용하시는 것이 좋다는 사실을 미리 말씀드립니다.팁에 어떤 요인이 있는지를 분석해달라고 의뢰를 하셔서 어떤 방면으로 분석결과를 활용하실 것인지, 왜 팁에 대한 상관관계가 궁금했을지를 제일 먼저 생각해봤습니다.팁이 만약 종업원에게 전부 돌아간다면 경영자의 입장에서 분석을 의뢰하실 정도로 크게 의미있는 요소는 아니라고 생각되어 한가지 전제를 가지고 데이터 분석을 실시하였습니다.  전제1. 레스토랑에서 받는 팁은 레스토랑측과 종업원이 일정비율로 나누어 갖는다. 즉 팁을 받는 것은 매출 향상에도 기여하고,  전제2. 따라서 팁을 많이 받을 수 있는 요인을 분석하는 것이 레스토랑의 매출을 향상시킬 수 있다고 판단하였습니다.자세한 분석결과를 보고싶으시다면 아래 세부정보를 눌러서 글을 펼쳐주세요.### 요일별 테이블 당 매출 &amp; 팁먼저 전체 매출과 팁은 어떤 연관성이 있는지에 대한 결과입니다.![image](https://user-images.githubusercontent.com/70134676/92865871-d1e54c00-f439-11ea-9d87-684d531ef7c8.png)![image](https://user-images.githubusercontent.com/70134676/92866746-dcecac00-f43a-11ea-8d7f-f27e2ac66761.png)분석결과는 평균값으로 각 요일마다 테이블당 평균 매출과 팁이 얼마나 발생했는지를 보여주고 있습니다. 전반적으로 주중보다는 주말이 매출이 높았으며 팁은 매출과 비례하여 발생한다는 사실을 알 수 있습니다. 큰 의미가 있는 지표는 아니며 테이블당 얼마의 매출이 발생하는지, 전체 매출에 대한 팁은 어떤 연관관계가 있는지 정도의 기본정보라고 보시면 됩니다.다음은 팁을 받는데는 어떤 요인이 작용할지에 대한 분석자료입니다. 메뉴나 가격에 대한 정보가 없어 팁을 늘리기 위한 방편으로 팁을 받는 횟수(즉 매출이 발생한 횟수)를 늘려야 한다는 점에 초점을 맞추고 분석을 진행하였습니다.### 요일 별 팁 발생 횟수![image](https://user-images.githubusercontent.com/70134676/92867635-ceeb5b00-f43b-11ea-97f0-ff06315c2b7c.png)위의 그래프를 보시면 **금요일날 매출이 유독 적게 발생한다는 사실을 알 수 있습니다.**### 일행 숫자 별 팁 발생 횟수![image](https://user-images.githubusercontent.com/70134676/92867964-2db0d480-f43c-11ea-9a0c-0d09d201f0bb.png)위의 그래프는 일행 숫자에 따른 팁 횟수입니다. 여기서는 의미가 있는 결과가 나왔는데**일주일간 전체  매출 횟수 중에 2인 일행이 이용한 횟수가 절대적으로 많다는 사실을 보여줍니다.**### 성별에 따른 계산 횟수![image](https://user-images.githubusercontent.com/70134676/92868498-bdef1980-f43c-11ea-88f1-0a2852502431.png)다음은 어느 성별이 더 많이 계산을 하는가에 대한 지표입니다. 보통 **남성이 여성에 비해 두배에 가깝게 계산을 많이 한다는 것을 알 수 있습니다.**### 시간에 따른 팁 발생 횟수![image](https://user-images.githubusercontent.com/70134676/92868567-ce9f8f80-f43c-11ea-8f39-946bf3b6182a.png)마지막으로 전체 매출 중에 점심, 저녁으로 나누어 언제 팁이 많이 발생했는지에 대한 지표입니다.**팁 발생 횟수는 저녁이 점심보다 290%정도 많았다는 사실을 알 수 있습니다.**위의 분석 결과를 토대로 제가 코드레스토랑에 드리는 서비스 제언입니다.  금요일에 테이블회전율이 낮으니 금요일에 매출을 늘릴 수 있는 방법을 강구해야 합니다. 혹은 금요일의 매출이 정상궤도에 올라오기 전까지 인건비의 절약 차원에서 금요일에 종업원들에게 휴무일을 지정하거나 잉여노동력을 매장이 바쁜 시간대에 재배치 하는 등의 조치를 통해 노동력을 활용함에 있어 효율을 높이시길 바랍니다.  점심에 이용하는 고객보다 저녁에 이용하는 고객이 많으므로 점심에 더 많은 고객을 유치할 수 있도록 하거나 저녁시간에 많은 손님이 몰릴 때 불만사항이 생기지 않도록 서비스를 더욱 강화해야 합니다.  2인 테이블에서 대부분의 매출이 나오고 있으니 2인 테이블 손님들에게 좋은 인상을 줄만한 서비스를 발전시키는 것이 매출 향상에 유리합니다.위의 제언을 참고하셔서 구정환 매니저님의 코드레스토랑이 더욱 번창하시길 기원합니다.사실 읽어보시면 아시겠지만 분석결과가 허접합니다.  그래서 어떤 코드를 작성했고 어떤 기법을 통해 예측을 했고 이런 기술적인 면 보다는 우리가 실제로 현업에 나가게 된다면 비전공자에게 분석 결과를 이해시켜야 할 일이 많을 것 같아 어떻게 쉽게 설명할 수 있을까? 에 초점을 맞춘 글이라고 자기합리화를 해봅니다. (사실 판다스를 다루는 능력이 일천하여 시간내에 제가 원하는 연산들을 할 수가 없었습니다.ㅠㅠ)긴 글을 읽어주셔서 고맙습니다.",
        "url": "/proposal"
    }
    ,
    
    "github": {
        "title": "Git, Github 왜 써야 해?",
            "author": "woongE",
            "category": "",
            "content": "#git #github목차     미세먼지와 공시지가의 상관관계     통계학의 두 축, 빈도주의와 베이지안 관점에 대해 알아보자     코스레스토랑에 보내는 제안서     Git, Github 왜 써야 해?                                                                                             Git, Github 왜 써야 해?Github를 사용하라 는 마치 마른하늘의 날벼락마냥 내게 미션처럼 주어졌다. 하라는 대로 따라하면 할 수는 있겠지만 별로 유용해보이지도 않는데(구글드라이브로 하면 안되나?) 굳이 내가 왜 이 Github에 익숙해져야 하는지 그 이유가 궁금했다. 개인적으로 ‘왜’ 인지 이유를 알아야 의욕이 불타오르기도 하고 이유를 스스로 납득하면 Github에 익숙해지는데 더 동기유발도 되고 애정도 생길 것 같아 스스로 공부해볼 겸, 혹은 내가 궁금해하는 점을 똑같이 느끼는 누군가를 위하여, Git과 Github에 대하여 Github와 markdown을 이용하여 연습삼아 글을 몇글자 적어본다.Github를 검색하면 이렇게 다양한 정보를 얻을 수 있다.읽어도 이해가 잘 안가는 말이 태반이기에 이런 wiki문서를 내가 쉬운버전으로 번역하여 다시 작성해본다고 생각하고 글을 작성하려고 한다.Github를 왜 사용해야하는지 이해하기 위해서는 먼저 Git에 대한 이해가 필요하다.Git이란 무엇인가?Git이란 분산형 버전관리시스템(DVCS)인데 버전관리시스템(VCS) 에 대해 먼저 알아보면 소스 코드를 관리할 때 버전별로 즉, 변화가 생길때마다 새롭게 저장을 하여 롤백하기 쉽게 하는 그런 시스템이라고 생각하면 될 듯 하다. 어려울 수 있는 말인데 예를들어 메모장을 열어서 작업 중간마다 저장을 하면 하나의 파일에 덮어씌워지는 구조이지만 Git같은 경우는 파일 변화를 시간에 따라 기록했다가 나중에 특정 시점의 버전을 다시 불러올 수 있게하는 시스템이다. 단순하게 생각해봐도 VCS를 사용하면 파일을 잃어버리거나 혹은 잘못 고쳤을 때도 쉽게 복구가 가능하다.  한마디로 ‘다른이름으로저장’ 을 수정이 될때마다 수행해서 백업해준다는 것. 다만 각 버전의 변경사항만 기록되기때문에 여러 번 변경되고 저장된다고 하더라도 용량을 몇배로 차지하지는 않는다.그렇다면 버전관리시스템 앞의 ‘분산형’이라는 말은 무엇일까?분산형 버전관리시스템은 중앙집중식과 달리 중앙서버자체가 없거나 그 역할을 하는 컴퓨터가 있다고 하더라도, 그 서버가 망가졌을 때 다른 컴퓨터로부터 자료를 복구할 수 있다는 특징이 있다.Git은 왜 필요할까?개발분야에서 협업을 하다보면 수없이 많은 오류와 수정이 반복된다. 그러다보면 프로젝트를 과거로 되돌려야하는 일이 오는데 이럴 때 Git은 변화가 생긴 지점을 찾아 다시 되돌리기가 편하기 때문에 중간마다 따로 백업을 해둘 필요가 없다. 또한 다른사람들과 일을 같이 할 때도 수정사항을 쉽게 공유가 가능한 것도  Git이 가진 큰 장점이다.조금 더 자세히 들어가보면  분산개발이 가능하다.Git을 사용하는 전체 개발 내역을 각 개발자의 로컬 컴퓨터로 복사하여 작업 후, 나중에 수정된 내역을 합칠 수 있다.  비선형적인 개발이 가능하다.Git은 브랜치(Branch)라는 개념이 사용되는데 쉽게 말해서 프로젝트의 가지치기가 가능하다.  효율적인 개발이 가능하다.Git은 변경 이력이 많더라도 변경된 내용만 처리한다는 점에서 메모리적인 효율성이 뛰어나다.만약 Git을 사용하지 않는다면?Git을 사용하지 않으면 프로젝트를 진행할 때 서로 일한 내용 공유하려고 소스코드를 USB에 담아서 다른사람한테 넘기거나 메일 등 다른수단으로 보내야 한다. 그러면 A가 수정한 내역이 B에게 즉각적으로 전달이 안 되기 때문에 다른 메신저를 이용하거나 전화하는 등의 추가적인 소통이 필요하여 A와 B가 동시에 작업을 진행하기가 어렵다. 또 작업을 완료해서 작업물을 업데이트 했는데 작동이 되지 않는다면 다시 이전버전으로 돌려야하는데 어디서부터 어떻게 손을 대야 할지 막막한 사태가 벌어지는 것이다.또한 누가 어떤 부분을 어떻게 고쳤는지 확인하기 어려워 마지막으로 파일의 어떤 부분이 변경됐는지 확인하려면 일일이 대조 작업을 해야 한다.Github?Git이 분산버전관리시스템이라는 소프트웨어라면 Github는 Git에서 진행되었던 각각의 내용들을 공유할 수 있게 공간을 제공하는 서비스 라고 생각할 수 있다. 다수와 협업을 하면서도 물리적인 공간에 모여있을 필요 없이 원활하게 일을 공유할 수 있게 해주는 가상공간이라고 생각하면 쉬울 것이다.Github는 전 세계의 개발자들이 이용하고 있기 때문에 내가 프로젝트를 진행하며 작업했던 소스코드들을 공개, 공유할 수 있고 그렇게 되면 전 세계에서 나와 비슷한 프로젝트를 가지고 고민했던 사람들과 소통하며 피드백을 받거나 줄 수 있다.오픈소스의 성지라고 불릴만큼 세계 곳곳의 뛰어난 프로그래머들이 작성한 수많은 코드들을 살펴볼 수 있고 본인이 의지만 있다면 코드를 올린 후 코드 리뷰도 받을 수 있으니 코딩 실력을 향상시키는데 활용하면 좋을 것이다.이번에는 Git과 Github에 대해 알아보았는데 이 글을 읽고 Github를 사용하려는 사람이 왜 Github와 친해져야 하는지 나처럼 이해가 되었기를 바라며.",
        "url": "/github"
    }
    
    
    };
</script>
<script src="assets/js/lunr.js"></script>
<script src="assets/js/search.js"></script>
            </section>

        </article>

    </div>
</main>

<!-- /post -->

<!-- The #contentFor helper here will send everything inside it up to the matching #block helper found in default.hbs -->
<script>
$(function() {
    var $postContent = $(".post-full-content");
    $postContent.fitVids();
});
</script>



        <!-- Previous/next page links - displayed on every page -->
        

        <!-- The footer at the very bottom of the screen -->
        <footer class="site-footer outer">
            <div class="site-footer-content inner">
                <section class="copyright"><a href="http://localhost:4000/">Data Scientist 성장기</a> &copy; 2023</section>
                <section class="poweredby">Proudly published with <a href="https://jekyllrb.com/">Jekyll</a> &
                    <a href="https://pages.github.com/" target="_blank" rel="noopener">GitHub Pages</a> using
                    <a href="https://github.com/jekyller/jasper2" target="_blank" rel="noopener">Jasper2</a></section>
                <nav class="site-footer-nav">
                    <a href="/">Latest Posts</a>
                    
                    
                    <a href="https://ghost.org" target="_blank" rel="noopener">Ghost</a>
                </nav>
            </div>
        </footer>

    </div>

    <!-- The big email subscribe modal content -->
    
    <div id="subscribe" class="subscribe-overlay">
        <a class="subscribe-overlay-close" href="#"></a>
        <div class="subscribe-overlay-content">
            
            <h1 class="subscribe-overlay-title">Search Data Scientist 성장기</h1>
            <p class="subscribe-overlay-description">
            lunr.js를 이용한 posts 검색 </p>
            <span id="searchform" method="post" action="/subscribe/" class="">
    <input class="confirm" type="hidden" name="confirm"  />
    <input class="location" type="hidden" name="location"  />
    <input class="referrer" type="hidden" name="referrer"  />

    <div class="form-group">
        <input class="subscribe-email" onkeyup="myFunc()" 
               id="searchtext" type="text" name="searchtext"  
               placeholder="Search..." />
    </div>
    <script type="text/javascript">
        function myFunc() {
            if(event.keyCode == 13) {
                var url = encodeURIComponent($("#searchtext").val());
                location.href = "/search.html?query=" + url;
            }
        }
    </script>
</span>
        </div>
    </div>
    

    <!-- highlight.js -->
    <script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.10.0/components/prism-abap.min.js"></script>
    <script>$(document).ready(function() {
      $('pre code').each(function(i, block) {
        hljs.highlightBlock(block);
      });
    });</script>

    <!-- jQuery + Fitvids, which makes all video embeds responsive -->
    <script
        src="https://code.jquery.com/jquery-3.2.1.min.js"
        integrity="sha256-hwg4gsxgFZhOsEEamdOYGBf13FyQuiTwlAQgxVSNgt4="
        crossorigin="anonymous">
    </script>
    <script type="text/javascript" src="/assets/js/jquery.fitvids.js"></script>
    <script type="text/javascript" src="https://demo.ghost.io/assets/js/jquery.fitvids.js?v=724281a32e"></script>


    <!-- Paginator increased to "infinit" in _config.yml -->
    <!-- if paginator.posts  -->
    <!-- <script>
        var maxPages = parseInt('');
    </script>
    <script src="/assets/js/infinitescroll.js"></script> -->
    <!-- /endif -->

    


    <!-- Add Google Analytics  -->
    <!-- Google Analytics Tracking code -->
 <script>
  (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
  (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
  m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
  })(window,document,'script','//www.google-analytics.com/analytics.js','ga');

  ga('create', 'UA-xxxxxxxx-x', 'auto');
  ga('send', 'pageview');

 </script>


    <!-- The #block helper will pull in data from the #contentFor other template files. In this case, there's some JavaScript which we only want to use in post.hbs, but it needs to be included down here, after jQuery has already loaded. -->
    
        <script>
$(function() {
    var $postContent = $(".post-full-content");
    $postContent.fitVids();
});
</script>

    

    <!-- Ghost outputs important scripts and data with this tag - it should always be the very last thing before the closing body tag -->
    <!-- ghost_foot -->

</body>
</html>
